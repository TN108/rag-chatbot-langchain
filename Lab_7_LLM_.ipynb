{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnSo3HSlobBL"
      },
      "source": [
        "# **Building a RAG-Based Chatbot with LangChain From Youtube Playlist**\n",
        "\n",
        "#### **Lead TA: M. Safiullah 25100056@lums.edu.pk**\n",
        "In this lab, we will explore how to build a **Retrieval-Augmented Generation (RAG)**-based chatbot using **LangChain**, a powerful framework for developing applications with large language models (LLMs). A RAG-based model enhances the capabilities of language models by augmenting their responses with external knowledge retrieved from a document corpus.\n",
        "\n",
        "> ‚ö†Ô∏è **Note:** It is highly recommended to run this lab in **Google Colab** for ease of setup, access to required packages, and GPU acceleration if needed. You can upload this notebook to [Google Colab](https://colab.research.google.com/) and run it directly there.\n",
        "\n",
        "## üéØ **Objective**\n",
        "\n",
        "The goal of this lab is to build a chatbot that can:\n",
        "\n",
        "1. **Retrieve relevant information** from a document store or knowledge base based on the user's question.\n",
        "2. **Generate coherent answers** using the retrieved context, effectively combining retrieval with generative responses.\n",
        "\n",
        "This approach makes it possible to create chatbots that are more **context-aware**, **factual**, and can handle a wide variety of questions by leveraging external data.\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Concepts**\n",
        "\n",
        "1. **Retrieval-Augmented Generation (RAG)**:\n",
        "   - RAG models combine two stages ‚Äî first, retrieving relevant information from an external corpus, and second, generating a response using the retrieved information.\n",
        "   - This allows the model to provide more accurate and up-to-date answers.\n",
        "\n",
        "2. **LangChain**:\n",
        "   - LangChain is a Python framework designed to simplify the development of applications using LLMs. It provides modules for **document processing**, **prompt management**, and managing the flow of information between components.\n",
        "   - LangChain allows developers to easily integrate LLMs with external sources of information, creating powerful applications like RAG-based chatbots.\n",
        "\n",
        "3. **Document Retriever**:\n",
        "   - This is a key part of the RAG system that identifies and retrieves relevant documents or pieces of information based on a query.\n",
        "   - A good document retriever ensures that the chatbot's responses are informed by the most relevant information.\n",
        "\n",
        "4. **Generative Models**:\n",
        "   - The generative model (e.g., **GPT-3**, **Mistral**, etc.) takes the retrieved context and produces a natural language response.\n",
        "   - The generative model allows the system to convert retrieved knowledge into a coherent and natural response.\n",
        "\n",
        "---\n",
        "\n",
        "By the end of this lab, you will have a working **RAG-based chatbot** that can intelligently answer questions based on external knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txQW5LtLoYik"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Import the necessary libraries and tools to get started with our RAG-based chatbot implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aac2GDozocTd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Install the necessary libraries. This might take some time\n",
        "!pip -q install langchain openai chromadb tiktoken sentence_transformers langchainhub langchain-community tqdm yt-dlp whisper --upgrade openai-whisper\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjDK1BS3HcJr"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://python.langchain.com/docs/use_cases/question_answering/\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import yt_dlp\n",
        "import whisper\n",
        "from tqdm import tqdm\n",
        "from langchain import hub\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2r_AlwK5zax"
      },
      "source": [
        "Add your Hugging Face Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XOiFn8baTRB"
      },
      "outputs": [],
      "source": [
        "# We'll be using Mistral 7B for inference\n",
        "# Make you hugging face token and make sure to tick the required checkboxes for inference\n",
        "os.environ['HUGGINGFACE_API_TOKEN'] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFIsTwSqoLg9"
      },
      "source": [
        "\n",
        "# 1 - Gathering the Dataset\n",
        "\n",
        "In this step, we will gather a dataset by downloading and transcribing audio from YouTube videos in a playlist. We will use the **Whisper** model for transcription and **yt-dlp** to download the audio from YouTube videos. The transcriptions will be saved as text files for further processing.\n",
        "\n",
        "\n",
        "Your task is to:\n",
        "1. Download the audio of YouTube videos from a specific playlist.\n",
        "2. Transcribe the audio using the Whisper model.\n",
        "3. Save the transcriptions along with video metadata such as the video URL and title.\n",
        "\n",
        "#### Steps Involved\n",
        "\n",
        "1. **Playlist URL**: We will define the playlist URL that contains multiple YouTube videos from which we need to extract and transcribe the audio.\n",
        "\n",
        "2. **Download Audio**: Using **yt-dlp** (a powerful downloader), we will extract the audio from the videos in the playlist. The audio will be saved as MP3 files in a specific folder.\n",
        "\n",
        "3. **Transcribe Audio**: After downloading the audio, we will use the **Whisper** model to transcribe the speech to text. This will allow us to convert the spoken content of the videos into a text format.\n",
        "\n",
        "4. **Save Transcriptions**: The transcriptions will be saved as text files, where each file will include the YouTube video URL, title, and the transcribed text.\n",
        "\n",
        "5. **Organize Files**: The downloaded audio files and their corresponding transcriptions will be saved in separate folders, ensuring proper organization of the data.\n",
        "\n",
        "### üé¨ **3Blue1Brown Playlist**\n",
        "\n",
        "For this lab, we will be using the **3Blue1Brown YouTube Neural Networks playlist**, which is renowned for its engaging and educational content on mathematics and machine learning. The playlist contains several videos that break down complex concepts related to neural networks and deep learning using visually intuitive animations. These videos provide clear explanations and are perfect for transcription and knowledge extraction, as they offer valuable insights into the workings of neural networks while being easy to follow and understand.\n",
        "\n",
        "\n",
        "We will extract the audio from each of these videos, transcribe the speech using the Whisper model, and organize the content for further use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg-WHrfAbQML",
        "outputId": "ed1d4dc4-797a-45db-dccc-14dfb208f253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:01<00:00, 77.5MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# Define the 3Blue1Brown Neural Networks playlist URL\n",
        "playlist_url = \"https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\"\n",
        "\n",
        "# Initialize The base Whisper model and use Cuda for faster computations\n",
        "model = whisper.load_model(\"base\").to(\"cuda\") #CODE HERE\n",
        "\n",
        "# Set up yt-dlp options for downloading audio\n",
        "ydl_opts = {\n",
        "    # 'cookiefile': '/content/cookies.txt', #you might need a cookei file if you get error downloading the audios.\n",
        "    'format': 'bestaudio/best',\n",
        "    'outtmpl': 'downloads/%(id)s.%(ext)s',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',  # Set bitrate/quality to 192 kbps\n",
        "    }],\n",
        "}\n",
        "\n",
        "os.makedirs('downloads', exist_ok=True) # Creating directories for saving files if not exist\n",
        "os.makedirs('transcriptions', exist_ok=True)\n",
        "\n",
        "# CODE to complete the function\n",
        "def process_video(url):\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info_dict = ydl.extract_info(url, download=True)\n",
        "            video_id = info_dict.get('id', None)\n",
        "            video_title = info_dict.get('title', None)\n",
        "            audio_file = f'downloads/{video_id}.mp3'\n",
        "\n",
        "            video_title = re.sub(r'[\\\\/*?:\"<>|]', \"_\", video_title)\n",
        "            result = model.transcribe(audio_file)\n",
        "            transcription_file = f'transcriptions/{video_id}.txt'\n",
        "            with open(transcription_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"URL: {url}\\n\")\n",
        "                f.write(f\"Title: {video_title}\\n\")\n",
        "                f.write(\"TRANSCRIPT\\n\")\n",
        "                f.write(result['text'])\n",
        "            # Use the whisper model defined on the audio file to transcribe it and then save the transcription as txt file in\n",
        "            # transcription folder\n",
        "            # While writign to the txt file, follow the pattern where first lines would look like.\n",
        "                # url\n",
        "                # title\n",
        "                # \"TRANSCRIPT\"\n",
        "                # Transcripted content.............\n",
        "\n",
        "            ###.   CODE HERE\n",
        "            ###.   CODE HERE\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {url}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgyif7gZ6gOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f495f343-0a24-44cd-c17d-db795981b685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing videos:   0%|          | 0/8 [00:00<?, ?video/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=aircAruvnKk\n",
            "[youtube] aircAruvnKk: Downloading webpage\n",
            "[youtube] aircAruvnKk: Downloading tv client config\n",
            "[youtube] aircAruvnKk: Downloading player 73381ccc-main\n",
            "[youtube] aircAruvnKk: Downloading tv player API JSON\n",
            "[youtube] aircAruvnKk: Downloading ios player API JSON\n",
            "[youtube] aircAruvnKk: Downloading m3u8 information\n",
            "[info] aircAruvnKk: Downloading 1 format(s): 251-7\n",
            "[download] Destination: downloads/aircAruvnKk.webm\n",
            "[download] 100% of   17.91MiB in 00:00:00 at 29.05MiB/s  \n",
            "[ExtractAudio] Destination: downloads/aircAruvnKk.mp3\n",
            "Deleting original file downloads/aircAruvnKk.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing videos:  12%|‚ñà‚ñé        | 1/8 [01:22<09:40, 82.93s/video]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
            "[youtube] IHZwWFHWa-w: Downloading webpage\n",
            "[youtube] IHZwWFHWa-w: Downloading tv client config\n",
            "[youtube] IHZwWFHWa-w: Downloading player 73381ccc-main\n",
            "[youtube] IHZwWFHWa-w: Downloading tv player API JSON\n",
            "[youtube] IHZwWFHWa-w: Downloading ios player API JSON\n",
            "[youtube] IHZwWFHWa-w: Downloading m3u8 information\n",
            "[info] IHZwWFHWa-w: Downloading 1 format(s): 251-5\n",
            "[download] Destination: downloads/IHZwWFHWa-w.webm\n",
            "[download] 100% of   19.74MiB in 00:00:00 at 28.67MiB/s  \n",
            "[ExtractAudio] Destination: downloads/IHZwWFHWa-w.mp3\n",
            "Deleting original file downloads/IHZwWFHWa-w.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing videos:  25%|‚ñà‚ñà‚ñå       | 2/8 [02:53<08:44, 87.44s/video]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=Ilg3gGewQ5U\n",
            "[youtube] Ilg3gGewQ5U: Downloading webpage\n",
            "[youtube] Ilg3gGewQ5U: Downloading tv client config\n",
            "[youtube] Ilg3gGewQ5U: Downloading player 73381ccc-main\n",
            "[youtube] Ilg3gGewQ5U: Downloading tv player API JSON\n",
            "[youtube] Ilg3gGewQ5U: Downloading ios player API JSON\n",
            "[youtube] Ilg3gGewQ5U: Downloading m3u8 information\n",
            "[info] Ilg3gGewQ5U: Downloading 1 format(s): 251-4\n",
            "[download] Destination: downloads/Ilg3gGewQ5U.webm\n",
            "[download] 100% of   12.95MiB in 00:00:00 at 19.65MiB/s  \n",
            "[ExtractAudio] Destination: downloads/Ilg3gGewQ5U.mp3\n",
            "Deleting original file downloads/Ilg3gGewQ5U.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing videos:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [03:51<06:10, 74.19s/video]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
            "[youtube] tIeHLnjs5U8: Downloading webpage\n",
            "[youtube] tIeHLnjs5U8: Downloading tv client config\n",
            "[youtube] tIeHLnjs5U8: Downloading player 73381ccc-main\n",
            "[youtube] tIeHLnjs5U8: Downloading tv player API JSON\n",
            "[youtube] tIeHLnjs5U8: Downloading ios player API JSON\n",
            "[youtube] tIeHLnjs5U8: Downloading m3u8 information\n",
            "[info] tIeHLnjs5U8: Downloading 1 format(s): 251-3\n",
            "[download] Destination: downloads/tIeHLnjs5U8.webm\n",
            "[download] 100% of    9.98MiB in 00:00:00 at 17.79MiB/s  \n",
            "[ExtractAudio] Destination: downloads/tIeHLnjs5U8.mp3\n",
            "Deleting original file downloads/tIeHLnjs5U8.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing videos:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [04:37<04:11, 62.76s/video]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=LPZh9BOjkQs\n",
            "[youtube] LPZh9BOjkQs: Downloading webpage\n",
            "[youtube] LPZh9BOjkQs: Downloading tv client config\n",
            "[youtube] LPZh9BOjkQs: Downloading player 73381ccc-main\n",
            "[youtube] LPZh9BOjkQs: Downloading tv player API JSON\n",
            "[youtube] LPZh9BOjkQs: Downloading ios player API JSON\n",
            "[youtube] LPZh9BOjkQs: Downloading m3u8 information\n",
            "[info] LPZh9BOjkQs: Downloading 1 format(s): 251-10\n",
            "[download] Destination: downloads/LPZh9BOjkQs.webm\n",
            "[download] 100% of    7.79MiB in 00:00:00 at 18.49MiB/s  \n",
            "[ExtractAudio] Destination: downloads/LPZh9BOjkQs.mp3\n",
            "Deleting original file downloads/LPZh9BOjkQs.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing videos:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [05:12<02:38, 52.92s/video]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=wjZofJX0v4M\n",
            "[youtube] wjZofJX0v4M: Downloading webpage\n",
            "[youtube] wjZofJX0v4M: Downloading tv client config\n",
            "[youtube] wjZofJX0v4M: Downloading player 73381ccc-main\n",
            "[youtube] wjZofJX0v4M: Downloading tv player API JSON\n",
            "[youtube] wjZofJX0v4M: Downloading ios player API JSON\n",
            "[youtube] wjZofJX0v4M: Downloading m3u8 information\n",
            "[info] wjZofJX0v4M: Downloading 1 format(s): 251-3\n",
            "[download] Destination: downloads/wjZofJX0v4M.webm\n",
            "[download] 100% of   27.39MiB in 00:00:00 at 36.89MiB/s  \n",
            "[ExtractAudio] Destination: downloads/wjZofJX0v4M.mp3\n",
            "Deleting original file downloads/wjZofJX0v4M.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing videos:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [07:11<02:30, 75.43s/video]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=eMlx5fFNoYc\n",
            "[youtube] eMlx5fFNoYc: Downloading webpage\n",
            "[youtube] eMlx5fFNoYc: Downloading tv client config\n",
            "[youtube] eMlx5fFNoYc: Downloading player 73381ccc-main\n",
            "[youtube] eMlx5fFNoYc: Downloading tv player API JSON\n",
            "[youtube] eMlx5fFNoYc: Downloading ios player API JSON\n",
            "[youtube] eMlx5fFNoYc: Downloading m3u8 information\n",
            "[info] eMlx5fFNoYc: Downloading 1 format(s): 251-3\n",
            "[download] Destination: downloads/eMlx5fFNoYc.webm\n",
            "[download] 100% of   26.38MiB in 00:00:00 at 35.89MiB/s  \n",
            "[ExtractAudio] Destination: downloads/eMlx5fFNoYc.mp3\n",
            "Deleting original file downloads/eMlx5fFNoYc.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing videos:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [09:06<01:28, 88.34s/video]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=9-Jl0dxWQs8\n",
            "[youtube] 9-Jl0dxWQs8: Downloading webpage\n",
            "[youtube] 9-Jl0dxWQs8: Downloading tv client config\n",
            "[youtube] 9-Jl0dxWQs8: Downloading player 73381ccc-main\n",
            "[youtube] 9-Jl0dxWQs8: Downloading tv player API JSON\n",
            "[youtube] 9-Jl0dxWQs8: Downloading ios player API JSON\n",
            "[youtube] 9-Jl0dxWQs8: Downloading m3u8 information\n",
            "[info] 9-Jl0dxWQs8: Downloading 1 format(s): 251-3\n",
            "[download] Destination: downloads/9-Jl0dxWQs8.webm\n",
            "[download] 100% of   23.48MiB in 00:00:00 at 29.82MiB/s  \n",
            "[ExtractAudio] Destination: downloads/9-Jl0dxWQs8.mp3\n",
            "Deleting original file downloads/9-Jl0dxWQs8.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [10:45<00:00, 80.71s/video]\n"
          ]
        }
      ],
      "source": [
        "# Fetch the playlist using yt-dlp\n",
        "with yt_dlp.YoutubeDL({'quiet': True}) as ydl:\n",
        "    playlist_info = ydl.extract_info(playlist_url, download=False)\n",
        "\n",
        "# If you get an Error in donwloading youtube vides, watch the video at the following link to see how you can\n",
        "# work with cookies and use the code below\n",
        "# https://www.youtube.com/watch?v=DsS1jCDZGek&t=26s\n",
        "\n",
        "# with yt_dlp.YoutubeDL({'cookiefile': '/content/cookies.txt', 'quiet': True}) as ydl:\n",
        "#     playlist_info = ydl.extract_info(playlist_url, download=False)\n",
        "\n",
        "video_urls = []\n",
        "if 'entries' in playlist_info:\n",
        "    video_urls = [entry['webpage_url'] for entry in playlist_info['entries'] if 'webpage_url' in entry]\n",
        "\n",
        "# Process all videos in the playlist to transcribe them\n",
        "for url in tqdm(video_urls, desc=\"Processing videos\", unit=\"video\"):\n",
        "    process_video(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJwJRUORMtoL"
      },
      "outputs": [],
      "source": [
        "# !yt-dlp --cookies /content/cookies.txt https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxtSDUm2iAVi"
      },
      "source": [
        "# 2 - Process Dataset into LangChain Documents\n",
        "\n",
        "In this step, we will begin by fetching the dataset from the **3Blue1Brown YouTube Neural Networks playlist**. This dataset consists of transcriptions of the videos, where each transcription is stored as a plaintext file. Each file begins with the YouTube URL and the video title, which we'll parse as metadata. The actual transcript content starts after the \"TRANSCRIPT\" separator.\n",
        "\n",
        "We'll use LangChain to convert these transcriptions into documents, allowing us to later utilize them for retrieval-augmented generation (RAG) tasks in building our chatbot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJfoiQ88ifV7"
      },
      "source": [
        "We'll process each video transcription and load it into a LangChain **Document** object (https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/creating_documents). This object consists of two key attributes:\n",
        "\n",
        "- **page_content**: This contains the actual content of the transcript that we want to index and search semantically.\n",
        "- **metadata**: This includes any associated metadata, such as the video title and YouTube URL, which we can use to identify and retrieve specific documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0uD6MAod4ym",
        "outputId": "62faea54-6b80-4a2f-a502-41f16c362541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8 documents\n",
            "Metadata of first document: {'source': 'https://www.youtube.com/watch?v=tIeHLnjs5U8', 'title': 'Backpropagation calculus _ DL4'}\n",
            "Content of first document: The hard assumption here is that you've watched Part 3, giving an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the relevant calculus. It'\n"
          ]
        }
      ],
      "source": [
        "def process_txt_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "            # Extract URL, Title, and page content after \"TRANSCRIPT\"\n",
        "            url = lines[0].strip().split(\": \", 1)[1]\n",
        "            title = lines[1].strip().split(\": \", 1)[1]\n",
        "            page_content = ''.join(lines[3:]).strip()  # Assuming content starts from the 4th line\n",
        "\n",
        "            # Return a Document object with page content and metadata\n",
        "            return Document(page_content=page_content, metadata={'source': url, 'title': title})\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_documents_from_directory(directory_path):\n",
        "    documents = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            doc = process_txt_file(os.path.join(directory_path, filename))\n",
        "            if doc:\n",
        "                documents.append(doc)\n",
        "    return documents\n",
        "\n",
        "# Give the path of your directory where you saved the transcriptions\n",
        "directory_path = 'transcriptions'  # Replace with your actual directory path\n",
        "docs = create_documents_from_directory(directory_path)\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "print(\"Metadata of first document:\", docs[0].metadata)\n",
        "print(\"Content of first document:\", docs[0].page_content[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-28h5DhDi6He"
      },
      "source": [
        "# 3 - Splitting the Documents into Chunks\n",
        "\n",
        "In this step, we will split the transcripts into smaller, manageable chunks. This is important because large documents can be inefficient for semantic search and retrieval. By breaking them down, we can increase the accuracy and efficiency of our retrieval-augmented generation (RAG) model.\n",
        "\n",
        "Each chunk will contain a segment of the transcript, ensuring that the content remains coherent and relevant for further processing in the LangChain framework.\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/5.3%20-%20RAG/chunks.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z83r8e4fL1Gz",
        "outputId": "4090f512-ce60-43b5-b0cc-7743e5afd685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 300\n",
            "Metadata of first chunk: {'source': 'https://www.youtube.com/watch?v=tIeHLnjs5U8', 'title': 'Backpropagation calculus _ DL4', 'start_index': 0}\n",
            "Content of first chunk: The hard assumption here is that you've watched Part 3, giving an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the relevant calculus. It's normal for this to be at least a little confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has kind of a different feel from how most introductory calculus courses approach the subject. For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. Let's just start off\n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200, add_start_index=True)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(\"Number of chunks:\", len(all_splits))\n",
        "print(\"Metadata of first chunk:\", all_splits[0].metadata)\n",
        "print(\"Content of first chunk:\", all_splits[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTJ6RctsjFVp"
      },
      "source": [
        "# 4 - Embedding the Chunks and Loading into a Vector Database\n",
        "\n",
        "This step is crucial for enabling semantic search over the transcript data. The goal is to convert the text chunks into numerical representations (embeddings) that can be indexed and searched efficiently. These embeddings will allow us to retrieve relevant context for generating answers to user queries.\n",
        "\n",
        "### **BGE Embeddings**\n",
        "\n",
        "- **BGE (Beijing General Embeddings)**: BGE models available on HuggingFace are some of the best-performing open-source embedding models. BGE models transform text into dense vectors (embeddings), which capture the semantic meaning of the text. The BGE model we will use is designed to be highly efficient and accurate in understanding the relationships between text data, making it ideal for our use case of semantic search.\n",
        "  \n",
        "  For more information about the BGE model, you can visit [HuggingFace BGE](https://huggingface.co/BAAI/bge-large-en).\n",
        "\n",
        "### **Chroma**\n",
        "\n",
        "- **Chroma**: Chroma is an open-source vector database specifically designed for managing and querying embeddings. Once the text chunks are converted into embeddings, they are stored in Chroma, where they can be quickly retrieved for semantic search tasks. Chroma integrates seamlessly with LangChain, making it a great choice for building AI applications that require fast and accurate retrieval of relevant information.\n",
        "\n",
        "  Chroma runs directly on your machine, allowing you to easily get started without requiring complex cloud services. It supports features like vector search, similarity queries, and more.\n",
        "\n",
        "  Check out a more comprehensive list of vector databases [here](https://www.datacamp.com/blog/the-top-5-vector-databases).\n",
        "\n",
        "### Embedding Process and Visualization\n",
        "\n",
        "In the diagram below, you can see the process of embedding text chunks into numerical representations, and then storing these embeddings in a vector database (Chroma). This allows us to perform efficient semantic searches later on, improving the quality of our chatbot's responses.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/5.3%20-%20RAG/vector-store.png)\n",
        "\n",
        "By the end of this step, we'll have a vector store of embedded text chunks, ready to be used for retrieving relevant context based on a given query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zDzhNakdfb_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "afede7beef5b42d180a542bd964834e5",
            "15269081ec00482dbce6b7f00b0f270d",
            "5f5b2f331f194df8a24326cf86af9ea0",
            "091a97f89f064280ada55f2426f2a817",
            "c5a5e4667db14002ab76f6680f98eaa6",
            "bfc341c6259744a1a0793fd02d678ac8",
            "72e53132a4bf4d278d0225d336dbdfdf",
            "c53a584e42554d9c85bc7bf1f05920f9",
            "eaf78000b7a04b91aded555eacb3a9f2",
            "c14daad888b9427d9cb21617c40adb9d",
            "249e95d64ec6437784204ff854b9bb12",
            "ed49078acfa24144bac00ac286fda058",
            "07c8e1904e6c4536aa4e3f17977b9519",
            "ab6ffa0031eb40b0a4f17a91c9c73312",
            "e0691dd7eb314550855ece83be3a5e3e",
            "2d8e66d3065d49b0ac34d3d7a405640f",
            "77d079abfa74479f8dc846d381fe0c71",
            "15d5c14398054885ad7f19a28026e709",
            "43b11d270eba4566b91cf5ad52da7efb",
            "b87d6171a98d4224ac987fee580ba0ce",
            "8afca47dab7142f19ee812bac0d0a6a4",
            "1d17f86752d544778e96aad3484b8f8b",
            "fae9a33752244eebbb7ea6b58192bf93",
            "831d11b5813144e5bbe9314634cdd056",
            "6317712cd28c45e6a81b654fa67750ea",
            "b8f118efd5ef406c9acb9845a3731290",
            "b8f71a543c2e4e2bb94d854f9640a57b",
            "a3f59877cc604f2aaff3d8459ec45ec6",
            "177ae749f7db4c0c8e7202a5578cfe93",
            "78a2808da09b4ffe8d1fd0a807253a06",
            "723b09ee739d402f9737763322c74d2d",
            "286c3c3e09754ebbaa56234d0525cd4d",
            "5883404f0dae461e9434d485898b28d4",
            "28b790a576c24395adb668efe6d7896a",
            "be181be21b37411bb5f4073348df1cf5",
            "a288e3d2f05944bab12abf1fa45cf57a",
            "1d60c970da224435980d3fd8129c3ee3",
            "2b58f7ee6ffb45eeba73c8815aeb9613",
            "b7cfd11f00cc4652ab3bc735d0fa5363",
            "b38314a06b3c4792bcd7b448a0ef3b93",
            "6aabe0234473471687683015de8198b7",
            "3ee007f6fffb484096c0f8aa2d45093c",
            "131b34524e6f4bc49bf6320dc4504c0c",
            "29e41c6a781a465cb523b9f2e9495066",
            "ff5fc402d415469a91af8e2e97300be1",
            "67a6eb17b5da4cbbbac3a777126b9368",
            "514b9d4ca8de46e3ba4ce71d0af1eeb9",
            "de02a76c9a114251960f64a0cd6267db",
            "bd2de18e8d5547b19a12d6c27c026f51",
            "ba38f64a0c0043ee86bd0dfa53f2b9d5",
            "fce9cce645264628a22143aed95fd416",
            "1fd29a410a6c4110ab566c34fe196946",
            "4692a8f8a3f14d5da7b78bfc9e1238d7",
            "c15ba2f6ac1f44c9973faa30b6e2f2d9",
            "ac8a755343144a6588733838bce53585",
            "b50021544ba148d9bdf28320ae756b36",
            "3e2374699dfb48f3801bc107c92251ad",
            "93a5e9ea029c4043ba908ac552066adc",
            "ffd256cdad164b35b28647bd92896420",
            "7a5ca52007754b4baa9b5414ab346da6",
            "309d14f9f73c4f46a6430b03c3ef8f63",
            "48f52f8f33b4459ab2b6a08909038d73",
            "86995a783ba847e397bc14d6c36063ac",
            "96739e945b694088bd8b86af223b6667",
            "c19b50b88d074cddaacd2f61e76b3dbb",
            "fb9963daa6a148c78573c4aa83f9d5d8",
            "532607063cf04320a6a282f644fca438",
            "9edba58c82c34c90a3ec8ae51f551e07",
            "83fe066cfa964447ab8f7ab311fcc820",
            "49e4fac0bb4d4cb3a001ce483dd62411",
            "4dfd5c7c6a9e44ba80123d2c9839de9a",
            "34a770703bed4b0394466a631d8bbcb6",
            "6cf85aab0abd463c959b8a174fa7116b",
            "df76689d4b7b4727a52f8c0ca748fb03",
            "17de5066bda844e7b47da10b04bb8c26",
            "f840b3a4fd174a498f2d95d8fb2fa5f7",
            "b7f2562edbb147c3bb8a65c7f8c85fb5",
            "d0c066b52d94493289473ee98dfa0282",
            "197e0495377c4e7fbf45076a5474055d",
            "0f32f69418754162acbc350410ebc454",
            "06be9fa6a4484e6fb45d5adfe385f642",
            "3b15d1cca9074a78aeddeb5db1084a7c",
            "8c2ccb35a7cb4db29838d2c4f9b37eb7",
            "c90276d5ed9e409b85e062fae77aeec2",
            "75f63fc784b541d58c77aa5d28e2cee9",
            "4be6ffab9e9b44dd96b9eaf29c56d9a2",
            "d6c59845e82c44c789ffa2933f6ed93d",
            "5ec617bf3ec548b8bc72c7b6299fb2b9",
            "acde75cef7cd4c489f479244309f1839",
            "9b330b33328a420b8bd7683666129c62",
            "1612205dd8d748698f1c6f0f53c42c32",
            "09f771dfdbdd41af99445fce72cf31a9",
            "34f4e44a754e47e19f3ccbb934a4690b",
            "e3c0a957187645fca7de1b06284cc0ab",
            "82947310a26647deb1f11ec2e19ea958",
            "fc039026db9b4589b90ed5174ca26f85",
            "c572528ddc9841c29420523ced1e0f31",
            "7335efdc79c240d1856396fe865cb95c",
            "7d19e0d732ed47ad837a323061aa212b",
            "bdf7a653af6743108a444763f781778b",
            "b5404673346a40aa8d7356d7dccd51c2",
            "8ca167b2b0db48bb926916a65596e4b7",
            "b79231fae1384628809ecb68d4310e1f",
            "8cbf05461eb34a18bc5c600845ac264c",
            "52f9fa59fa794350af6b3ca281849fad",
            "3c0ed2d4c14b47378f8443c86af04758",
            "b5c35468e43f49329d884d8c561d2d9b",
            "afbef1111f244868b3147ad0c8132b73",
            "8a8a709d3d6a4320a91096691054f301",
            "7ca1559f346948d79840cbbff65770f3",
            "67180b30b4244ed096ed332975bdbfc1",
            "8978e23552de45a3b8e004bc4299ee37",
            "21b05f6a67b44d88a81c38957b59b5dc",
            "eb40d295b1174f39a575d929fd8124b1",
            "ad9f78b024264757aa44be6f75d07eb9",
            "406e7c2afde040a38d48089a7b2830e9",
            "f987889727f44df0adf51484b42a11d9",
            "39869865c0f34831be670dc97311125c",
            "db6911111853488aa4b47f2c94680bf5",
            "f097580494dc458cbc7021cd6c0459ea",
            "2dbeba8785b94ab083c3f58a458ff0a9"
          ]
        },
        "outputId": "fad63a70-d586-4d31-d5c4-01e66ef5401e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-7d0ca243e0b6>:4: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  bge_embeddings = HuggingFaceBgeEmbeddings(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afede7beef5b42d180a542bd964834e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed49078acfa24144bac00ac286fda058"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/90.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fae9a33752244eebbb7ea6b58192bf93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28b790a576c24395adb668efe6d7896a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff5fc402d415469a91af8e2e97300be1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b50021544ba148d9bdf28320ae756b36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "532607063cf04320a6a282f644fca438"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0c066b52d94493289473ee98dfa0282"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acde75cef7cd4c489f479244309f1839"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdf7a653af6743108a444763f781778b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67180b30b4244ed096ed332975bdbfc1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"BAAI/bge-base-en\"\n",
        "encode_kwargs = {'normalize_embeddings': True}  # set True to compute cosine similarity\n",
        "\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Complete the code here to define the vectorstore\n",
        "vectorstore = Chroma.from_documents(documents=docs, embedding=bge_embeddings)  # Replace docs with your documents\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "\n",
        "# Now the retriever can be used for similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjyJlbyycL4N",
        "outputId": "457f241e-3704-4a5b-835c-dd509f487ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total docs retrieved 5\n",
            "\n",
            "Document 1:\n",
            "Metadata: {'source': 'https://www.youtube.com/watch?v=Ilg3gGewQ5U', 'title': 'Backpropagation, intuitively _ DL3'}\n",
            "Content: Here we tackle back propagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walk-through for what the algorithm is actually doing without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. Here, we're doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as a tensor. I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. As a quick reminder for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted it to give, and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now, is that because thinking of the gradient vector as a direction in 13,000 dimensions is to put it lightly beyond the scope of our imaginations, there's another way you can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process I'm about to describe when you compute the negative gradient and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive. It's just that there's a lot of little adjustments getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through those effects that each training example is having on the weights and biases. Because the cost function involves averaging a certain cost per example, over all the tens of thousands of training examples, the way that we adjust the weights and biases for a single gradient descent step also depends on every single example, or rather, in principle it should, but for computational efficiency, we're going to do a little trick later to keep you from needing to hit every single example for every single step. In other case, right now, all we're going to do is focus our attention on one single example, this image of a two. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now, we can't directly change those activations. We only have influence on the weights and biases. But it is helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify the image as a two, we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. For example, the increase to that number two neurons activation is in a sense more important than the decrease to the number eight neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ray-lew. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing just on how the weights should be adjusted? Notice how the weights actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect, since those weights are multiplied by larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your butt. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, heavy in theory. Often summed up in the phrase, neurons that fire together, wire together. Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a two get more strongly linked to those firing when thinking about a two. To be clear, I really am not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together, wire together idea comes with a couple meaningful asterisks. But taken as a very loose analogy, I do find it interesting to note. Anyway, the third way that we can help increase this neuron's activation is by changing all the activations in the previous layer. Namely, if everything connected to that digit two neuron with a positive weight, got brighter, and if everything connected with a negative weight got dimmer, then that digit two neuron would become more active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now, of course, we cannot directly influence those activations. We only have control over the weights and biases. But just as with the last layer, it's helpful to just keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit two output neuron wants. Remember, we also want all of the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So the desire of this digit two neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer. Again, in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listen to what that two wanted, the network would ultimately be incentivized just to classify all images as a two. What you do is you go through this same back property for every other training example, recording how each of them would like to change the weights and the biases. And you average together those desired changes. This collection here of the average to nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get quantitatively precise about those nudges. But if you understood every change that I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what back propagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence of every single training example, every single gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a whole bunch of mini-batches. Let's say each one having 100 training examples. Then you compute a step according to the mini-batch. It's not going to be the actual gradient to the cost function, which depends on all of the training data, not this tiny subset. So it's not the most efficient step downhill. But each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speed up. If you would applaud the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's kind of a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases. Not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes that you get. But that's computationally slow, so instead you randomly subdivide the data into these mini batches and compute each step with respect to a mini batch. Repeatedly going through all of the mini batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network is going to end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing Backprop actually corresponds with something that you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the M-NIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data that you actually need, whether that's having people label tens of thousands of images or whatever other data type you might be dealing with.\n",
            "\n",
            "Document 2:\n",
            "Metadata: {'title': 'Gradient descent, how neural networks learn _ DL2', 'source': 'https://www.youtube.com/watch?v=IHZwWFHWa-w'}\n",
            "Content: Last video, I laid out the structure of a neural network. I'll give a quick recap here just so that it's fresh in our minds and then I have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well. Then after that, we're going to dig in a little more to how this particular network performs and what those hidden layers of neurons end up actually looking for. As a reminder, our goal here is the classic example of handwritten digit recognition, the Hello World of Neural Networks. These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons in the input layer of the network. And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer plus some special number called a bias. Then you compose that sum with some other function, like the sigmoid squishification or a ray-loo, the way that I walked through last video. In total, given the somewhat arbitrary choice of two hidden layers here with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does. And what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit. And remember, the motivation that we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits. So here we learn how the network learns. What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data. And the way we test that is that after you train the network, you show it more labeled data, that it's never seen before, and you see how accurately it classifies those new images. Fortunately for us, and what makes this such a common example to start with, is that the good people behind the M-NIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers that they're supposed to be. And it's provocative as it is to describe a machine as learning. Once you actually see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like, a calculus exercise. I mean, basically, it comes down to finding the minimum of a certain function. Remember, conceptually, we're thinking of each neuron as being connected to all of the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections. The bias is some indication of whether that neuron tends to be active or inactive. And to start things off, we're just going to initialize all of those weights and biases totally randomly. Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random. For example, you feed in this image of a three, and the output layer just looks like a mess. So what you do is you define a cost function, a way of telling the computer, no, bad computer, that output should have activations, which are zero for most neurons, but one for this neuron. What you gave me is utter trash. To say that a little more mathematically, what you do is add up the squares of the differences between each of those trash output activations and the value that you want them to have. And this is what we'll call the cost of a single training example. Notice, this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't really know what it's doing. So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal. This average cost is our measure for how lousy the network is and how bad the computer should feel. And that's a complicated thing. Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output. And in a sense, it's parameterized by all these weights and biases. Well the cost function is a layer of complexity on top of that. It takes as its input, those 13,000 or so weights and biases, and it spits out a single number describing how bad those weights and biases are. And the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data. That's a lot to think about. But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to change those weights and biases so that it gets better. To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output. How do you find an input that minimizes the value of this function? Circular students will know that you can sometimes figure out that minimum explicitly. But that's not always feasible for really complicated functions. Certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function. A more flexible tactic is to start at any all input and figure out which direction you should step to make that output lower. Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive and shift the input to the right if that slope is negative. If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function. And the image you might have in mind here is a ball rolling down a hill. And notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at. There's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function. That's going to carry over to our neural network case as well. And I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that kind of helps you from overshooting. Bumping up the complexity a bit, imagine instead a function with two inputs and one output. You might think of the input space as the x, y plane and the cost function as being graft as a surface above it. Now instead of asking about the slope of the function, you have to ask which direction should you step in this input space so as to decrease the output of the function most quickly. In other words, what's the downhill direction? And again, it's helpful to think of a ball rolling down that hill. Most of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, basically which direction should you step to increase the function most quickly. Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly. And even more than that, the length of this gradient vector is actually an indication for just how steep that steepest slope is. Now if you're unfamiliar with multivariable calculus and you want to learn more, check out some of the work that I did for Khan Academy on the topic. Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector. This vector that tells you what the downhill direction is and how steep it is. You'll be okay if that's all you know and you're not rock solid on the details. Because if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill and just repeat that over and over. It's the same basic idea for a function that has 13,000 inputs instead of two inputs. Imagine organizing all 13,000 weights and biases of our network into a giant column vector. The negative gradient of the cost function is just a vector. It's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function. And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values and more like an actual decision that we want it to make. It's important to remember, this cost function involves an average over all of the training data. So if you minimize it, it means it's a better performance on all of those samples. The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called back propagation. And it's what I'm going to be talking about next video. There I really want to take the time to walk through what exactly happens to each weight and each bias for a given piece of training data. Trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas. Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function. And notice, one consequence of that is that it's important for this cost function to have a nice smooth output so that we can find a local minimum by taking little steps down hill. This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way that biological neurons are. This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent. It's a way to converge toward some local minimum of a cost function, basically a valley in this graph. I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000-dimensional input space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to think about this. Each component of the negative gradient tells us two things. The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you which changes matter more. You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight. Some of these connections just matter more for our training data. So a way that you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck. This really is just another way of thinking about direction. To take a simpler example, if you have some function with two variables as an input, and you compute that it's gradient at some particular point, comes out as 3-1. Then on the one hand, you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly. But when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction. But another way to read that is to say that changes to this first variable have three times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck. Alright, let's zoom out and sum up where we are so far. The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted sums. The cost function is a layer of complexity on top of that. It takes the 13,000 weights and biases as inputs, and spits out a single measure of laziness based on the training examples. And the gradient of the cost function is one more layer of complexity still. It tells us what nudges to all of these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most. So, when you initialize the network with random weights and biases and adjust them many times based on this gradient descent process, how well does it actually perform on images that it's never seen before? Well the one that I've described here, with the two hidden layers of 16 neurons each, goes in mostly for aesthetic reasons. Well, it's not bad. It classifies about 96% of the new images that it sees correctly. And honestly, if you look at some of the examples that it messes up on, you kind of feel compelled to cut it a little slack. Now, if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%. And that's pretty good. It's not the best. You can certainly get better performance by getting more sophisticated than this plain vanilla network. But given how daunting the initial task is, I just think there's something incredible about any network doing this well on images that it's never seen before, given that we never specifically told it what patterns to look for. Originally, the way that I motivated this structure was by describing a hope that we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops in longer lines, and that those might be piece together to recognize digits. So is this what our network is actually doing? Well, for this one, at least, not at all. Remember how last video we looked at how the weights of the connections from all of the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that that second layer neuron is picking up on? Well, when we actually do that, for the weights associated with these transitions from the first layer to the next. Instead of picking up on isolated little edges here and there, they look almost random, just with some very loose patterns in the middle there. It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns that we might have hoped for. And to really drive this point home, watch what happens when you input a random image. If the system was smart, you might expect it to either feel uncertain, maybe, not really activating any of those 10 output neurons or activating them all evenly. But instead, it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as it does that an actual image of a 5 is a 5. Fraze differently? Even if this network can recognize digits pretty well, it has no idea how to draw them. A lot of this is because it's such a tightly constrained training setup. I mean, put yourself in the network's shoes here. From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid. And its cost function just never gave it any incentive to be anything but utterly confident in its decisions. So with this is the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. That's just not at all what it ends up doing. Well this is not meant to be our end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s and 90s. And you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems. But the more you dig in to what those hidden layers are really doing, the less intelligent it seems. Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow. One pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns. But better than that, to actually engage with the material, I highly recommend the book by Michael Neilsen on deep learning and neural networks. In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing. What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Neilsen's efforts. I've also linked a couple other resources that I like a lot in the description, including the phenomenal and beautiful blog posts by Chris Ola and the articles in Distill. To close things off here for the last few minutes, I want to jump back into a snippet of the interview that I had with Lisha Lee. You might remember her from the last video, she did her PhD work in deep learning. And in this little snippet, she talks about two recent papers that really dig in to how some of the more modern image recognition networks are actually learning. Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled data set, it shuffled all of the labels around before training. Obviously the testing accuracy here was going to be no better than random, since everything's just randomly labeled. But it was still able to achieve the same training accuracy as you would on a properly labeled data set. Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which kind of raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization? It reminds the entire data set of what the correct classification is. And so a couple of half a year later at ICML this year, there was not exactly rebuttal paper, paper that addressed some aspects of like, hey, actually these networks are doing something a little bit smarter than that. If you look at that accuracy curve, if you were just training on a random data set, that curve sort of went down very slowly in almost a linear fashion. So you're really struggling to find that local minimum of possible, the right weights that would get you that accuracy. Whereas if you're actually training on a structure data set, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level. And so in some sense, it was easier to find that local maxima. And so it was also interesting about that, is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers. But one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality. So in some sense, if your data set a structure, you should be able to find that much more easily. My thanks, as always, to those of you supporting on Patreon. I've said before just what a game changer in Patreon is, but these videos really would not be possible without you. I also want to give a special thanks to the VC firm Amplify Partners in their support of these initial videos in the series.\n",
            "\n",
            "Document 3:\n",
            "Metadata: {'source': 'https://www.youtube.com/watch?v=tIeHLnjs5U8', 'title': 'Backpropagation calculus _ DL4'}\n",
            "Content: The hard assumption here is that you've watched Part 3, giving an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the relevant calculus. It's normal for this to be at least a little confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has kind of a different feel from how most introductory calculus courses approach the subject. For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. Let's just start off with an extremely simple network, one where each layer has a single neuron in it. So this particular network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. That way we know which adjustments to those terms is going to cause the most efficient decrease to the cost function. And we're just going to focus on the connection between the last two neurons. Let's label the activation of that last neuron with a superscript L indicating which layer it's in. So the activation of the previous neuron is a L minus 1. These are not exponents, they're just a way of indexing what we're talking about since I want to save subscripts for different indices later on. Now let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. So the cost of this simple network for a single training example is AL minus y squared. We'll denote the cost of that one training example as C0. As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neurons activation plus some bias, which I'll call BL. And then you pump that through some special nonlinear function like the sigmoid or a ray loop. It's actually going to make things easier for us if we give a special name to this weighted sum, like Z, with the same superscript as the relevant activations. So this is a lot of terms and a way that you might conceptualize it is that the weight, the previous action and the bias altogether are used to compute Z, which in turn lets us compute A, which finally, along with a constant Y, lets us compute the cost. And of course, AL minus 1 is influenced by its own weight and bias and such. But we're not going to focus on that right now. Now all of these are just numbers, right? And it can be nice to think of each one as having its own little number line. Our first goal is to understand how sensitive the cost function is to small changes in our weight, WL. Never phrase differently. What is the derivative of C with respect to WL? When you see this Dell W term, think of it as meaning some tiny nudge to W, like a change by 0.01. And think of this Dell C term as meaning whatever the resulting nudge to the cost is. What we want is their ratio. Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost. So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is the derivative of ZL with respect to WL. Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to C and this intermediate nudge to AL. This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of C to small changes in WL. So on screen right now, there's kind of a lot of symbols, and take a moment to just make sure it's clear what they all are. Because now we're going to compute the relevant derivatives. The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this means that its size is proportional to the difference between the networks output and the thing that we want it to be. So if that output was very different, even slight changes stand to have a big impact on the final cost function. The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever non-linearity you choose to use. And the derivative of ZL with respect to WL, in this case comes out just to be AL minus 1. Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all actually mean. In the case of this last derivative, the amount that that small nudge to the weight influenced the last layer depends on how strong the previous neuron is. Remember, this is where that neurons that fire together wire together idea comes in. And all of this is the derivative with respect to WL only of the cost for a specific single training example. Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression that we found over all training examples. And of course that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases. But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. The sensitivity to the bias, for example, is almost identical. We just need to change out this del Z del W term for a del Z del B. And if you look at the relevant formula, that derivative comes out to be 1. Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. Namely, this initial derivative in the chain rule expression, the sensitivity of Z to the previous activation, comes out to be the weight WL. And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of. Because now, we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. And you might think that this is an overly simple example, since all layers just have one neuron, and that things are going to get exponentially more complicated for a real network. But honestly, not that much changes when we give the layers multiple neurons. Really it's just a few more indices to keep track of. Rather than the activation of a given layer simply being AL, it's also going to have a subscript, indicating which neuron of that layer it is. Let's go ahead and use the letter K to index the layer L minus 1, and J to index the layer L. For the cost, again, we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. That is, you take a sum over ALJ minus YJ squared. Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is. So let's call the weight of the edge connecting this Kth neuron to the Jth neuron, WLJK. Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix that I talked about in the Part 1 video. Just as before, it's still nice to give a name to the relevant weighted sum, like Z, so that the activation of the last layer is just your special function, like the sigmoid, applied to Z. You can kind of see what I mean, right, where all of these are essentially the same equations that we had before in the one neuron per layer case. It's just that it looks a little more complicated. And indeed, the chain ruled the rivetive expression, describing how sensitive the cost is to a specific weight, looks essentially the same. I'll leave it to you to pause and think about each of those terms if you want. What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L minus 1. In this case, the difference is that the neuron influences the cost function through multiple different paths. It is on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. And that, well, that's pretty much it. Once you know how sensitive the cost function is to the activations in this second to last layer, you can just repeat the process for all the weights and biases feeding into that layer. So pat yourself on the back. If all of this makes sense, you have now looked deep into the heart of back propagation, the workhorse behind how neural networks learn. These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around. So don't worry if it takes time for your mind to digest it all.\n",
            "\n",
            "Document 4:\n",
            "Metadata: {'title': 'How might LLMs store facts _ DL7', 'source': 'https://www.youtube.com/watch?v=9-Jl0dxWQs8'}\n",
            "Content: If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball. This would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport. And I think in general anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts. So a reasonable question you could ask is, how exactly does that work, and where do those facts live? Last December a few researchers from Google Deep Mind posted about work on this question, and they were using this specific example of matching athletes to their sports. And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short. In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI. In the most recent chapter we were focusing on a piece called Attention, and the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network. The computation here is actually relatively simple, especially when you compare it to attention. It boils down essentially to a pair of matrix multiplications with a simple something in between. However, interpreting what these computations are doing is exceedingly challenging. Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact. Specifically, it'll be storing the fact that Michael Jordan plays basketball. I should mention the layout here is inspired by a conversation I had with one of those deep-mind researchers, Neil Nanda. For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow. You and I have been studying a model that's trained to take in a piece of text and predict what comes next. That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers. This sequence of vectors then repeatedly passes through two kinds of operation. Attention, which allows the vectors to pass information between one another, and then the multi-layer perceptrons, the thing that we're going to dig into today. And also there's a certain normalization step in between. After the sequence of vectors has flowed through many many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words and the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next. One of the key ideas that I want you to have in your mind is that all of these vectors live in a very very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning. So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very very close to the corresponding feminine noun. In this sense, this particular direction encodes gender information. The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent. In a transformer, these vectors don't merely encode the meaning of a single word, though. As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge. Ultimately, each one needs to encode something far far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next. We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts. Like I said, the lesson here is going to center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball. Now this toy example is going to require that you and I make a couple of assumptions about that high-dimensional space. First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball. So specifically what I mean by this is if you look in the network and you plug out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name. Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction. And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one. Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan, or basketball. So let's say a vector is meant to represent the full name Michael Jordan, then its dot product with both of these directions would have to be one. Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names. With all of those as the assumptions, let's now dive into the meat of the lesson. What happens inside a multi-layer perceptron? You might think of this sequence of vectors flowing into the block, and remember each vector was originally associated with one of the tokens from the input text. What's going to happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end we'll get another vector with the same dimension. That other vector is going to get added to the original one that flowed in, and that sum is the result flowing out. This sequence of operations is something you apply to every vector in the sequence associated with every token in the input, and it all happens in parallel. In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing. And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them. When I say this block is going to encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes, first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what we'll add on to the vector in that position. The first step of this process looks like multiplying that vector by a very big matrix, no surprises there, this is deep learning, and this matrix like all of the other ones we've seen is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is. Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding. For example, suppose that very first row happened to equal this first name Michael direction that were presuming exists, that would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael and zero or negative otherwise. Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction. And for simplicity, let me go ahead and write that down as m plus j, then taking a dot product with this embedding E, things distribute really nicely, so it looks like m.e plus j.e and notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan and otherwise it would be one or something smaller than one. And that's just one row in this matrix. You might think of all of the other rows as in parallel asking some other kinds of questions probing at some other sorts of features of the vector being processed. Very often this step also involves adding another vector through the output, which is full of model parameters learned from data, this other vector is known as the bias. For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one. You might very reasonably ask why I would want you to assume that the model has learned this. And in a moment, you'll see why it's very clean and nice if we have a value here, which is positive, if and only if a vector encodes the full name Michael Jordan and otherwise it's zero or negative. The total number of rows in this matrix, which is something like the number of questions being asked in the case of GPT3, whose numbers we've been following is just under 50,000. In fact, it's exactly four times the number of dimensions in this embedding space. That's a design choice you could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware. Since this matrix full of weights maps us into a higher dimensional space, I'm going to give it the shorthand W up. I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram. At this point, a problem is that this operation is purely linear, but language is a very non-linear process. If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually. What you really want is a simple yes or no for the full name. So the next step is to pass this large intermediate vector through a very simple non-linear function. A common choice is one that takes all of the negative values and maps them to zero, and leaves all of the positive values unchanged. And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or Rayloo for short. Here's what the graph looks like. So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan, and zero or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value where all of the zero and negative values just get clipped to zero. So this output would be one for the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the behavior of an AND gate. Often models will use a slightly modified function that's called the J-LU, which has the same basic shape, it's just a bit smoother, but for our purposes, it's a little bit cleaner if we only think about the Rayloo. Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here. Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple, termwise non-linear function like a Rayloo. You would say that this neuron is active whenever this value is positive, and that it's inactive if that value is zero. The next step looks very similar to the first one. You multiply by a very large matrix and you add on a certain bias term. In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm going to go ahead and call this the down projection matrix. And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column. You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns. The reason it's nicer to think about this way is because here, the columns have the same dimension as the embedding space, so we can think of them as directions in that space. For instance, we will imagine that the model has learned to make that first column into this basket ball direction that we suppose exists. What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result. But if that neuron was inactive, if that number was zero, then this would have no effect. And it doesn't just have to be basketball. The model could also bake into this column many other features that it wants to associate with something that has the full name Michael Jordan. And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active. And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values. You might wonder what's that doing, as with all parameter field objects here, it's kind of hard to say exactly, maybe there's some book keeping that the network needs to do, but you can feel free to ignore it for now. Making our notation a little more compact again, I'll call this big matrix W down, and similarly call that bias vector B down and put that back into our diagram. Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position, and that gets you this final result. So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction. So what pops out will encode all of those together. And remember, this is a process happening to every one of those vectors in parallel. In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input. So that is the entire operation, two matrix products each with a bias added, and a simple clipping function in between. Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there. In that example, it was trained to recognize handwritten digits. Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture, and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high dimensional embedding space. That is the core lesson, but I do want to step back and reflect on two different things. The first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into Transformers. In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live. So let's quickly finish up the game here. I already mentioned how this up projection matrix has just under 50,000 rows, and that each row matches the size of the embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transpose to shape. So together, they give about 1.2 billion parameters. The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even going to show it. In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion. This is around two-thirds of the total parameters in the network, and when you add it to everything that we had before for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised. It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total. As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real-large language models. It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction. It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active. Both of those are just mathematical facts. However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan. And there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition. This is a hypothesis that might help to explain both why the models are especially hard to interpret, and also why they scale surprisingly well. The basic idea is that if you have an end-dimensional space and you want to represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction it doesn't influence any of the other directions. Then the maximum number of vectors you can fit is only n, the number of dimensions. To a mathematician actually this is the definition of dimension, but where it gets interesting is if you relax that constraint a little bit and you tolerate some noise. Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart. If we were in two or three dimensions this makes no difference, that gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions the answer changes dramatically. I can give you a really quick and dirty illustration of this using some scrappy python that's going to create a list of 100 dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions. This plot right here shows the distribution of angles between pairs of these vectors, so because they started at random those angles could be anything from 0 to 180 degrees, but you'll notice that already even just for random vectors there's this heavy bias for things to be closer to 90 degrees. Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another. After repeating this many different times, here's what the distribution of angles looks like. We have to actually zoom in on it here, because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees. In general, a consequence of something known as the Johnson-Lindon-Strauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this, grows exponentially with the number of dimensions. This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions. It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted. This might partially explain why model performance seems to scale so well with size. A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas. And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multi-layer perceptron that we just studied. That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed. But if it was doing that, what it means is that individual features aren't going to be visible as a single neuron lighting up. It would have to look like some specific combination of neurons instead, a superposition. For any of you curious to learn more, a key relevant search term here is Sparce Autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're superimposed on all these neurons. I'll link to a couple really great anthropic posts all about this. At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points. The main thing that I want to cover in a next chapter is the training process. On the one hand, the short answer for how training works is that it's all back propagation, and we covered back propagation in a separate context with earlier chapters in the series. But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of scaling laws. Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sync my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time. So, I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one.\n",
            "\n",
            "Document 5:\n",
            "Metadata: {'source': 'https://www.youtube.com/watch?v=wjZofJX0v4M', 'title': 'Transformers (how LLMs work) explained visually _ DL5'}\n",
            "Content: The initials GPT stand for Generative Pre-Trained Transformer. So that first word is straightforward enough, these are bots that generate new text. Pre-trained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine tune it on specific tasks with additional training. But the last word, that's the real key piece. A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI. What I want to do with this video and the following chapters is go through a visually driven explanation for what actually happens inside a transformer. We're going to follow the data that flows through it and go step by step. There are many different kinds of models that you can build using transformers. Some models take in audio and produce a transcript. This sentence comes from a model going the other way around, producing synthetic speech just from text. All those tools that took the world by storm in 2022, like Dolly and Mid-Journey that take in a text description and produce an image are based on transformers. And even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible. And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another. But the variant that you and I will focus on, which is the type that underlies tools like chat GPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage. That prediction takes the form of a probability distribution over many different chunks of text that might follow. At first glance, you might think that predicting the next word feels like a very different goal from generating new text. But once you have a prediction model like this, a simple thing you could try to make it generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added. I don't know about you, but it really doesn't feel like this should actually work. In this animation, for example, I'm running GPT2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text. And the story just doesn't actually really make that much sense. But if I swap it out for API calls to GPT3 instead, which is the same basic model just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pie creature would live in a land of math and computation. This process here of repeated prediction and sampling is essentially what's happening when you interact with chat GPT or any of these other large language models, and you see them producing one word at a time. In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses. Let's kick things off with a very high level preview of how data flows through a transformer. We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood. First, the input is broken up into a bunch of little pieces. These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations. If images or sound are involved, then tokens could be little patches of that image, or little chunks of that sound. Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece. If you think of these vectors as giving coordinates in some very high-dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space. This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values. For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model. The attention block is what's responsible for figuring out which words in the context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated. And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors. After that, these vectors pass through a different kind of operation, and depending on the source that you're reading, this will be referred to as a multi-layer perceptron, or maybe a feed-forward layer, and here the vectors don't talk to each other. They all go through the same operation in parallel, and while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions. All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices. I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview. After that, the process essentially repeats. You go back and forth between attention blocks and multi-layer perceptron blocks. Until at the very end, the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence. We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next. And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over. Some of you in the know may remember how long before chat GPT came into the scene, this is what early demos of GPT3 looked like. You would have it auto-complete stories and essays based on an initial snippet. To make a tool like this into a chat bot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response. There is more to say about an added step of training that's required to make this work well, but at a high level this is the general idea. In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around. If you're comfortable with that background knowledge and a little impatient, you could probably feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer. After that, I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point. For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformer specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning. At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves. What I mean by that is let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word, given a passage of text, or any other task that seems to require some element of intuition and pattern recognition. We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior. For example, maybe the simplest form of machine learning is linear regression, where your inputs and your outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices. That line is described by two continuous parameters, say the slope and the y intercept, and the goal of linear regression, is to determine those parameters to closely match the data. Needless to say, deep learning models get much more complicated, GPT-3, for example, has not two, but 175 billion parameters. But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data, or being completely intractable to train. Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well. What unifies them is that they all use the same training algorithm, it's called back propagation, we talked about it in previous chapters, and the context that I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format, and if you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling kind of arbitrary. First, whatever kind of model you're making, the input has to be formatted as an array of real numbers. This could simply mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor. You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output. For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens. In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums. You also sprinkle some nonlinear functions throughout, but they won't depend on parameters. Typically, though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product. It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum. It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters, that transform vectors that are drawn from the data being processed. For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices. Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does. As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from. Even if nowadays there are bigger and better models, this one has a certain charm as the first large language model to really capture the world's attention outside of ML communities. Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks. I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like JGPT, almost all of the actual computation looks like matrix vector multiplication. There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray. The weights are the actual brains. They are the things learned during training, and they determine how it behaves. The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text. With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors. I mentioned how those chunks are called tokens, which might be pieces of words or punctuation. But every now and then in this chapter, and especially in the next one, I'd like to just pretend that it's broken more cleanly into words, because we humans think in words, this'll just make it much easier to reference little examples and clarify each step. The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words. These columns are what determines what vector each word turns into in that first step. We label it WE, and like all the matrices we see, its values begin random, but they're going to be learned based on data. Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sits the foundation for everything that follows, so let's take a moment to get familiar with it. We often call this embedding a word, which invites you to think of these vectors very geometrically, as points in some high dimensional space. Visualizing a list of three numbers, as coordinates for points in 3D space, would be no problem, but word embeddings tend to be much, much higher dimensional. In GPT-3, they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions. In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results. The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning. For the simple word to vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes, and if you want to pull up some python and play along at home, this is the specific model that I'm using to make the animations. It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning. A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector in the space, connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen. So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point. At least, kind of, despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way that queen is used in training data is not merely a feminine version of king. When I played around family relations seem to illustrate the idea much better. The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information. Another example is that if you take the embedding of Italy and you subtract the embedding of Germany and then you add that to the embedding of Hitler, you get something very close to the embedding of Mussolini. It's as if the model learned to associate some directions with Italianness and others with World War II Axis leaders. Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan and you add it to sushi, you end up very close to broadwurst. Also in playing this game of finding nearest neighbors, I was very pleased to see how close cat was to both beast and monster. One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align. Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good since so much of our computation has to look like weighted sums. Geometrically, the dot product is positive when vectors point in similar directions. It's zero if they're perpendicular and it's negative whenever they point in opposite directions. For example, let's say you were playing with this model and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space. To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns and compare it to the dot products with the corresponding plural nouns. If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction. It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3 and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word. Again, the specifics for how words get embedded is learned using data. This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model, and using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se but of tokens, and the embedding dimension is 12,288. Multiplying those tells us this consists of about 617 million weights. Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion. In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words. For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context. A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language. Think about your own understanding of a given word. The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently. To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings. But you should think of the primary goal of this network that it flows through, as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent. The network can only process a fixed number of vectors at a time, known as its context size. For GPT-3, it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions. This context size limits how much text the transformer can incorporate when it's making a prediction of the next word. This is why long conversations with certain chatbots, like the early versions of chat GPT, often gave the feeling of the bot kind of losing the threat of conversation as you continued too long. We'll go into the details of attention in due time, but skipping ahead, I want to talk for a minute about what happens at the very end. Remember, the desired output is a probability distribution over all tokens that might come next. For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately proceeding, we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape. This involves two different steps. The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary. Then there's a function that normalizes this into a probability distribution. It's called Softmax, and we'll talk more about it in just a second, but before that, it might seem a little bit weird to only use this last embedding to make a prediction. When, after all, in that last step, there are thousands of other vectors in the layer just sitting there with their own context-rich meanings. This has to do with the fact that in the training process, it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it. There's a lot more to be said about training later on, but I just want to call that out right now. This matrix is called the Unimbedding Matrix, and we give it the label WU. Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process. Keeping score on our total parameter count, this unimbedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension. It's very similar to the embedding matrix just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far, is a little over a billion. A small, but not wholly insignificant fraction of the 175 billion that we'll end up with in total. As the very last mini-lesson for this chapter, I want to talk more about the softmax function, since it makes another appearance for us once we dive into the attention blocks. The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1. However, if you're playing the deep learning game, where everything you do looks like matrix vector multiplication, the outputs that you get by defaults don't abide by this at all. The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1. Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution, in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0. That's all you really need to know, but if you're curious, the way that it works, is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values, and divide each term by that sum, which normalizes it into a list that adds up to 1. You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output, the corresponding term dominates the distribution, so if you are sampling from it, you'd almost certainly just be picking the maximizing input, but it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs. In some situations, like when ChatchyPt is using this distribution to create a next word, there's room for a little bit of extra fun, by adding a little extra spice into this function, with a constant T thrown into the denominator of those exponents. We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when T is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if T is smaller, then the bigger values will dominate more aggressively. We're in the extreme, setting T equal to 0 means all of the weight goes to that maximum value. For example, I'll have GPT-3 generate a story with the seed text once upon a time there was A, but I'm going to use different temperatures in each case. Temperature 0 means that it always goes with the most predictable word, and what you get ends up being kind of a trite derivative of Goldilocks. A higher temperature gives it a chance to choose less likely words, but it comes with a risk. In this case, the story starts out a bit more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense. Technically speaking, the API doesn't actually let you pick a temperature bigger than two. There is no mathematical reason for this. It's just an arbitrary constraint imposed, I suppose, to keep their tool from being seen generating things that are too nonsensical. So, if you're curious the way this animation is actually working, is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1-5th. As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm going to say logits. So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output, as the logits for the next word prediction. A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid Waxon Wax Off-Style. You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI should be relatively smooth. For that, come join me in the next chapter. As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters. A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review. In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting.\n"
          ]
        }
      ],
      "source": [
        "prompt_question = \"What is Gradient Descent?\"\n",
        "\n",
        "#based on the provided prompt, using retriever, retrieve the relevant docs from vectorDB which matches best (highest similarity) with the prompt.\n",
        "# use get_relevant_documents()\n",
        "retrieved_docs = retriever.get_relevant_documents(prompt_question) ##CODE HERE\n",
        "\n",
        "print(\"Total docs retrieved\",len(retrieved_docs))\n",
        "\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(\"Metadata:\", doc.metadata)\n",
        "    print(\"Content:\", doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFrNLjY2kgRX"
      },
      "source": [
        "# 5 - Full RAG Chain\n",
        "\n",
        "Let's now put everything together to build a fully functional RAG chain using Lanchain Expression Language -> https://python.langchain.com/docs/expression_language/.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/5.3%20-%20RAG/retrieval.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "QSP8ofy8etse",
        "outputId": "027ac7eb-0b48-4893-ebdd-2010469610ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is Gradient Descent? \\nContext: Here we tackle back propagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walk-through for what the algorithm is actually doing without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. Here, we're doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as a tensor. I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. As a quick reminder for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted it to give, and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now, is that because thinking of the gradient vector as a direction in 13,000 dimensions is to put it lightly beyond the scope of our imaginations, there's another way you can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process I'm about to describe when you compute the negative gradient and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive. It's just that there's a lot of little adjustments getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through those effects that each training example is having on the weights and biases. Because the cost function involves averaging a certain cost per example, over all the tens of thousands of training examples, the way that we adjust the weights and biases for a single gradient descent step also depends on every single example, or rather, in principle it should, but for computational efficiency, we're going to do a little trick later to keep you from needing to hit every single example for every single step. In other case, right now, all we're going to do is focus our attention on one single example, this image of a two. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now, we can't directly change those activations. We only have influence on the weights and biases. But it is helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify the image as a two, we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. For example, the increase to that number two neurons activation is in a sense more important than the decrease to the number eight neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ray-lew. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing just on how the weights should be adjusted? Notice how the weights actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect, since those weights are multiplied by larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your butt. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, heavy in theory. Often summed up in the phrase, neurons that fire together, wire together. Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a two get more strongly linked to those firing when thinking about a two. To be clear, I really am not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together, wire together idea comes with a couple meaningful asterisks. But taken as a very loose analogy, I do find it interesting to note. Anyway, the third way that we can help increase this neuron's activation is by changing all the activations in the previous layer. Namely, if everything connected to that digit two neuron with a positive weight, got brighter, and if everything connected with a negative weight got dimmer, then that digit two neuron would become more active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now, of course, we cannot directly influence those activations. We only have control over the weights and biases. But just as with the last layer, it's helpful to just keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit two output neuron wants. Remember, we also want all of the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So the desire of this digit two neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer. Again, in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listen to what that two wanted, the network would ultimately be incentivized just to classify all images as a two. What you do is you go through this same back property for every other training example, recording how each of them would like to change the weights and the biases. And you average together those desired changes. This collection here of the average to nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get quantitatively precise about those nudges. But if you understood every change that I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what back propagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence of every single training example, every single gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a whole bunch of mini-batches. Let's say each one having 100 training examples. Then you compute a step according to the mini-batch. It's not going to be the actual gradient to the cost function, which depends on all of the training data, not this tiny subset. So it's not the most efficient step downhill. But each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speed up. If you would applaud the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's kind of a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases. Not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes that you get. But that's computationally slow, so instead you randomly subdivide the data into these mini batches and compute each step with respect to a mini batch. Repeatedly going through all of the mini batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network is going to end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing Backprop actually corresponds with something that you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the M-NIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data that you actually need, whether that's having people label tens of thousands of images or whatever other data type you might be dealing with.\\nLast video, I laid out the structure of a neural network. I'll give a quick recap here just so that it's fresh in our minds and then I have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well. Then after that, we're going to dig in a little more to how this particular network performs and what those hidden layers of neurons end up actually looking for. As a reminder, our goal here is the classic example of handwritten digit recognition, the Hello World of Neural Networks. These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons in the input layer of the network. And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer plus some special number called a bias. Then you compose that sum with some other function, like the sigmoid squishification or a ray-loo, the way that I walked through last video. In total, given the somewhat arbitrary choice of two hidden layers here with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does. And what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit. And remember, the motivation that we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits. So here we learn how the network learns. What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data. And the way we test that is that after you train the network, you show it more labeled data, that it's never seen before, and you see how accurately it classifies those new images. Fortunately for us, and what makes this such a common example to start with, is that the good people behind the M-NIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers that they're supposed to be. And it's provocative as it is to describe a machine as learning. Once you actually see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like, a calculus exercise. I mean, basically, it comes down to finding the minimum of a certain function. Remember, conceptually, we're thinking of each neuron as being connected to all of the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections. The bias is some indication of whether that neuron tends to be active or inactive. And to start things off, we're just going to initialize all of those weights and biases totally randomly. Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random. For example, you feed in this image of a three, and the output layer just looks like a mess. So what you do is you define a cost function, a way of telling the computer, no, bad computer, that output should have activations, which are zero for most neurons, but one for this neuron. What you gave me is utter trash. To say that a little more mathematically, what you do is add up the squares of the differences between each of those trash output activations and the value that you want them to have. And this is what we'll call the cost of a single training example. Notice, this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't really know what it's doing. So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal. This average cost is our measure for how lousy the network is and how bad the computer should feel. And that's a complicated thing. Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output. And in a sense, it's parameterized by all these weights and biases. Well the cost function is a layer of complexity on top of that. It takes as its input, those 13,000 or so weights and biases, and it spits out a single number describing how bad those weights and biases are. And the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data. That's a lot to think about. But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to change those weights and biases so that it gets better. To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output. How do you find an input that minimizes the value of this function? Circular students will know that you can sometimes figure out that minimum explicitly. But that's not always feasible for really complicated functions. Certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function. A more flexible tactic is to start at any all input and figure out which direction you should step to make that output lower. Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive and shift the input to the right if that slope is negative. If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function. And the image you might have in mind here is a ball rolling down a hill. And notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at. There's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function. That's going to carry over to our neural network case as well. And I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that kind of helps you from overshooting. Bumping up the complexity a bit, imagine instead a function with two inputs and one output. You might think of the input space as the x, y plane and the cost function as being graft as a surface above it. Now instead of asking about the slope of the function, you have to ask which direction should you step in this input space so as to decrease the output of the function most quickly. In other words, what's the downhill direction? And again, it's helpful to think of a ball rolling down that hill. Most of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, basically which direction should you step to increase the function most quickly. Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly. And even more than that, the length of this gradient vector is actually an indication for just how steep that steepest slope is. Now if you're unfamiliar with multivariable calculus and you want to learn more, check out some of the work that I did for Khan Academy on the topic. Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector. This vector that tells you what the downhill direction is and how steep it is. You'll be okay if that's all you know and you're not rock solid on the details. Because if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill and just repeat that over and over. It's the same basic idea for a function that has 13,000 inputs instead of two inputs. Imagine organizing all 13,000 weights and biases of our network into a giant column vector. The negative gradient of the cost function is just a vector. It's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function. And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values and more like an actual decision that we want it to make. It's important to remember, this cost function involves an average over all of the training data. So if you minimize it, it means it's a better performance on all of those samples. The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called back propagation. And it's what I'm going to be talking about next video. There I really want to take the time to walk through what exactly happens to each weight and each bias for a given piece of training data. Trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas. Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function. And notice, one consequence of that is that it's important for this cost function to have a nice smooth output so that we can find a local minimum by taking little steps down hill. This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way that biological neurons are. This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent. It's a way to converge toward some local minimum of a cost function, basically a valley in this graph. I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000-dimensional input space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to think about this. Each component of the negative gradient tells us two things. The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you which changes matter more. You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight. Some of these connections just matter more for our training data. So a way that you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck. This really is just another way of thinking about direction. To take a simpler example, if you have some function with two variables as an input, and you compute that it's gradient at some particular point, comes out as 3-1. Then on the one hand, you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly. But when you graph the function above the plane of input points, that vector is what's giving you the straight uphill direction. But another way to read that is to say that changes to this first variable have three times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck. Alright, let's zoom out and sum up where we are so far. The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted sums. The cost function is a layer of complexity on top of that. It takes the 13,000 weights and biases as inputs, and spits out a single measure of laziness based on the training examples. And the gradient of the cost function is one more layer of complexity still. It tells us what nudges to all of these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most. So, when you initialize the network with random weights and biases and adjust them many times based on this gradient descent process, how well does it actually perform on images that it's never seen before? Well the one that I've described here, with the two hidden layers of 16 neurons each, goes in mostly for aesthetic reasons. Well, it's not bad. It classifies about 96% of the new images that it sees correctly. And honestly, if you look at some of the examples that it messes up on, you kind of feel compelled to cut it a little slack. Now, if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%. And that's pretty good. It's not the best. You can certainly get better performance by getting more sophisticated than this plain vanilla network. But given how daunting the initial task is, I just think there's something incredible about any network doing this well on images that it's never seen before, given that we never specifically told it what patterns to look for. Originally, the way that I motivated this structure was by describing a hope that we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops in longer lines, and that those might be piece together to recognize digits. So is this what our network is actually doing? Well, for this one, at least, not at all. Remember how last video we looked at how the weights of the connections from all of the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that that second layer neuron is picking up on? Well, when we actually do that, for the weights associated with these transitions from the first layer to the next. Instead of picking up on isolated little edges here and there, they look almost random, just with some very loose patterns in the middle there. It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns that we might have hoped for. And to really drive this point home, watch what happens when you input a random image. If the system was smart, you might expect it to either feel uncertain, maybe, not really activating any of those 10 output neurons or activating them all evenly. But instead, it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as it does that an actual image of a 5 is a 5. Fraze differently? Even if this network can recognize digits pretty well, it has no idea how to draw them. A lot of this is because it's such a tightly constrained training setup. I mean, put yourself in the network's shoes here. From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid. And its cost function just never gave it any incentive to be anything but utterly confident in its decisions. So with this is the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. That's just not at all what it ends up doing. Well this is not meant to be our end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s and 90s. And you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems. But the more you dig in to what those hidden layers are really doing, the less intelligent it seems. Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow. One pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns. But better than that, to actually engage with the material, I highly recommend the book by Michael Neilsen on deep learning and neural networks. In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing. What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Neilsen's efforts. I've also linked a couple other resources that I like a lot in the description, including the phenomenal and beautiful blog posts by Chris Ola and the articles in Distill. To close things off here for the last few minutes, I want to jump back into a snippet of the interview that I had with Lisha Lee. You might remember her from the last video, she did her PhD work in deep learning. And in this little snippet, she talks about two recent papers that really dig in to how some of the more modern image recognition networks are actually learning. Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition, and instead of training it on a properly labeled data set, it shuffled all of the labels around before training. Obviously the testing accuracy here was going to be no better than random, since everything's just randomly labeled. But it was still able to achieve the same training accuracy as you would on a properly labeled data set. Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which kind of raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization? It reminds the entire data set of what the correct classification is. And so a couple of half a year later at ICML this year, there was not exactly rebuttal paper, paper that addressed some aspects of like, hey, actually these networks are doing something a little bit smarter than that. If you look at that accuracy curve, if you were just training on a random data set, that curve sort of went down very slowly in almost a linear fashion. So you're really struggling to find that local minimum of possible, the right weights that would get you that accuracy. Whereas if you're actually training on a structure data set, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level. And so in some sense, it was easier to find that local maxima. And so it was also interesting about that, is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers. But one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality. So in some sense, if your data set a structure, you should be able to find that much more easily. My thanks, as always, to those of you supporting on Patreon. I've said before just what a game changer in Patreon is, but these videos really would not be possible without you. I also want to give a special thanks to the VC firm Amplify Partners in their support of these initial videos in the series.\\nThe hard assumption here is that you've watched Part 3, giving an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the relevant calculus. It's normal for this to be at least a little confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has kind of a different feel from how most introductory calculus courses approach the subject. For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. Let's just start off with an extremely simple network, one where each layer has a single neuron in it. So this particular network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. That way we know which adjustments to those terms is going to cause the most efficient decrease to the cost function. And we're just going to focus on the connection between the last two neurons. Let's label the activation of that last neuron with a superscript L indicating which layer it's in. So the activation of the previous neuron is a L minus 1. These are not exponents, they're just a way of indexing what we're talking about since I want to save subscripts for different indices later on. Now let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. So the cost of this simple network for a single training example is AL minus y squared. We'll denote the cost of that one training example as C0. As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neurons activation plus some bias, which I'll call BL. And then you pump that through some special nonlinear function like the sigmoid or a ray loop. It's actually going to make things easier for us if we give a special name to this weighted sum, like Z, with the same superscript as the relevant activations. So this is a lot of terms and a way that you might conceptualize it is that the weight, the previous action and the bias altogether are used to compute Z, which in turn lets us compute A, which finally, along with a constant Y, lets us compute the cost. And of course, AL minus 1 is influenced by its own weight and bias and such. But we're not going to focus on that right now. Now all of these are just numbers, right? And it can be nice to think of each one as having its own little number line. Our first goal is to understand how sensitive the cost function is to small changes in our weight, WL. Never phrase differently. What is the derivative of C with respect to WL? When you see this Dell W term, think of it as meaning some tiny nudge to W, like a change by 0.01. And think of this Dell C term as meaning whatever the resulting nudge to the cost is. What we want is their ratio. Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost. So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is the derivative of ZL with respect to WL. Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to C and this intermediate nudge to AL. This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of C to small changes in WL. So on screen right now, there's kind of a lot of symbols, and take a moment to just make sure it's clear what they all are. Because now we're going to compute the relevant derivatives. The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this means that its size is proportional to the difference between the networks output and the thing that we want it to be. So if that output was very different, even slight changes stand to have a big impact on the final cost function. The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever non-linearity you choose to use. And the derivative of ZL with respect to WL, in this case comes out just to be AL minus 1. Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all actually mean. In the case of this last derivative, the amount that that small nudge to the weight influenced the last layer depends on how strong the previous neuron is. Remember, this is where that neurons that fire together wire together idea comes in. And all of this is the derivative with respect to WL only of the cost for a specific single training example. Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression that we found over all training examples. And of course that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases. But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. The sensitivity to the bias, for example, is almost identical. We just need to change out this del Z del W term for a del Z del B. And if you look at the relevant formula, that derivative comes out to be 1. Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. Namely, this initial derivative in the chain rule expression, the sensitivity of Z to the previous activation, comes out to be the weight WL. And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of. Because now, we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. And you might think that this is an overly simple example, since all layers just have one neuron, and that things are going to get exponentially more complicated for a real network. But honestly, not that much changes when we give the layers multiple neurons. Really it's just a few more indices to keep track of. Rather than the activation of a given layer simply being AL, it's also going to have a subscript, indicating which neuron of that layer it is. Let's go ahead and use the letter K to index the layer L minus 1, and J to index the layer L. For the cost, again, we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. That is, you take a sum over ALJ minus YJ squared. Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is. So let's call the weight of the edge connecting this Kth neuron to the Jth neuron, WLJK. Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix that I talked about in the Part 1 video. Just as before, it's still nice to give a name to the relevant weighted sum, like Z, so that the activation of the last layer is just your special function, like the sigmoid, applied to Z. You can kind of see what I mean, right, where all of these are essentially the same equations that we had before in the one neuron per layer case. It's just that it looks a little more complicated. And indeed, the chain ruled the rivetive expression, describing how sensitive the cost is to a specific weight, looks essentially the same. I'll leave it to you to pause and think about each of those terms if you want. What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L minus 1. In this case, the difference is that the neuron influences the cost function through multiple different paths. It is on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. And that, well, that's pretty much it. Once you know how sensitive the cost function is to the activations in this second to last layer, you can just repeat the process for all the weights and biases feeding into that layer. So pat yourself on the back. If all of this makes sense, you have now looked deep into the heart of back propagation, the workhorse behind how neural networks learn. These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around. So don't worry if it takes time for your mind to digest it all.\\nIf you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball. This would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport. And I think in general anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts. So a reasonable question you could ask is, how exactly does that work, and where do those facts live? Last December a few researchers from Google Deep Mind posted about work on this question, and they were using this specific example of matching athletes to their sports. And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short. In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI. In the most recent chapter we were focusing on a piece called Attention, and the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network. The computation here is actually relatively simple, especially when you compare it to attention. It boils down essentially to a pair of matrix multiplications with a simple something in between. However, interpreting what these computations are doing is exceedingly challenging. Our main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact. Specifically, it'll be storing the fact that Michael Jordan plays basketball. I should mention the layout here is inspired by a conversation I had with one of those deep-mind researchers, Neil Nanda. For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow. You and I have been studying a model that's trained to take in a piece of text and predict what comes next. That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers. This sequence of vectors then repeatedly passes through two kinds of operation. Attention, which allows the vectors to pass information between one another, and then the multi-layer perceptrons, the thing that we're going to dig into today. And also there's a certain normalization step in between. After the sequence of vectors has flowed through many many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words and the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next. One of the key ideas that I want you to have in your mind is that all of these vectors live in a very very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning. So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very very close to the corresponding feminine noun. In this sense, this particular direction encodes gender information. The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent. In a transformer, these vectors don't merely encode the meaning of a single word, though. As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's knowledge. Ultimately, each one needs to encode something far far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next. We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts. Like I said, the lesson here is going to center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball. Now this toy example is going to require that you and I make a couple of assumptions about that high-dimensional space. First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball. So specifically what I mean by this is if you look in the network and you plug out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name. Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction. And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one. Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan, or basketball. So let's say a vector is meant to represent the full name Michael Jordan, then its dot product with both of these directions would have to be one. Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names. With all of those as the assumptions, let's now dive into the meat of the lesson. What happens inside a multi-layer perceptron? You might think of this sequence of vectors flowing into the block, and remember each vector was originally associated with one of the tokens from the input text. What's going to happen is that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end we'll get another vector with the same dimension. That other vector is going to get added to the original one that flowed in, and that sum is the result flowing out. This sequence of operations is something you apply to every vector in the sequence associated with every token in the input, and it all happens in parallel. In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing. And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them. When I say this block is going to encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes, first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what we'll add on to the vector in that position. The first step of this process looks like multiplying that vector by a very big matrix, no surprises there, this is deep learning, and this matrix like all of the other ones we've seen is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is. Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector and taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding. For example, suppose that very first row happened to equal this first name Michael direction that were presuming exists, that would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael and zero or negative otherwise. Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction. And for simplicity, let me go ahead and write that down as m plus j, then taking a dot product with this embedding E, things distribute really nicely, so it looks like m.e plus j.e and notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan and otherwise it would be one or something smaller than one. And that's just one row in this matrix. You might think of all of the other rows as in parallel asking some other kinds of questions probing at some other sorts of features of the vector being processed. Very often this step also involves adding another vector through the output, which is full of model parameters learned from data, this other vector is known as the bias. For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one. You might very reasonably ask why I would want you to assume that the model has learned this. And in a moment, you'll see why it's very clean and nice if we have a value here, which is positive, if and only if a vector encodes the full name Michael Jordan and otherwise it's zero or negative. The total number of rows in this matrix, which is something like the number of questions being asked in the case of GPT3, whose numbers we've been following is just under 50,000. In fact, it's exactly four times the number of dimensions in this embedding space. That's a design choice you could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware. Since this matrix full of weights maps us into a higher dimensional space, I'm going to give it the shorthand W up. I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram. At this point, a problem is that this operation is purely linear, but language is a very non-linear process. If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually. What you really want is a simple yes or no for the full name. So the next step is to pass this large intermediate vector through a very simple non-linear function. A common choice is one that takes all of the negative values and maps them to zero, and leaves all of the positive values unchanged. And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or Rayloo for short. Here's what the graph looks like. So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan, and zero or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value where all of the zero and negative values just get clipped to zero. So this output would be one for the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the behavior of an AND gate. Often models will use a slightly modified function that's called the J-LU, which has the same basic shape, it's just a bit smoother, but for our purposes, it's a little bit cleaner if we only think about the Rayloo. Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here. Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple, termwise non-linear function like a Rayloo. You would say that this neuron is active whenever this value is positive, and that it's inactive if that value is zero. The next step looks very similar to the first one. You multiply by a very large matrix and you add on a certain bias term. In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm going to go ahead and call this the down projection matrix. And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column. You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns. The reason it's nicer to think about this way is because here, the columns have the same dimension as the embedding space, so we can think of them as directions in that space. For instance, we will imagine that the model has learned to make that first column into this basket ball direction that we suppose exists. What that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result. But if that neuron was inactive, if that number was zero, then this would have no effect. And it doesn't just have to be basketball. The model could also bake into this column many other features that it wants to associate with something that has the full name Michael Jordan. And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active. And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values. You might wonder what's that doing, as with all parameter field objects here, it's kind of hard to say exactly, maybe there's some book keeping that the network needs to do, but you can feel free to ignore it for now. Making our notation a little more compact again, I'll call this big matrix W down, and similarly call that bias vector B down and put that back into our diagram. Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position, and that gets you this final result. So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction. So what pops out will encode all of those together. And remember, this is a process happening to every one of those vectors in parallel. In particular, taking the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input. So that is the entire operation, two matrix products each with a bias added, and a simple clipping function in between. Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there. In that example, it was trained to recognize handwritten digits. Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture, and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high dimensional embedding space. That is the core lesson, but I do want to step back and reflect on two different things. The first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into Transformers. In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live. So let's quickly finish up the game here. I already mentioned how this up projection matrix has just under 50,000 rows, and that each row matches the size of the embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transpose to shape. So together, they give about 1.2 billion parameters. The bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even going to show it. In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion. This is around two-thirds of the total parameters in the network, and when you add it to everything that we had before for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised. It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total. As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts are actually stored in real-large language models. It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction. It's also true that the columns of that second matrix tell you what will be added to the result if that neuron is active. Both of those are just mathematical facts. However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan. And there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition. This is a hypothesis that might help to explain both why the models are especially hard to interpret, and also why they scale surprisingly well. The basic idea is that if you have an end-dimensional space and you want to represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction it doesn't influence any of the other directions. Then the maximum number of vectors you can fit is only n, the number of dimensions. To a mathematician actually this is the definition of dimension, but where it gets interesting is if you relax that constraint a little bit and you tolerate some noise. Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart. If we were in two or three dimensions this makes no difference, that gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions the answer changes dramatically. I can give you a really quick and dirty illustration of this using some scrappy python that's going to create a list of 100 dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions. This plot right here shows the distribution of angles between pairs of these vectors, so because they started at random those angles could be anything from 0 to 180 degrees, but you'll notice that already even just for random vectors there's this heavy bias for things to be closer to 90 degrees. Then what I'm going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another. After repeating this many different times, here's what the distribution of angles looks like. We have to actually zoom in on it here, because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees. In general, a consequence of something known as the Johnson-Lindon-Strauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this, grows exponentially with the number of dimensions. This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions. It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted. This might partially explain why model performance seems to scale so well with size. A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas. And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multi-layer perceptron that we just studied. That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed. But if it was doing that, what it means is that individual features aren't going to be visible as a single neuron lighting up. It would have to look like some specific combination of neurons instead, a superposition. For any of you curious to learn more, a key relevant search term here is Sparce Autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're superimposed on all these neurons. I'll link to a couple really great anthropic posts all about this. At this point, we haven't touched every detail of a transformer, but you and I have hit the most important points. The main thing that I want to cover in a next chapter is the training process. On the one hand, the short answer for how training works is that it's all back propagation, and we covered back propagation in a separate context with earlier chapters in the series. But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of scaling laws. Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sync my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due time. So, I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one.\\nThe initials GPT stand for Generative Pre-Trained Transformer. So that first word is straightforward enough, these are bots that generate new text. Pre-trained refers to how the model went through a process of learning from a massive amount of data, and the prefix insinuates that there's more room to fine tune it on specific tasks with additional training. But the last word, that's the real key piece. A transformer is a specific kind of neural network, a machine learning model, and it's the core invention underlying the current boom in AI. What I want to do with this video and the following chapters is go through a visually driven explanation for what actually happens inside a transformer. We're going to follow the data that flows through it and go step by step. There are many different kinds of models that you can build using transformers. Some models take in audio and produce a transcript. This sentence comes from a model going the other way around, producing synthetic speech just from text. All those tools that took the world by storm in 2022, like Dolly and Mid-Journey that take in a text description and produce an image are based on transformers. And even if I can't quite get it to understand what a pie creature is supposed to be, I'm still blown away that this kind of thing is even remotely possible. And the original transformer introduced in 2017 by Google was invented for the specific use case of translating text from one language into another. But the variant that you and I will focus on, which is the type that underlies tools like chat GPT, will be a model that's trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, and produce a prediction for what comes next in the passage. That prediction takes the form of a probability distribution over many different chunks of text that might follow. At first glance, you might think that predicting the next word feels like a very different goal from generating new text. But once you have a prediction model like this, a simple thing you could try to make it generate a longer piece of text is to give it an initial snippet to work with, have it take a random sample from the distribution it just generated, append that sample to the text, and then run the whole process again to make a new prediction based on all the new text, including what it just added. I don't know about you, but it really doesn't feel like this should actually work. In this animation, for example, I'm running GPT2 on my laptop and having it repeatedly predict and sample the next chunk of text to generate a story based on the seed text. And the story just doesn't actually really make that much sense. But if I swap it out for API calls to GPT3 instead, which is the same basic model just much bigger, suddenly almost magically we do get a sensible story, one that even seems to infer that a pie creature would live in a land of math and computation. This process here of repeated prediction and sampling is essentially what's happening when you interact with chat GPT or any of these other large language models, and you see them producing one word at a time. In fact, one feature that I would very much enjoy is the ability to see the underlying distribution for each new word that it chooses. Let's kick things off with a very high level preview of how data flows through a transformer. We will spend much more time motivating and interpreting and expanding on the details of each step, but in broad strokes, when one of these chatbots generates a given word, here's what's going on under the hood. First, the input is broken up into a bunch of little pieces. These pieces are called tokens, and in the case of text these tend to be words or little pieces of words or other common character combinations. If images or sound are involved, then tokens could be little patches of that image, or little chunks of that sound. Each one of these tokens is then associated with a vector, meaning some list of numbers, which is meant to somehow encode the meaning of that piece. If you think of these vectors as giving coordinates in some very high-dimensional space, words with similar meanings tend to land on vectors that are close to each other in that space. This sequence of vectors then passes through an operation that's known as an attention block, and this allows the vectors to talk to each other and pass information back and forth to update their values. For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model. The attention block is what's responsible for figuring out which words in the context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated. And again, whenever I use the word meaning, this is somehow entirely encoded in the entries of those vectors. After that, these vectors pass through a different kind of operation, and depending on the source that you're reading, this will be referred to as a multi-layer perceptron, or maybe a feed-forward layer, and here the vectors don't talk to each other. They all go through the same operation in parallel, and while this block is a little bit harder to interpret, later on we'll talk about how the step is a little bit like asking a long list of questions about each vector, and then updating them based on the answers to those questions. All of the operations in both of these blocks look like a giant pile of matrix multiplications, and our primary job is going to be to understand how to read the underlying matrices. I'm glossing over some details about some normalization steps that happen in between, but this is after all a high-level preview. After that, the process essentially repeats. You go back and forth between attention blocks and multi-layer perceptron blocks. Until at the very end, the hope is that all of the essential meaning of the passage has somehow been baked into the very last vector in the sequence. We then perform a certain operation on that last vector that produces a probability distribution over all possible tokens, all possible little chunks of text that might come next. And like I said, once you have a tool that predicts what comes next given a snippet of text, you can feed it a little bit of seed text and have it repeatedly play this game of predicting what comes next, sampling from the distribution, appending it, and then repeating over and over. Some of you in the know may remember how long before chat GPT came into the scene, this is what early demos of GPT3 looked like. You would have it auto-complete stories and essays based on an initial snippet. To make a tool like this into a chat bot, the easiest starting point is to have a little bit of text that establishes the setting of a user interacting with a helpful AI assistant, what you would call the system prompt, and then you would use the user's initial question or prompt as the first bit of dialogue, and then you have it start predicting what such a helpful AI assistant would say in response. There is more to say about an added step of training that's required to make this work well, but at a high level this is the general idea. In this chapter, you and I are going to expand on the details of what happens at the very beginning of the network, at the very end of the network, and I also want to spend a lot of time reviewing some important bits of background knowledge, things that would have been second nature to any machine learning engineer by the time transformers came around. If you're comfortable with that background knowledge and a little impatient, you could probably feel free to skip to the next chapter, which is going to focus on the attention blocks, generally considered the heart of the transformer. After that, I want to talk more about these multi-layer perceptron blocks, how training works, and a number of other details that will have been skipped up to that point. For broader context, these videos are additions to a mini-series about deep learning, and it's okay if you haven't watched the previous ones, I think you can do it out of order, but before diving into transformer specifically, I do think it's worth making sure that we're on the same page about the basic premise and structure of deep learning. At the risk of stating the obvious, this is one approach to machine learning, which describes any model where you're using data to somehow determine how a model behaves. What I mean by that is let's say you want a function that takes in an image and it produces a label describing it, or our example of predicting the next word, given a passage of text, or any other task that seems to require some element of intuition and pattern recognition. We almost take this for granted these days, but the idea with machine learning is that rather than trying to explicitly define a procedure for how to do that task in code, which is what people would have done in the earliest days of AI, instead you set up a very flexible structure with tunable parameters, like a bunch of knobs and dials, and then somehow you use many examples of what the output should look like for a given input to tweak and tune the values of those parameters to mimic this behavior. For example, maybe the simplest form of machine learning is linear regression, where your inputs and your outputs are each single numbers, something like the square footage of a house and its price, and what you want is to find a line of best fit through this data, you know, to predict future house prices. That line is described by two continuous parameters, say the slope and the y intercept, and the goal of linear regression, is to determine those parameters to closely match the data. Needless to say, deep learning models get much more complicated, GPT-3, for example, has not two, but 175 billion parameters. But here's the thing, it's not a given that you can create some giant model with a huge number of parameters without it either grossly overfitting the training data, or being completely intractable to train. Deep learning describes a class of models that in the last couple decades have proven to scale remarkably well. What unifies them is that they all use the same training algorithm, it's called back propagation, we talked about it in previous chapters, and the context that I want you to have as we go in is that in order for this training algorithm to work well at scale, these models have to follow a certain specific format, and if you know this format going in, it helps to explain many of the choices for how a transformer processes language, which otherwise run the risk of feeling kind of arbitrary. First, whatever kind of model you're making, the input has to be formatted as an array of real numbers. This could simply mean a list of numbers, it could be a two-dimensional array, or very often you deal with higher dimensional arrays, where the general term used is tensor. You often think of that input data as being progressively transformed into many distinct layers, where again, each layer is always structured as some kind of array of real numbers, until you get to a final layer which you consider the output. For example, the final layer in our text processing model is a list of numbers representing the probability distribution for all possible next tokens. In deep learning, these model parameters are almost always referred to as weights, and this is because a key feature of these models is that the only way these parameters interact with the data being processed is through weighted sums. You also sprinkle some nonlinear functions throughout, but they won't depend on parameters. Typically, though, instead of seeing the weighted sums all naked and written out explicitly like this, you'll instead find them packaged together as various components in a matrix vector product. It amounts to saying the same thing, if you think back to how matrix vector multiplication works, each component in the output looks like a weighted sum. It's just often conceptually cleaner for you and me to think about matrices that are filled with tunable parameters, that transform vectors that are drawn from the data being processed. For example, those 175 billion weights in GPT-3 are organized into just under 28,000 distinct matrices. Those matrices in turn fall into eight different categories, and what you and I are going to do is step through each one of those categories to understand what that type does. As we go through, I think it's kind of fun to reference the specific numbers from GPT-3 to count up exactly where those 175 billion come from. Even if nowadays there are bigger and better models, this one has a certain charm as the first large language model to really capture the world's attention outside of ML communities. Also, practically speaking, companies tend to keep much tighter lips around the specific numbers for more modern networks. I just want to set the scene going in, that as you peek under the hood to see what happens inside a tool like JGPT, almost all of the actual computation looks like matrix vector multiplication. There's a little bit of a risk getting lost in the sea of billions of numbers, but you should draw a very sharp distinction in your mind between the weights of the model, which I'll always color in blue or red, and the data being processed, which I'll always color in gray. The weights are the actual brains. They are the things learned during training, and they determine how it behaves. The data being processed simply encodes whatever specific input is fed into the model for a given run, like an example snippet of text. With all of that as foundation, let's dig into the first step of this text processing example, which is to break up the input into little chunks and turn those chunks into vectors. I mentioned how those chunks are called tokens, which might be pieces of words or punctuation. But every now and then in this chapter, and especially in the next one, I'd like to just pretend that it's broken more cleanly into words, because we humans think in words, this'll just make it much easier to reference little examples and clarify each step. The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix that we'll encounter, known as the embedding matrix, has a single column for each one of these words. These columns are what determines what vector each word turns into in that first step. We label it WE, and like all the matrices we see, its values begin random, but they're going to be learned based on data. Turning words into vectors was common practice in machine learning long before transformers, but it's a little weird if you've never seen it before, and it sits the foundation for everything that follows, so let's take a moment to get familiar with it. We often call this embedding a word, which invites you to think of these vectors very geometrically, as points in some high dimensional space. Visualizing a list of three numbers, as coordinates for points in 3D space, would be no problem, but word embeddings tend to be much, much higher dimensional. In GPT-3, they have 12,288 dimensions, and as you'll see, it matters to work in a space that has a lot of distinct directions. In the same way that you could take a two-dimensional slice through a 3D space and project all the points onto that slice, for the sake of animating word embeddings that a simple model is giving me, I'm going to do an analogous thing by choosing a three-dimensional slice through this very high dimensional space, and projecting the word vectors down onto that and displaying the results. The big idea here is that as a model tweaks and tunes its weights to determine how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in the space have a kind of semantic meaning. For the simple word to vector model I'm running here, if I run a search for all the words whose embeddings are closest to that of tower, you'll notice how they all seem to give very similar tower-ish vibes, and if you want to pull up some python and play along at home, this is the specific model that I'm using to make the animations. It's not a transformer, but it's enough to illustrate the idea that directions in the space can carry semantic meaning. A very classic example of this is how if you take the difference between the vectors for woman and man, something you would visualize as a little vector in the space, connecting the tip of one to the tip of the other, it's very similar to the difference between king and queen. So let's say you didn't know the word for a female monarch, you could find it by taking king, adding this woman-man direction, and searching for the embeddings closest to that point. At least, kind of, despite this being a classic example for the model I'm playing with, the true embedding of queen is actually a little farther off than this would suggest, presumably because the way that queen is used in training data is not merely a feminine version of king. When I played around family relations seem to illustrate the idea much better. The point is, it looks like during training the model found it advantageous to choose embeddings such that one direction in this space encodes gender information. Another example is that if you take the embedding of Italy and you subtract the embedding of Germany and then you add that to the embedding of Hitler, you get something very close to the embedding of Mussolini. It's as if the model learned to associate some directions with Italianness and others with World War II Axis leaders. Maybe my favorite example in this vein is how in some models, if you take the difference between Germany and Japan and you add it to sushi, you end up very close to broadwurst. Also in playing this game of finding nearest neighbors, I was very pleased to see how close cat was to both beast and monster. One bit of mathematical intuition that's helpful to have in mind, especially for the next chapter, is how the dot product of two vectors can be thought of as a way to measure how well they align. Computationally, dot products involve multiplying all the corresponding components and then adding the results, which is good since so much of our computation has to look like weighted sums. Geometrically, the dot product is positive when vectors point in similar directions. It's zero if they're perpendicular and it's negative whenever they point in opposite directions. For example, let's say you were playing with this model and you hypothesize that the embedding of cats minus cat might represent a sort of plurality direction in this space. To test this, I'm going to take this vector and compute its dot product against the embeddings of certain singular nouns and compare it to the dot products with the corresponding plural nouns. If you play around with this, you'll notice that the plural ones do indeed seem to consistently give higher values than the singular ones, indicating that they align more with this direction. It's also fun how if you take this dot product with the embeddings of the words 1, 2, 3 and so on, they give increasing values, so it's as if we can quantitatively measure how plural the model finds a given word. Again, the specifics for how words get embedded is learned using data. This embedding matrix, whose columns tell us what happens to each word, is the first pile of weights in our model, and using the GPT-3 numbers, the vocabulary size specifically is 50,257, and again, technically this consists not of words per se but of tokens, and the embedding dimension is 12,288. Multiplying those tells us this consists of about 617 million weights. Let's go ahead and add this to a running tally, remembering that by the end we should count up to 175 billion. In the case of transformers, you really want to think of the vectors in this embedding space as not merely representing individual words. For one thing, they also encode information about the position of that word, which we'll talk about later, but more importantly, you should think of them as having the capacity to soak in context. A vector that started its life as the embedding of the word king, for example, might progressively get tugged and pulled by various blocks in this network so that by the end it points in a much more specific and nuanced direction that somehow encodes that it was a king who lived in Scotland, and who had achieved his post after murdering the previous king, and who's being described in Shakespearean language. Think about your own understanding of a given word. The meaning of that word is clearly informed by the surroundings, and sometimes this includes context from a long distance away, so in putting together a model that has the ability to predict what word comes next, the goal is to somehow empower it to incorporate context efficiently. To be clear, in that very first step, when you create the array of vectors based on the input text, each one of those is simply plucked out of the embedding matrix, so initially each one can only encode the meaning of a single word without any input from its surroundings. But you should think of the primary goal of this network that it flows through, as being to enable each one of those vectors to soak up a meaning that's much more rich and specific than what mere individual words could represent. The network can only process a fixed number of vectors at a time, known as its context size. For GPT-3, it was trained with a context size of 2048, so the data flowing through the network always looks like this array of 2048 columns, each of which has 12,000 dimensions. This context size limits how much text the transformer can incorporate when it's making a prediction of the next word. This is why long conversations with certain chatbots, like the early versions of chat GPT, often gave the feeling of the bot kind of losing the threat of conversation as you continued too long. We'll go into the details of attention in due time, but skipping ahead, I want to talk for a minute about what happens at the very end. Remember, the desired output is a probability distribution over all tokens that might come next. For example, if the very last word is Professor, and the context includes words like Harry Potter, and immediately proceeding, we see least favorite teacher, and also if you give me some leeway by letting me pretend that tokens simply look like full words, then a well-trained network that had built up knowledge of Harry Potter would presumably assign a high number to the word Snape. This involves two different steps. The first one is to use another matrix that maps the very last vector in that context to a list of 50,000 values, one for each token in the vocabulary. Then there's a function that normalizes this into a probability distribution. It's called Softmax, and we'll talk more about it in just a second, but before that, it might seem a little bit weird to only use this last embedding to make a prediction. When, after all, in that last step, there are thousands of other vectors in the layer just sitting there with their own context-rich meanings. This has to do with the fact that in the training process, it turns out to be much more efficient if you use each one of those vectors in the final layer to simultaneously make a prediction for what would come immediately after it. There's a lot more to be said about training later on, but I just want to call that out right now. This matrix is called the Unimbedding Matrix, and we give it the label WU. Again, like all the weight matrices we see, its entries begin at random, but they are learned during the training process. Keeping score on our total parameter count, this unimbedding matrix has one row for each word in the vocabulary, and each row has the same number of elements as the embedding dimension. It's very similar to the embedding matrix just with the order swapped, so it adds another 617 million parameters to the network, meaning our count so far, is a little over a billion. A small, but not wholly insignificant fraction of the 175 billion that we'll end up with in total. As the very last mini-lesson for this chapter, I want to talk more about the softmax function, since it makes another appearance for us once we dive into the attention blocks. The idea is that if you want a sequence of numbers to act as a probability distribution, say a distribution over all possible next words, then each value has to be between 0 and 1, and you also need all of them to add up to 1. However, if you're playing the deep learning game, where everything you do looks like matrix vector multiplication, the outputs that you get by defaults don't abide by this at all. The values are often negative, or much bigger than 1, and they almost certainly don't add up to 1. Softmax is the standard way to turn an arbitrary list of numbers into a valid distribution, in such a way that the largest values end up closest to 1, and the smaller values end up very close to 0. That's all you really need to know, but if you're curious, the way that it works, is to first raise e to the power of each of the numbers, which means you now have a list of positive values, and then you can take the sum of all those positive values, and divide each term by that sum, which normalizes it into a list that adds up to 1. You'll notice that if one of the numbers in the input is meaningfully bigger than the rest, then in the output, the corresponding term dominates the distribution, so if you are sampling from it, you'd almost certainly just be picking the maximizing input, but it's softer than just picking the max in the sense that when other values are similarly large, they also get meaningful weight in the distribution, and everything changes continuously as you continuously vary the inputs. In some situations, like when ChatchyPt is using this distribution to create a next word, there's room for a little bit of extra fun, by adding a little extra spice into this function, with a constant T thrown into the denominator of those exponents. We call it the temperature, since it vaguely resembles the role of temperature in certain thermodynamics equations, and the effect is that when T is larger, you give more weight to the lower values, meaning the distribution is a little bit more uniform, and if T is smaller, then the bigger values will dominate more aggressively. We're in the extreme, setting T equal to 0 means all of the weight goes to that maximum value. For example, I'll have GPT-3 generate a story with the seed text once upon a time there was A, but I'm going to use different temperatures in each case. Temperature 0 means that it always goes with the most predictable word, and what you get ends up being kind of a trite derivative of Goldilocks. A higher temperature gives it a chance to choose less likely words, but it comes with a risk. In this case, the story starts out a bit more originally, about a young web artist from South Korea, but it quickly degenerates into nonsense. Technically speaking, the API doesn't actually let you pick a temperature bigger than two. There is no mathematical reason for this. It's just an arbitrary constraint imposed, I suppose, to keep their tool from being seen generating things that are too nonsensical. So, if you're curious the way this animation is actually working, is I'm taking the 20 most probable next tokens that GPT-3 generates, which seems to be the maximum they'll give me, and then I tweak the probabilities based on an exponent of 1-5th. As another bit of jargon, in the same way that you might call the components of the output of this function probabilities, people often refer to the inputs as logits, or some people say logits, some people say logits, I'm going to say logits. So for instance, when you feed in some text, you have all these word embeddings flow through the network, and you do this final multiplication with the unembedding matrix, machine learning people would refer to the components in that raw, unnormalized output, as the logits for the next word prediction. A lot of the goal with this chapter was to lay the foundations for understanding the attention mechanism, Karate Kid Waxon Wax Off-Style. You see, if you have a strong intuition for word embeddings, for softmax, for how dot products measure similarity, and also the underlying premise that most of the calculations have to look like matrix multiplication with matrices full of tunable parameters, then understanding the attention mechanism, this cornerstone piece in the whole modern boom in AI should be relatively smooth. For that, come join me in the next chapter. As I'm publishing this, a draft of that next chapter is available for review by Patreon supporters. A final version should be up in public in a week or two, it usually depends on how much I end up changing based on that review. In the meantime, if you want to dive into attention, and if you want to help the channel out a little bit, it's there waiting. \\nAnswer: Gradient Descent is an optimization algorithm used to minimize a cost function by iteratively adjusting the weights and biases of a model based on the negative of the gradient of the cost function. It is a key algorithm in training neural networks and other machine learning models. The goal is to find the set of weights and biases that minimizes the cost function, which represents the error between the predicted output and the actual output. Gradient Descent updates the weights and biases in the direction of the steepest decrease of the cost function, which is determined by the negative gradient. The learning rate determines the size of the steps taken in each iteration. The algorithm converges to a local minimum, which is a point where the gradient is close to zero. Gradient Descent is an iterative process and requires many iterations to converge to a solution. It is a fundamental concept in machine learning and deep learning, and is used in various optimization algorithms such as Stochastic Gradient Descent and Mini-Batch Gradient Descent.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFaceHub, HuggingFaceEndpoint\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Initialize 'llm' using HuggingFaceHub with the specified parameters\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    huggingfacehub_api_token=os.environ['HUGGINGFACE_API_TOKEN'],\n",
        "    # top_p=0.95,\n",
        "    task=\"text-generation\"\n",
        "    )\n",
        "\n",
        "def format_docs(docs):\n",
        "    # Combine all document contents into one string (separated by newlines)\n",
        "    return \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Invoke the RAG chain with the question \"What is Gradient Descent?\"\n",
        "rag_chain.invoke(\"What is Gradient Descent?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbmk8_oFku2b"
      },
      "source": [
        "# 6 - Quoting sources\n",
        "\n",
        "One key benefit of RAG (Retrieval-Augmented Generation) systems is the ability to trace answers back to their original sources. By modifying our chain, we can return not only the generated answer but also the metadata from the retrieved documents‚Äîeffectively quoting the sources used by the LLM to generate its response.\n",
        "\n",
        "In this part, you need to:\n",
        "1. Modify rag_chain_with_source to retrieve relevant documents based on the input question.\n",
        "\n",
        "2. Return Metadata: Ensure the chain returns both the generated answer and metadata (e.g., source, title, author) of the retrieved documents.\n",
        "\n",
        "3. Invoke the Chain:\n",
        "\n",
        "Observe the generated answer along with the quoted sources.\n",
        "\n",
        "<!-- ![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/5.3%20-%20RAG/references.png) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLmvl_svgDKa",
        "outputId": "8e106772-43ac-466f-dcfb-930d2c292ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'documents': [{'title': 'Backpropagation calculus _ DL4', 'source': 'https://www.youtube.com/watch?v=tIeHLnjs5U8'}, {'source': 'https://www.youtube.com/watch?v=Ilg3gGewQ5U', 'title': 'Backpropagation, intuitively _ DL3'}, {'title': 'Gradient descent, how neural networks learn _ DL2', 'source': 'https://www.youtube.com/watch?v=IHZwWFHWa-w'}, {'source': 'https://www.youtube.com/watch?v=aircAruvnKk', 'title': 'But what is a neural network_ _ Deep learning chapter 1'}, {'source': 'https://www.youtube.com/watch?v=9-Jl0dxWQs8', 'title': 'How might LLMs store facts _ DL7'}], 'answer': 'Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\nQuestion: When to use Relu or Sigmoid in a Neural Network? \\nContext: The hard assumption here is that you\\'ve watched Part 3, giving an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the relevant calculus. It\\'s normal for this to be at least a little confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has kind of a different feel from how most introductory calculus courses approach the subject. For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. Let\\'s just start off with an extremely simple network, one where each layer has a single neuron in it. So this particular network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. That way we know which adjustments to those terms is going to cause the most efficient decrease to the cost function. And we\\'re just going to focus on the connection between the last two neurons. Let\\'s label the activation of that last neuron with a superscript L indicating which layer it\\'s in. So the activation of the previous neuron is a L minus 1. These are not exponents, they\\'re just a way of indexing what we\\'re talking about since I want to save subscripts for different indices later on. Now let\\'s say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. So the cost of this simple network for a single training example is AL minus y squared. We\\'ll denote the cost of that one training example as C0. As a reminder, this last activation is determined by a weight, which I\\'m going to call WL, times the previous neurons activation plus some bias, which I\\'ll call BL. And then you pump that through some special nonlinear function like the sigmoid or a ray loop. It\\'s actually going to make things easier for us if we give a special name to this weighted sum, like Z, with the same superscript as the relevant activations. So this is a lot of terms and a way that you might conceptualize it is that the weight, the previous action and the bias altogether are used to compute Z, which in turn lets us compute A, which finally, along with a constant Y, lets us compute the cost. And of course, AL minus 1 is influenced by its own weight and bias and such. But we\\'re not going to focus on that right now. Now all of these are just numbers, right? And it can be nice to think of each one as having its own little number line. Our first goal is to understand how sensitive the cost function is to small changes in our weight, WL. Never phrase differently. What is the derivative of C with respect to WL? When you see this Dell W term, think of it as meaning some tiny nudge to W, like a change by 0.01. And think of this Dell C term as meaning whatever the resulting nudge to the cost is. What we want is their ratio. Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost. So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is the derivative of ZL with respect to WL. Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge to C and this intermediate nudge to AL. This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of C to small changes in WL. So on screen right now, there\\'s kind of a lot of symbols, and take a moment to just make sure it\\'s clear what they all are. Because now we\\'re going to compute the relevant derivatives. The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this means that its size is proportional to the difference between the networks output and the thing that we want it to be. So if that output was very different, even slight changes stand to have a big impact on the final cost function. The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever non-linearity you choose to use. And the derivative of ZL with respect to WL, in this case comes out just to be AL minus 1. Now I don\\'t know about you, but I think it\\'s easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all actually mean. In the case of this last derivative, the amount that that small nudge to the weight influenced the last layer depends on how strong the previous neuron is. Remember, this is where that neurons that fire together wire together idea comes in. And all of this is the derivative with respect to WL only of the cost for a specific single training example. Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression that we found over all training examples. And of course that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases. But even though that\\'s just one of the many partial derivatives we need, it\\'s more than 50% of the work. The sensitivity to the bias, for example, is almost identical. We just need to change out this del Z del W term for a del Z del B. And if you look at the relevant formula, that derivative comes out to be 1. Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. Namely, this initial derivative in the chain rule expression, the sensitivity of Z to the previous activation, comes out to be the weight WL. And again, even though we\\'re not going to be able to directly influence that previous layer activation, it\\'s helpful to keep track of. Because now, we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. And you might think that this is an overly simple example, since all layers just have one neuron, and that things are going to get exponentially more complicated for a real network. But honestly, not that much changes when we give the layers multiple neurons. Really it\\'s just a few more indices to keep track of. Rather than the activation of a given layer simply being AL, it\\'s also going to have a subscript, indicating which neuron of that layer it is. Let\\'s go ahead and use the letter K to index the layer L minus 1, and J to index the layer L. For the cost, again, we look at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. That is, you take a sum over ALJ minus YJ squared. Since there\\'s a lot more weights, each one has to have a couple more indices to keep track of where it is. So let\\'s call the weight of the edge connecting this Kth neuron to the Jth neuron, WLJK. Those indices might feel a little backwards at first, but it lines up with how you\\'d index the weight matrix that I talked about in the Part 1 video. Just as before, it\\'s still nice to give a name to the relevant weighted sum, like Z, so that the activation of the last layer is just your special function, like the sigmoid, applied to Z. You can kind of see what I mean, right, where all of these are essentially the same equations that we had before in the one neuron per layer case. It\\'s just that it looks a little more complicated. And indeed, the chain ruled the rivetive expression, describing how sensitive the cost is to a specific weight, looks essentially the same. I\\'ll leave it to you to pause and think about each of those terms if you want. What does change here, though, is the derivative of the cost with respect to one of the activations in the layer L minus 1. In this case, the difference is that the neuron influences the cost function through multiple different paths. It is on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. And that, well, that\\'s pretty much it. Once you know how sensitive the cost function is to the activations in this second to last layer, you can just repeat the process for all the weights and biases feeding into that layer. So pat yourself on the back. If all of this makes sense, you have now looked deep into the heart of back propagation, the workhorse behind how neural networks learn. These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around. So don\\'t worry if it takes time for your mind to digest it all.\\nHere we tackle back propagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I\\'ll do is an intuitive walk-through for what the algorithm is actually doing without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos, or if you\\'re just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. Here, we\\'re doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons, and I\\'ve been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as a tensor. I\\'m also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. As a quick reminder for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted it to give, and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. As if that\\'s not enough to think about, as described in the last video, the thing that we\\'re looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now, is that because thinking of the gradient vector as a direction in 13,000 dimensions is to put it lightly beyond the scope of our imaginations, there\\'s another way you can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let\\'s say you go through the process I\\'m about to describe when you compute the negative gradient and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if you were to wiggle that value just a little bit, it\\'s going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect that it\\'s having is actually pretty intuitive. It\\'s just that there\\'s a lot of little adjustments getting layered on top of each other. So I\\'m going to start things off here with a complete disregard for the notation, and just step through those effects that each training example is having on the weights and biases. Because the cost function involves averaging a certain cost per example, over all the tens of thousands of training examples, the way that we adjust the weights and biases for a single gradient descent step also depends on every single example, or rather, in principle it should, but for computational efficiency, we\\'re going to do a little trick later to keep you from needing to hit every single example for every single step. In other case, right now, all we\\'re going to do is focus our attention on one single example, this image of a two. What effect should this one training example have on how the weights and biases get adjusted? Let\\'s say we\\'re at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now, we can\\'t directly change those activations. We only have influence on the weights and biases. But it is helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify the image as a two, we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. For example, the increase to that number two neurons activation is in a sense more important than the decrease to the number eight neuron, which is already pretty close to where it should be. So zooming in further, let\\'s focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ray-lew. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing just on how the weights should be adjusted? Notice how the weights actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect, since those weights are multiplied by larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don\\'t just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your butt. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, heavy in theory. Often summed up in the phrase, neurons that fire together, wire together. Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a two get more strongly linked to those firing when thinking about a two. To be clear, I really am not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together, wire together idea comes with a couple meaningful asterisks. But taken as a very loose analogy, I do find it interesting to note. Anyway, the third way that we can help increase this neuron\\'s activation is by changing all the activations in the previous layer. Namely, if everything connected to that digit two neuron with a positive weight, got brighter, and if everything connected with a negative weight got dimmer, then that digit two neuron would become more active. And similar to the weight changes, you\\'re going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now, of course, we cannot directly influence those activations. We only have control over the weights and biases. But just as with the last layer, it\\'s helpful to just keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit two output neuron wants. Remember, we also want all of the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So the desire of this digit two neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer. Again, in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listen to what that two wanted, the network would ultimately be incentivized just to classify all images as a two. What you do is you go through this same back property for every other training example, recording how each of them would like to change the weights and the biases. And you average together those desired changes. This collection here of the average to nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get quantitatively precise about those nudges. But if you understood every change that I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what back propagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence of every single training example, every single gradient descent step. So here\\'s what\\'s commonly done instead. You randomly shuffle your training data and then divide it into a whole bunch of mini-batches. Let\\'s say each one having 100 training examples. Then you compute a step according to the mini-batch. It\\'s not going to be the actual gradient to the cost function, which depends on all of the training data, not this tiny subset. So it\\'s not the most efficient step downhill. But each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speed up. If you would applaud the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There\\'s kind of a lot going on here, so let\\'s just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases. Not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired changes that you get. But that\\'s computationally slow, so instead you randomly subdivide the data into these mini batches and compute each step with respect to a mini batch. Repeatedly going through all of the mini batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network is going to end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing Backprop actually corresponds with something that you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the M-NIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data that you actually need, whether that\\'s having people label tens of thousands of images or whatever other data type you might be dealing with.\\nLast video, I laid out the structure of a neural network. I\\'ll give a quick recap here just so that it\\'s fresh in our minds and then I have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well. Then after that, we\\'re going to dig in a little more to how this particular network performs and what those hidden layers of neurons end up actually looking for. As a reminder, our goal here is the classic example of handwritten digit recognition, the Hello World of Neural Networks. These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons in the input layer of the network. And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer plus some special number called a bias. Then you compose that sum with some other function, like the sigmoid squishification or a ray-loo, the way that I walked through last video. In total, given the somewhat arbitrary choice of two hidden layers here with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it\\'s these values that determine what exactly the network actually does. And what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit. And remember, the motivation that we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits. So here we learn how the network learns. What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they\\'re supposed to be, and it\\'ll adjust those 13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data. And the way we test that is that after you train the network, you show it more labeled data, that it\\'s never seen before, and you see how accurately it classifies those new images. Fortunately for us, and what makes this such a common example to start with, is that the good people behind the M-NIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers that they\\'re supposed to be. And it\\'s provocative as it is to describe a machine as learning. Once you actually see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like, a calculus exercise. I mean, basically, it comes down to finding the minimum of a certain function. Remember, conceptually, we\\'re thinking of each neuron as being connected to all of the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections. The bias is some indication of whether that neuron tends to be active or inactive. And to start things off, we\\'re just going to initialize all of those weights and biases totally randomly. Needless to say, this network is going to perform pretty horribly on a given training example, since it\\'s just doing something random. For example, you feed in this image of a three, and the output layer just looks like a mess. So what you do is you define a cost function, a way of telling the computer, no, bad computer, that output should have activations, which are zero for most neurons, but one for this neuron. What you gave me is utter trash. To say that a little more mathematically, what you do is add up the squares of the differences between each of those trash output activations and the value that you want them to have. And this is what we\\'ll call the cost of a single training example. Notice, this sum is small when the network confidently classifies the image correctly, but it\\'s large when the network seems like it doesn\\'t really know what it\\'s doing. So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal. This average cost is our measure for how lousy the network is and how bad the computer should feel. And that\\'s a complicated thing. Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output. And in a sense, it\\'s parameterized by all these weights and biases. Well the cost function is a layer of complexity on top of that. It takes as its input, those 13,000 or so weights and biases, and it spits out a single number describing how bad those weights and biases are. And the way it\\'s defined depends on the network\\'s behavior over all the tens of thousands of pieces of training data. That\\'s a lot to think about. But just telling the computer what a crappy job it\\'s doing isn\\'t very helpful. You want to tell it how to change those weights and biases so that it gets better. To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output. How do you find an input that minimizes the value of this function? Circular students will know that you can sometimes figure out that minimum explicitly. But that\\'s not always feasible for really complicated functions. Certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function. A more flexible tactic is to start at any all input and figure out which direction you should step to make that output lower. Specifically, if you can figure out the slope of the function where you are, then shift to the left if that slope is positive and shift the input to the right if that slope is negative. If you do this repeatedly, at each point checking the new slope and taking the appropriate step, you\\'re going to approach some local minimum of the function. And the image you might have in mind here is a ball rolling down a hill. And notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at. There\\'s no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function. That\\'s going to carry over to our neural network case as well. And I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that kind of helps you from overshooting. Bumping up the complexity a bit, imagine instead a function with two inputs and one output. You might think of the input space as the x, y plane and the cost function as being graft as a surface above it. Now instead of asking about the slope of the function, you have to ask which direction should you step in this input space so as to decrease the output of the function most quickly. In other words, what\\'s the downhill direction? And again, it\\'s helpful to think of a ball rolling down that hill. Most of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, basically which direction should you step to increase the function most quickly. Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly. And even more than that, the length of this gradient vector is actually an indication for just how steep that steepest slope is. Now if you\\'re unfamiliar with multivariable calculus and you want to learn more, check out some of the work that I did for Khan Academy on the topic. Honestly though, all that matters for you and me right now is that in principle there exists a way to compute this vector. This vector that tells you what the downhill direction is and how steep it is. You\\'ll be okay if that\\'s all you know and you\\'re not rock solid on the details. Because if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill and just repeat that over and over. It\\'s the same basic idea for a function that has 13,000 inputs instead of two inputs. Imagine organizing all 13,000 weights and biases of our network into a giant column vector. The negative gradient of the cost function is just a vector. It\\'s some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function. And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values and more like an actual decision that we want it to make. It\\'s important to remember, this cost function involves an average over all of the training data. So if you minimize it, it means it\\'s a better performance on all of those samples. The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called back propagation. And it\\'s what I\\'m going to be talking about next video. There I really want to take the time to walk through what exactly happens to each weight and each bias for a given piece of training data. Trying to give an intuitive feel for what\\'s happening beyond the pile of relevant calculus and formulas. Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it\\'s just minimizing a cost function. And notice, one consequence of that is that it\\'s important for this cost function to have a nice smooth output so that we can find a local minimum by taking little steps down hill. This is why, by the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way that biological neurons are. This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent. It\\'s a way to converge toward some local minimum of a cost function, basically a valley in this graph. I\\'m still showing the picture of a function with two inputs, of course, because nudges in a 13,000-dimensional input space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to think about this. Each component of the negative gradient tells us two things. The sign, of course, tells us whether the corresponding component of the input vector should be nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you which changes matter more. You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight. Some of these connections just matter more for our training data. So a way that you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck. This really is just another way of thinking about direction. To take a simpler example, if you have some function with two variables as an input, and you compute that it\\'s gradient at some particular point, comes out as 3-1. Then on the one hand, you can interpret that as saying that when you\\'re standing at that input, moving along this direction increases the function most quickly. But when you graph the function above the plane of input points, that vector is what\\'s giving you the straight uphill direction. But another way to read that is to say that changes to this first variable have three times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck. Alright, let\\'s zoom out and sum up where we are so far. The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted sums. The cost function is a layer of complexity on top of that. It takes the 13,000 weights and biases as inputs, and spits out a single measure of laziness based on the training examples. And the gradient of the cost function is one more layer of complexity still. It tells us what nudges to all of these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying which changes to which weights matter the most. So, when you initialize the network with random weights and biases and adjust them many times based on this gradient descent process, how well does it actually perform on images that it\\'s never seen before? Well the one that I\\'ve described here, with the two hidden layers of 16 neurons each, goes in mostly for aesthetic reasons. Well, it\\'s not bad. It classifies about 96% of the new images that it sees correctly. And honestly, if you look at some of the examples that it messes up on, you kind of feel compelled to cut it a little slack. Now, if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%. And that\\'s pretty good. It\\'s not the best. You can certainly get better performance by getting more sophisticated than this plain vanilla network. But given how daunting the initial task is, I just think there\\'s something incredible about any network doing this well on images that it\\'s never seen before, given that we never specifically told it what patterns to look for. Originally, the way that I motivated this structure was by describing a hope that we might have, that the second layer might pick up on little edges, that the third layer would piece together those edges to recognize loops in longer lines, and that those might be piece together to recognize digits. So is this what our network is actually doing? Well, for this one, at least, not at all. Remember how last video we looked at how the weights of the connections from all of the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that that second layer neuron is picking up on? Well, when we actually do that, for the weights associated with these transitions from the first layer to the next. Instead of picking up on isolated little edges here and there, they look almost random, just with some very loose patterns in the middle there. It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that, despite successfully classifying most images, doesn\\'t exactly pick up on the patterns that we might have hoped for. And to really drive this point home, watch what happens when you input a random image. If the system was smart, you might expect it to either feel uncertain, maybe, not really activating any of those 10 output neurons or activating them all evenly. But instead, it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as it does that an actual image of a 5 is a 5. Fraze differently? Even if this network can recognize digits pretty well, it has no idea how to draw them. A lot of this is because it\\'s such a tightly constrained training setup. I mean, put yourself in the network\\'s shoes here. From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid. And its cost function just never gave it any incentive to be anything but utterly confident in its decisions. So with this is the image of what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. That\\'s just not at all what it ends up doing. Well this is not meant to be our end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s and 90s. And you do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems. But the more you dig in to what those hidden layers are really doing, the less intelligent it seems. Shifting the focus for a moment from how networks learn to how you learn, that\\'ll only happen if you engage actively with the material here somehow. One pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you wanted it to better pick up on things like edges and patterns. But better than that, to actually engage with the material, I highly recommend the book by Michael Neilsen on deep learning and neural networks. In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that code is doing. What\\'s awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Neilsen\\'s efforts. I\\'ve also linked a couple other resources that I like a lot in the description, including the phenomenal and beautiful blog posts by Chris Ola and the articles in Distill. To close things off here for the last few minutes, I want to jump back into a snippet of the interview that I had with Lisha Lee. You might remember her from the last video, she did her PhD work in deep learning. And in this little snippet, she talks about two recent papers that really dig in to how some of the more modern image recognition networks are actually learning. Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that\\'s really good at image recognition, and instead of training it on a properly labeled data set, it shuffled all of the labels around before training. Obviously the testing accuracy here was going to be no better than random, since everything\\'s just randomly labeled. But it was still able to achieve the same training accuracy as you would on a properly labeled data set. Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which kind of raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization? It reminds the entire data set of what the correct classification is. And so a couple of half a year later at ICML this year, there was not exactly rebuttal paper, paper that addressed some aspects of like, hey, actually these networks are doing something a little bit smarter than that. If you look at that accuracy curve, if you were just training on a random data set, that curve sort of went down very slowly in almost a linear fashion. So you\\'re really struggling to find that local minimum of possible, the right weights that would get you that accuracy. Whereas if you\\'re actually training on a structure data set, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level. And so in some sense, it was easier to find that local maxima. And so it was also interesting about that, is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about the network layers. But one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal quality. So in some sense, if your data set a structure, you should be able to find that much more easily. My thanks, as always, to those of you supporting on Patreon. I\\'ve said before just what a game changer in Patreon is, but these videos really would not be possible without you. I also want to give a special thanks to the VC firm Amplify Partners in their support of these initial videos in the series.\\nThis is a 3. It\\'s slobily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3. But something in that crazy smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this, and outputs a single number between 0 and 10, telling you what it thinks the digit is, while the task goes from comically trivial to dauntingly difficult. Unless you\\'ve been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it\\'s doing. Not as a buzzword, but as a piece of math. My hope is just that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote unquote learning. This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we\\'re going to do is put together a neural network that can learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic, and I\\'m happy to stick with the status quote here, because at the end of the two videos, I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There are many, many variants of neural networks, and in recent years there\\'s been sort of a boom in research towards these variants, but in these two introductory videos, you and I are just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me, it still has plenty of complexity for us to wrap our minds around. But even in this simplest form, it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do. And at the same time, you\\'ll see how it does fall short of a couple hopes that we might have for it. As the name suggests, neural networks are inspired by the brain, but let\\'s break that down. What are the neurons and in what sense are they linked together? Right now, when I say neuron, all I want you to think about is a thing that holds a number, specifically a number between zero and one. It\\'s really not more than that. For example, the network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the gray scale value of the corresponding pixel, ranging from zero for black pixels up to one for white pixels. This number inside the neuron is called its activation. And the image you might have in mind here is that each neuron is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each representing one of the digits. The activation in these neurons, again, some number that\\'s between zero and one, represents how much the system thinks that a given image corresponds with a given digit. There\\'s also a couple layers in between called the hidden layers, which for the time being, should just be a giant question mark. For how on earth this process of recognizing digits is going to be handled. In this network, I chose two hidden layers, each one with 16 neurons. And admittedly, that\\'s kind of an arbitrary choice. To be honest, I chose two layers based on how I want to motivate the structure in just a moment. And 16, well, that was just a nice number to fit on the screen. In practice, there is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer determine the activations of the next layer. And of course, the heart of the network, as an information processing mechanism, comes down to exactly how those activations from one layer bring about activations in the next layer. It\\'s meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now, the network I\\'m showing here has already been trained to recognize digits. And let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer, according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer, which causes some pattern in the one after it, which finally gives some pattern in the output layer. And the brightest neuron of that output layer is the network\\'s choice, so to speak, for what digit this image represents. And before jumping into the math for how one layer influences the next or how training works, let\\'s just talk about why it\\'s even reasonable to expect a layered structure like this to behave intelligently. What are we expecting here? What is the best hope for what those middle layers might be doing? Well, when you or I recognize digits, we piece together various components. A 9 has a loop up top and a line on the right. And 8 also has a loop up top, but it\\'s paired with another loop down low. A 4 basically breaks down into 3 specific lines and things like that. Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents. That anytime you feed in an image with, say, a loop up top, like a 9 or an 8, there\\'s some specific neuron whose activation is going to be close to 1. And I don\\'t mean this specific loop of pixels. The hope would be that any generally loopy pattern towards the top sets off this neuron. That way, going from the third layer to the last one, just requires learning which combination of subcomponents corresponds to which digits. Of course, that just kicks the problem down the road because how would you recognize these sub components or even learn what the right subcomponents should be? And I still haven\\'t even talked about how one layer influences the next, but run with me on this one for a moment. Recognizing a loop can also break down into subproblems. One reasonable way to do this would be to first recognize the various little edges that make it up. Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, well that\\'s really just a long edge, or maybe you think of it as a certain pattern of several smaller edges. So maybe, our hope is that each neuron in the second layer of the network corresponds with the various relevant little edges. Maybe, when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10 specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with the 9. Whether or not this is what our final network actually does is another question, one that I\\'ll come back to once we see how to train the network. But this is a hope that we might have, a sort of goal with the layered structure like this. Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks. And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction. Parsing speech, for example, involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract thoughts, etc. But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the activations in the next. The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits. And to zoom in on one very specific example, let\\'s say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here. The question at hand is what parameters should the network have? What dials and knobs should you be able to tweak so that it\\'s expressive enough to potentially capture this pattern, or any other pixel pattern, or the pattern that several edges can make a loop and other such things? Well, what we\\'ll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer. These weights are just numbers. Then take all of those activations from the first layer and compute their weighted sum according to these weights. I find it helpful to think of these weights as being organized into a little grid of their own, and I\\'m going to use green pixels to indicate positive weights and red pixels to indicate negative weights, where the brightness of that pixel is some loose depiction of the weights value. Now if we made the weights associated with almost all of the pixels zero, except for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about. And if you really wanted to pick up on whether there\\'s an edge here, what you might do is have some negative weights associated with the surrounding pixels. Then the sum is largest when those middle pixels are bright, but the surrounding pixels are darker. When you compute a weighted sum like this, you might come out with any number, but for this network, what we want is for activations to be some value between zero and one. So a common thing to do is to pump this weighted sum into some function that squishes the real number line into the range between zero and one. And a common function that does this is called the sigmoid function, also known as a logistic curve, basically very negative inputs end up close to zero, very positive inputs end up close to one, and it just steadily increases around the input zero. So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is. But maybe it\\'s not that you want the neuron to light up when the weighted sum is bigger than zero. Maybe you only want it to be active when the sum is bigger than say 10. That is, you want some bias for it to be inactive. What we\\'ll do then is just add in some other number, like negative 10, to this weighted sum, before plugging it through the sigmoid squishing function. That additional number is called the bias. So the weights tell you what pixel pattern this neuron in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active. And that is just one neuron. Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated with it. Also, each one has some bias, some other number that you add onto the weighted sum before squishing it with the sigmoid. And that\\'s a lot to think about. With this hidden layer of 16 neurons, that\\'s a total of 784 x 16 weights, along with 16 biases. And all of that is just the connections from the first layer to the second. The connections between the other layers also have a bunch of weights and biases associated with them. All said and done, this network has almost exactly 13,000 total weights and biases. 13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways. So when we talk about learning, what that\\'s referring to is getting the computer to find a valid setting for all of these many, many numbers so that it\\'ll actually solve the problem at hand. One thought experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on patterns, etc. I personally find this satisfying rather than just treating the network as a total black box because when the network doesn\\'t perform the way you anticipate, if you\\'ve built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to improve. Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible solutions. By the way, the actual function here is a little cumbersome to write down, don\\'t you think? So let me show you a more notationally compact way that these connections are represented. This is how you\\'d see it if you choose to read out more about neural networks. Organize all of the activations from one layer into a column as a vector. Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular neuron in the next layer. What that means is that taking the weighted sum of the activations in the first layer, according to these weights, corresponds to one of the terms in the matrix vector product of everything we have on the left here. By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3. Back to our expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector and adding the entire vector to the previous matrix vector product. Then as a final step, I\\'ll wrap a sigmoid around the outside here, and what that\\'s supposed to represent is that you\\'re going to apply the sigmoid function to each specific component of the resulting vector inside. So once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression. And this makes the relevant code both a lot simpler and a lot faster, since many libraries optimize the heck out of matrix multiplication. Remember how earlier I said these neurons are simply things that hold numbers? Well, of course the specific numbers that they hold depends on the image you feed in. So it\\'s actually more accurate to think of each neuron as a function, one that takes in the outputs of all the neurons in the previous layer, and spits out a number between 0 and 1. Really, the entire network is just a function, one that takes in 784 numbers as an input, and spits out 10 numbers as an output. It\\'s an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and which involves iterating many matrix vector products and the sigmoid squishification function. But it\\'s just a function nonetheless. And in a way, it\\'s kind of reassuring that it looks complicated. I mean, if there were any simpler, what hope would we have that it could take on the challenge of recognizing digits? And how does it take on that challenge? How does this network learn the appropriate weights and biases just by looking at data? Oh, that\\'s what I\\'ll show in the next video, and I\\'ll also dig a little more into what this particular network we\\'re seeing is really doing. Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but realistically, most of you don\\'t actually receive notifications from YouTube, do you? Maybe more honestly, I should say subscribe so that the neural networks that underlie YouTube\\'s recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you. Anyway, stay posted for more. Thank you very much to everyone supporting these videos on Patreon. I\\'ve been a little slow to progress in the probability series this summer, but I\\'m jumping back into it after this project, so patrons, you can look out for updates there. To close things off here, I have with me Lisa Lee, who did her PhD work on the theoretical side of deep learning, and who currently works at a venture capital firm called Amplify Partners, who kindly provided some of the funding for this video. So, Lisa, one thing I think we should quickly bring up is this sigmoid function. As I understand it, early networks use this to squish the relevant weighted sum into that interval between 0 and 1, you know, kind of motivated by this biological analogy of neurons either being inactive or active. Exactly. But relatively few modern networks actually use sigmoid anymore. It\\'s kind of old school, right? Yeah, or rather, relu seems to be much easier to train. And relu stands for rectified linear unit? Yes, it\\'s this kind of function where you\\'re just taking a max of 0 and a, where a is given by what you were explaining in the video, and what this was sort of motivated from, I think, was a partially biological analogy with how neurons would either be activated or not. And so if it passes a certain threshold, it would be the identity function, but if it did not, then it would just not be activated, so be zero. So it\\'s kind of a simplification. Using sigmoids didn\\'t help training, or it was very difficult to train at some point, and people just tried relu, and it happened to work very well for these incredibly deep neural networks. All right. Thank you, Lisa.\\nIf you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts basketball. This would suggest that somewhere, inside its hundreds of billions of parameters, it\\'s baked in knowledge about a specific person and his specific sport. And I think in general anyone who\\'s played around with one of these models has the clear sense that it\\'s memorized tons and tons of facts. So a reasonable question you could ask is, how exactly does that work, and where do those facts live? Last December a few researchers from Google Deep Mind posted about work on this question, and they were using this specific example of matching athletes to their sports. And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short. In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other modern AI. In the most recent chapter we were focusing on a piece called Attention, and the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network. The computation here is actually relatively simple, especially when you compare it to attention. It boils down essentially to a pair of matrix multiplications with a simple something in between. However, interpreting what these computations are doing is exceedingly challenging. Our main goal here is to step through the computations and make them memorable, but I\\'d like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact. Specifically, it\\'ll be storing the fact that Michael Jordan plays basketball. I should mention the layout here is inspired by a conversation I had with one of those deep-mind researchers, Neil Nanda. For the most part, I will assume that you\\'ve either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here\\'s the quick reminder of the overall flow. You and I have been studying a model that\\'s trained to take in a piece of text and predict what comes next. That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers. This sequence of vectors then repeatedly passes through two kinds of operation. Attention, which allows the vectors to pass information between one another, and then the multi-layer perceptrons, the thing that we\\'re going to dig into today. And also there\\'s a certain normalization step in between. After the sequence of vectors has flowed through many many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words and the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next. One of the key ideas that I want you to have in your mind is that all of these vectors live in a very very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning. So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very very close to the corresponding feminine noun. In this sense, this particular direction encodes gender information. The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent. In a transformer, these vectors don\\'t merely encode the meaning of a single word, though. As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model\\'s knowledge. Ultimately, each one needs to encode something far far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next. We\\'ve already seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts. Like I said, the lesson here is going to center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball. Now this toy example is going to require that you and I make a couple of assumptions about that high-dimensional space. First, we\\'ll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball. So specifically what I mean by this is if you look in the network and you plug out one of the vectors being processed, if its dot product with this first name Michael direction is one, that\\'s what it would mean for the vector to be encoding the idea of a person with that first name. Otherwise, that dot product would be zero or negative, meaning the vector doesn\\'t really align with that direction. And for simplicity, let\\'s completely ignore the very reasonable question of what it might mean if that dot product was bigger than one. Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan, or basketball. So let\\'s say a vector is meant to represent the full name Michael Jordan, then its dot product with both of these directions would have to be one. Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to ensure that it can encode both names. With all of those as the assumptions, let\\'s now dive into the meat of the lesson. What happens inside a multi-layer perceptron? You might think of this sequence of vectors flowing into the block, and remember each vector was originally associated with one of the tokens from the input text. What\\'s going to happen is that each individual vector from that sequence goes through a short series of operations, we\\'ll unpack them in just a moment, and at the end we\\'ll get another vector with the same dimension. That other vector is going to get added to the original one that flowed in, and that sum is the result flowing out. This sequence of operations is something you apply to every vector in the sequence associated with every token in the input, and it all happens in parallel. In particular, the vectors don\\'t talk to each other in this step, they\\'re all kind of doing their own thing. And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them. When I say this block is going to encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes, first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is what we\\'ll add on to the vector in that position. The first step of this process looks like multiplying that vector by a very big matrix, no surprises there, this is deep learning, and this matrix like all of the other ones we\\'ve seen is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is. Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector and taking a bunch of dot products between those rows and the vector being processed, which I\\'ll label as E for embedding. For example, suppose that very first row happened to equal this first name Michael direction that were presuming exists, that would mean that the first component in this output, this dot product right here, would be one if that vector encodes the first name Michael and zero or negative otherwise. Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus last name Jordan direction. And for simplicity, let me go ahead and write that down as m plus j, then taking a dot product with this embedding E, things distribute really nicely, so it looks like m.e plus j.e and notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan and otherwise it would be one or something smaller than one. And that\\'s just one row in this matrix. You might think of all of the other rows as in parallel asking some other kinds of questions probing at some other sorts of features of the vector being processed. Very often this step also involves adding another vector through the output, which is full of model parameters learned from data, this other vector is known as the bias. For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one. You might very reasonably ask why I would want you to assume that the model has learned this. And in a moment, you\\'ll see why it\\'s very clean and nice if we have a value here, which is positive, if and only if a vector encodes the full name Michael Jordan and otherwise it\\'s zero or negative. The total number of rows in this matrix, which is something like the number of questions being asked in the case of GPT3, whose numbers we\\'ve been following is just under 50,000. In fact, it\\'s exactly four times the number of dimensions in this embedding space. That\\'s a design choice you could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware. Since this matrix full of weights maps us into a higher dimensional space, I\\'m going to give it the shorthand W up. I\\'ll continue labeling the vector we\\'re processing as E, and let\\'s label this bias vector as B up and put that all back down in the diagram. At this point, a problem is that this operation is purely linear, but language is a very non-linear process. If the entry that we\\'re measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually. What you really want is a simple yes or no for the full name. So the next step is to pass this large intermediate vector through a very simple non-linear function. A common choice is one that takes all of the negative values and maps them to zero, and leaves all of the positive values unchanged. And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or Rayloo for short. Here\\'s what the graph looks like. So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan, and zero or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value where all of the zero and negative values just get clipped to zero. So this output would be one for the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the behavior of an AND gate. Often models will use a slightly modified function that\\'s called the J-LU, which has the same basic shape, it\\'s just a bit smoother, but for our purposes, it\\'s a little bit cleaner if we only think about the Rayloo. Also, when you hear people refer to the neurons of a transformer, they\\'re talking about these values right here. Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that\\'s typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple, termwise non-linear function like a Rayloo. You would say that this neuron is active whenever this value is positive, and that it\\'s inactive if that value is zero. The next step looks very similar to the first one. You multiply by a very large matrix and you add on a certain bias term. In this case, the number of dimensions in the output is back down to the size of that embedding space, so I\\'m going to go ahead and call this the down projection matrix. And this time, instead of thinking of things row by row, it\\'s actually nicer to think of it column by column. You see, another way that you can hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it\\'s processing and adding together all of those rescaled columns. The reason it\\'s nicer to think about this way is because here, the columns have the same dimension as the embedding space, so we can think of them as directions in that space. For instance, we will imagine that the model has learned to make that first column into this basket ball direction that we suppose exists. What that would mean is that when the relevant neuron in that first position is active, we\\'ll be adding this column to the final result. But if that neuron was inactive, if that number was zero, then this would have no effect. And it doesn\\'t just have to be basketball. The model could also bake into this column many other features that it wants to associate with something that has the full name Michael Jordan. And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active. And if you have a bias in this case, it\\'s something that you\\'re just adding every single time, regardless of the neuron values. You might wonder what\\'s that doing, as with all parameter field objects here, it\\'s kind of hard to say exactly, maybe there\\'s some book keeping that the network needs to do, but you can feel free to ignore it for now. Making our notation a little more compact again, I\\'ll call this big matrix W down, and similarly call that bias vector B down and put that back into our diagram. Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position, and that gets you this final result. So for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction. So what pops out will encode all of those together. And remember, this is a process happening to every one of those vectors in parallel. In particular, taking the GPT-3 numbers, it means that this block doesn\\'t just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input. So that is the entire operation, two matrix products each with a bias added, and a simple clipping function in between. Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there. In that example, it was trained to recognize handwritten digits. Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture, and any attempt to interpret what exactly it\\'s doing is heavily intertwined with the idea of encoding information into vectors of a high dimensional embedding space. That is the core lesson, but I do want to step back and reflect on two different things. The first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact about higher dimensions that I actually didn\\'t know until I dug into Transformers. In the last two chapters, you and I started counting up the total number of parameters in GPT-3 and seeing exactly where they live. So let\\'s quickly finish up the game here. I already mentioned how this up projection matrix has just under 50,000 rows, and that each row matches the size of the embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has the same number of parameters just with a transpose to shape. So together, they give about 1.2 billion parameters. The bias vector also accounts for a couple more parameters, but it\\'s a trivial proportion of the total, so I\\'m not even going to show it. In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion. This is around two-thirds of the total parameters in the network, and when you add it to everything that we had before for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised. It\\'s probably worth mentioning there\\'s another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very trivial proportion of the total. As to that second point of reflection, you might be wondering if this central toy example we\\'ve been spending so much time on reflects how facts are actually stored in real-large language models. It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction. It\\'s also true that the columns of that second matrix tell you what will be added to the result if that neuron is active. Both of those are just mathematical facts. However, the evidence does suggest that individual neurons very rarely represent a single clean feature like Michael Jordan. And there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition. This is a hypothesis that might help to explain both why the models are especially hard to interpret, and also why they scale surprisingly well. The basic idea is that if you have an end-dimensional space and you want to represent a bunch of different features using directions that are all perpendicular to one another in that space, you know, that way if you add a component in one direction it doesn\\'t influence any of the other directions. Then the maximum number of vectors you can fit is only n, the number of dimensions. To a mathematician actually this is the definition of dimension, but where it gets interesting is if you relax that constraint a little bit and you tolerate some noise. Say you allow those features to be represented by vectors that aren\\'t exactly perpendicular, they\\'re just nearly perpendicular, maybe between 89 and 91 degrees apart. If we were in two or three dimensions this makes no difference, that gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher dimensions the answer changes dramatically. I can give you a really quick and dirty illustration of this using some scrappy python that\\'s going to create a list of 100 dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions. This plot right here shows the distribution of angles between pairs of these vectors, so because they started at random those angles could be anything from 0 to 180 degrees, but you\\'ll notice that already even just for random vectors there\\'s this heavy bias for things to be closer to 90 degrees. Then what I\\'m going to do is run a certain optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another. After repeating this many different times, here\\'s what the distribution of angles looks like. We have to actually zoom in on it here, because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees. In general, a consequence of something known as the Johnson-Lindon-Strauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this, grows exponentially with the number of dimensions. This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions. It means that it\\'s possible for it to store many, many more ideas than there are dimensions in the space that it\\'s allotted. This might partially explain why model performance seems to scale so well with size. A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas. And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multi-layer perceptron that we just studied. That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed. But if it was doing that, what it means is that individual features aren\\'t going to be visible as a single neuron lighting up. It would have to look like some specific combination of neurons instead, a superposition. For any of you curious to learn more, a key relevant search term here is Sparce Autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they\\'re superimposed on all these neurons. I\\'ll link to a couple really great anthropic posts all about this. At this point, we haven\\'t touched every detail of a transformer, but you and I have hit the most important points. The main thing that I want to cover in a next chapter is the training process. On the one hand, the short answer for how training works is that it\\'s all back propagation, and we covered back propagation in a separate context with earlier chapters in the series. But there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of scaling laws. Quick note for the active followers among you, there are a number of non-machine learning-related videos that I\\'m excited to sync my teeth into before I make that next chapter, so it might be a while, but I do promise it\\'ll come in due time. So, I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. I\\'ll start with the first one. \\nAnswer:\\nThe choice between using ReLU (Rectified Linear Unit) or sigmoid functions in a neural network depends on the specific requirements of the network. Sigmoid functions were commonly used in earlier neural networks due to their ability to introduce non-linearity and limit the output to a range between 0 and 1. However, they have some disadvantages, such as the \"vanishing gradient\" problem, which can make training difficult. ReLU functions, on the other hand, have a simpler structure and do not suffer from the vanishing gradient problem. They also allow the network to learn more complex features and can lead to faster convergence during training. Therefore, ReLU functions are often preferred in modern neural networks. However, there are also other activation functions, such as tanh and leaky ReLU, that can be used depending on the specific application.'}\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.runnable import RunnableMap\n",
        "\n",
        "# Define the rag_chain_from_docs that processes documents and question using format_docs\n",
        "rag_chain_from_docs = (\n",
        "    {\n",
        "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()  # Ensure you're using the correct output parser\n",
        ")\n",
        "\n",
        "# Define the rag_chain_with_source to get documents from the retriever and return metadata and answer\n",
        "rag_chain_with_source = RunnableMap(\n",
        "    {\n",
        "        \"documents\": retriever | RunnablePassthrough(),  # Retrieve the documents and pass them as-is\n",
        "        \"question\": RunnablePassthrough(),  # Pass the input question unchanged\n",
        "    }\n",
        ") | {\n",
        "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],  # Extract metadata of the documents\n",
        "    \"answer\": rag_chain_from_docs,  # Use the rag_chain_from_docs to generate the answer\n",
        "}\n",
        "\n",
        "# Invoke the chain with a specific question\n",
        "response = rag_chain_with_source.invoke(\"When to use Relu or Sigmoid in a Neural Network?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2vD4P6Ekj7s",
        "outputId": "07c67242-18b3-4b92-ba51-fa0fb06aa054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: When to use Relu or Sigmoid in a Neural Network?\n",
            "\n",
            "Context: The hard assumption here is that you've watched Part 3, giving an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the\n",
            "relevant calculus. It's normal for this to be at least a little confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal\n",
            "is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has kind of a different feel from how most\n",
            "introductory calculus courses approach the subject. For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. Let's just start off with an\n",
            "extremely simple network, one where each layer has a single neuron in it. So this particular network is determined by three weights and three biases, and our goal is to understand\n",
            "how sensitive the cost function is to these variables. That way we know which adjustments to those terms is going to cause the most efficient decrease to the cost function. And\n",
            "we're just going to focus on the connection between the last two neurons. Let's label the activation of that last neuron with a superscript L indicating which layer it's in. So the\n",
            "activation of the previous neuron is a L minus 1. These are not exponents, they're just a way of indexing what we're talking about since I want to save subscripts for different\n",
            "indices later on. Now let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. So the cost of this simple\n",
            "network for a single training example is AL minus y squared. We'll denote the cost of that one training example as C0. As a reminder, this last activation is determined by a\n",
            "weight, which I'm going to call WL, times the previous neurons activation plus some bias, which I'll call BL. And then you pump that through some special nonlinear function like\n",
            "the sigmoid or a ray loop. It's actually going to make things easier for us if we give a special name to this weighted sum, like Z, with the same superscript as the relevant\n",
            "activations. So this is a lot of terms and a way that you might conceptualize it is that the weight, the previous action and the bias altogether are used to compute Z, which in\n",
            "turn lets us compute A, which finally, along with a constant Y, lets us compute the cost. And of course, AL minus 1 is influenced by its own weight and bias and such. But we're not\n",
            "going to focus on that right now. Now all of these are just numbers, right? And it can be nice to think of each one as having its own little number line. Our first goal is to\n",
            "understand how sensitive the cost function is to small changes in our weight, WL. Never phrase differently. What is the derivative of C with respect to WL? When you see this Dell W\n",
            "term, think of it as meaning some tiny nudge to W, like a change by 0.01. And think of this Dell C term as meaning whatever the resulting nudge to the cost is. What we want is\n",
            "their ratio. Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost. So we break things up by first\n",
            "looking at the ratio of a tiny change to ZL to this tiny change W, that is the derivative of ZL with respect to WL. Likewise, you then consider the ratio of the change to AL to the\n",
            "tiny change in ZL that caused it, as well as the ratio between the final nudge to C and this intermediate nudge to AL. This right here is the chain rule, where multiplying together\n",
            "these three ratios gives us the sensitivity of C to small changes in WL. So on screen right now, there's kind of a lot of symbols, and take a moment to just make sure it's clear\n",
            "what they all are. Because now we're going to compute the relevant derivatives. The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this means that\n",
            "its size is proportional to the difference between the networks output and the thing that we want it to be. So if that output was very different, even slight changes stand to have\n",
            "a big impact on the final cost function. The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever non-linearity you choose to use. And\n",
            "the derivative of ZL with respect to WL, in this case comes out just to be AL minus 1. Now I don't know about you, but I think it's easy to get stuck head down in the formulas\n",
            "without taking a moment to sit back and remind yourself of what they all actually mean. In the case of this last derivative, the amount that that small nudge to the weight\n",
            "influenced the last layer depends on how strong the previous neuron is. Remember, this is where that neurons that fire together wire together idea comes in. And all of this is the\n",
            "derivative with respect to WL only of the cost for a specific single training example. Since the full cost function involves averaging together all those costs across many\n",
            "different training examples, its derivative requires averaging this expression that we found over all training examples. And of course that is just one component of the gradient\n",
            "vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases. But even though that's just one of the many partial\n",
            "derivatives we need, it's more than 50% of the work. The sensitivity to the bias, for example, is almost identical. We just need to change out this del Z del W term for a del Z del\n",
            "B. And if you look at the relevant formula, that derivative comes out to be 1. Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this\n",
            "cost function is to the activation of the previous layer. Namely, this initial derivative in the chain rule expression, the sensitivity of Z to the previous activation, comes out\n",
            "to be the weight WL. And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of. Because now, we can just\n",
            "keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. And you might think that this is an overly\n",
            "simple example, since all layers just have one neuron, and that things are going to get exponentially more complicated for a real network. But honestly, not that much changes when\n",
            "we give the layers multiple neurons. Really it's just a few more indices to keep track of. Rather than the activation of a given layer simply being AL, it's also going to have a\n",
            "subscript, indicating which neuron of that layer it is. Let's go ahead and use the letter K to index the layer L minus 1, and J to index the layer L. For the cost, again, we look\n",
            "at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. That is, you take a sum over ALJ\n",
            "minus YJ squared. Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is. So let's call the weight of the edge connecting this\n",
            "Kth neuron to the Jth neuron, WLJK. Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix that I talked about in the Part 1\n",
            "video. Just as before, it's still nice to give a name to the relevant weighted sum, like Z, so that the activation of the last layer is just your special function, like the\n",
            "sigmoid, applied to Z. You can kind of see what I mean, right, where all of these are essentially the same equations that we had before in the one neuron per layer case. It's just\n",
            "that it looks a little more complicated. And indeed, the chain ruled the rivetive expression, describing how sensitive the cost is to a specific weight, looks essentially the same.\n",
            "I'll leave it to you to pause and think about each of those terms if you want. What does change here, though, is the derivative of the cost with respect to one of the activations\n",
            "in the layer L minus 1. In this case, the difference is that the neuron influences the cost function through multiple different paths. It is on the one hand, it influences AL0,\n",
            "which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. And that, well, that's\n",
            "pretty much it. Once you know how sensitive the cost function is to the activations in this second to last layer, you can just repeat the process for all the weights and biases\n",
            "feeding into that layer. So pat yourself on the back. If all of this makes sense, you have now looked deep into the heart of back propagation, the workhorse behind how neural\n",
            "networks learn. These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly\n",
            "stepping downhill. If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around. So don't worry if it takes time for your mind to digest\n",
            "it all. Here we tackle back propagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walk-\n",
            "through for what the algorithm is actually doing without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the\n",
            "calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds\n",
            "forward information. Here, we're doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons, and\n",
            "I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as a tensor. I'm\n",
            "also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a\n",
            "certain cost function. As a quick reminder for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted\n",
            "it to give, and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results,\n",
            "this gives you the total cost of the network. As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of\n",
            "this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation,\n",
            "the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind\n",
            "right now, is that because thinking of the gradient vector as a direction in 13,000 dimensions is to put it lightly beyond the scope of our imaginations, there's another way you\n",
            "can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the\n",
            "process I'm about to describe when you compute the negative gradient and the component associated with the weight on this edge here comes out to be 3.2, while the component\n",
            "associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if\n",
            "you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight\n",
            "would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you\n",
            "unwrap what each part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive. It's just that there's a lot of little adjustments\n",
            "getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through those effects that each training\n",
            "example is having on the weights and biases. Because the cost function involves averaging a certain cost per example, over all the tens of thousands of training examples, the way\n",
            "that we adjust the weights and biases for a single gradient descent step also depends on every single example, or rather, in principle it should, but for computational efficiency,\n",
            "we're going to do a little trick later to keep you from needing to hit every single example for every single step. In other case, right now, all we're going to do is focus our\n",
            "attention on one single example, this image of a two. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where\n",
            "the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now, we can't directly change\n",
            "those activations. We only have influence on the weights and biases. But it is helpful to keep track of which adjustments we wish should take place to that output layer. And since\n",
            "we want it to classify the image as a two, we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be\n",
            "proportional to how far away each current value is from its target value. For example, the increase to that number two neurons activation is in a sense more important than the\n",
            "decrease to the number eight neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we\n",
            "wish to increase. Remember, that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias, which is all then plugged into\n",
            "something like the sigmoid squishification function, or a ray-lew. So there are three different avenues that can team up together to help increase that activation. You can increase\n",
            "the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing just on how the weights should be adjusted? Notice how the weights\n",
            "actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect, since those weights are multiplied by\n",
            "larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of\n",
            "connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each\n",
            "component should get nudged up or down, we care about which ones give you the most bang for your butt. This, by the way, is at least somewhat reminiscent of a theory in\n",
            "neuroscience for how biological networks of neurons learn, heavy in theory. Often summed up in the phrase, neurons that fire together, wire together. Here, the biggest increases to\n",
            "weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that\n",
            "are firing while seeing a two get more strongly linked to those firing when thinking about a two. To be clear, I really am not in a position to make statements one way or another\n",
            "about whether artificial networks of neurons behave anything like biological brains, and this fires together, wire together idea comes with a couple meaningful asterisks. But taken\n",
            "as a very loose analogy, I do find it interesting to note. Anyway, the third way that we can help increase this neuron's activation is by changing all the activations in the\n",
            "previous layer. Namely, if everything connected to that digit two neuron with a positive weight, got brighter, and if everything connected with a negative weight got dimmer, then\n",
            "that digit two neuron would become more active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the\n",
            "size of the corresponding weights. Now, of course, we cannot directly influence those activations. We only have control over the weights and biases. But just as with the last\n",
            "layer, it's helpful to just keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit two output neuron wants.\n",
            "Remember, we also want all of the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to\n",
            "that second to last layer. So the desire of this digit two neuron is added together with the desires of all the other output neurons for what should happen to this second to last\n",
            "layer. Again, in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating\n",
            "backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those,\n",
            "you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards\n",
            "through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only\n",
            "listen to what that two wanted, the network would ultimately be incentivized just to classify all images as a two. What you do is you go through this same back property for every\n",
            "other training example, recording how each of them would like to change the weights and the biases. And you average together those desired changes. This collection here of the\n",
            "average to nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I\n",
            "say loosely speaking only because I have yet to get quantitatively precise about those nudges. But if you understood every change that I just referenced, why some are\n",
            "proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what back propagation is actually doing. By the way, in practice, it\n",
            "takes computers an extremely long time to add up the influence of every single training example, every single gradient descent step. So here's what's commonly done instead. You\n",
            "randomly shuffle your training data and then divide it into a whole bunch of mini-batches. Let's say each one having 100 training examples. Then you compute a step according to the\n",
            "mini-batch. It's not going to be the actual gradient to the cost function, which depends on all of the training data, not this tiny subset. So it's not the most efficient step\n",
            "downhill. But each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speed up. If you would applaud the\n",
            "trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps, rather than a\n",
            "carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as\n",
            "stochastic gradient descent. There's kind of a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single\n",
            "training example would like to nudge the weights and biases. Not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes\n",
            "cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired\n",
            "changes that you get. But that's computationally slow, so instead you randomly subdivide the data into these mini batches and compute each step with respect to a mini batch.\n",
            "Repeatedly going through all of the mini batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network is going\n",
            "to end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing Backprop actually corresponds with\n",
            "something that you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it\n",
            "gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the\n",
            "underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this\n",
            "algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten\n",
            "digits such a nice example is that there exists the M-NIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in\n",
            "machine learning will be familiar with is just getting the labeled training data that you actually need, whether that's having people label tens of thousands of images or whatever\n",
            "other data type you might be dealing with. Last video, I laid out the structure of a neural network. I'll give a quick recap here just so that it's fresh in our minds and then I\n",
            "have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine\n",
            "learning works as well. Then after that, we're going to dig in a little more to how this particular network performs and what those hidden layers of neurons end up actually looking\n",
            "for. As a reminder, our goal here is the classic example of handwritten digit recognition, the Hello World of Neural Networks. These digits are rendered on a 28x28 pixel grid, each\n",
            "pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons in the input layer of the network. And then the activation for each neuron\n",
            "in the following layers is based on a weighted sum of all the activations in the previous layer plus some special number called a bias. Then you compose that sum with some other\n",
            "function, like the sigmoid squishification or a ray-loo, the way that I walked through last video. In total, given the somewhat arbitrary choice of two hidden layers here with 16\n",
            "neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does. And what we mean when\n",
            "we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit. And remember, the motivation that we had\n",
            "in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last\n",
            "one could just piece together those patterns to recognize digits. So here we learn how the network learns. What we want is an algorithm where you can show this network a whole\n",
            "bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those\n",
            "13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered structure will mean that what it learns generalizes to images beyond that\n",
            "training data. And the way we test that is that after you train the network, you show it more labeled data, that it's never seen before, and you see how accurately it classifies\n",
            "those new images. Fortunately for us, and what makes this such a common example to start with, is that the good people behind the M-NIST database have put together a collection of\n",
            "tens of thousands of handwritten digit images, each one labeled with the numbers that they're supposed to be. And it's provocative as it is to describe a machine as learning. Once\n",
            "you actually see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like, a calculus exercise. I mean, basically, it comes down to finding the minimum\n",
            "of a certain function. Remember, conceptually, we're thinking of each neuron as being connected to all of the neurons in the previous layer, and the weights in the weighted sum\n",
            "defining its activation are kind of like the strengths of those connections. The bias is some indication of whether that neuron tends to be active or inactive. And to start things\n",
            "off, we're just going to initialize all of those weights and biases totally randomly. Needless to say, this network is going to perform pretty horribly on a given training example,\n",
            "since it's just doing something random. For example, you feed in this image of a three, and the output layer just looks like a mess. So what you do is you define a cost function, a\n",
            "way of telling the computer, no, bad computer, that output should have activations, which are zero for most neurons, but one for this neuron. What you gave me is utter trash. To\n",
            "say that a little more mathematically, what you do is add up the squares of the differences between each of those trash output activations and the value that you want them to have.\n",
            "And this is what we'll call the cost of a single training example. Notice, this sum is small when the network confidently classifies the image correctly, but it's large when the\n",
            "network seems like it doesn't really know what it's doing. So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.\n",
            "This average cost is our measure for how lousy the network is and how bad the computer should feel. And that's a complicated thing. Remember how the network itself was basically a\n",
            "function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output. And in a sense, it's parameterized by all these weights and biases.\n",
            "Well the cost function is a layer of complexity on top of that. It takes as its input, those 13,000 or so weights and biases, and it spits out a single number describing how bad\n",
            "those weights and biases are. And the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data. That's a lot to think about. But\n",
            "just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to change those weights and biases so that it gets better. To make it easier,\n",
            "rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output. How do you find an\n",
            "input that minimizes the value of this function? Circular students will know that you can sometimes figure out that minimum explicitly. But that's not always feasible for really\n",
            "complicated functions. Certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function. A more flexible tactic is to start at any\n",
            "all input and figure out which direction you should step to make that output lower. Specifically, if you can figure out the slope of the function where you are, then shift to the\n",
            "left if that slope is positive and shift the input to the right if that slope is negative. If you do this repeatedly, at each point checking the new slope and taking the\n",
            "appropriate step, you're going to approach some local minimum of the function. And the image you might have in mind here is a ball rolling down a hill. And notice, even for this\n",
            "really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at. There's no guarantee that the local\n",
            "minimum you land in is going to be the smallest possible value of the cost function. That's going to carry over to our neural network case as well. And I also want you to notice\n",
            "how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that kind of helps you\n",
            "from overshooting. Bumping up the complexity a bit, imagine instead a function with two inputs and one output. You might think of the input space as the x, y plane and the cost\n",
            "function as being graft as a surface above it. Now instead of asking about the slope of the function, you have to ask which direction should you step in this input space so as to\n",
            "decrease the output of the function most quickly. In other words, what's the downhill direction? And again, it's helpful to think of a ball rolling down that hill. Most of you\n",
            "familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, basically which direction should you step to increase the\n",
            "function most quickly. Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly. And even more than that, the\n",
            "length of this gradient vector is actually an indication for just how steep that steepest slope is. Now if you're unfamiliar with multivariable calculus and you want to learn more,\n",
            "check out some of the work that I did for Khan Academy on the topic. Honestly though, all that matters for you and me right now is that in principle there exists a way to compute\n",
            "this vector. This vector that tells you what the downhill direction is and how steep it is. You'll be okay if that's all you know and you're not rock solid on the details. Because\n",
            "if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill and just repeat that over and over. It's the\n",
            "same basic idea for a function that has 13,000 inputs instead of two inputs. Imagine organizing all 13,000 weights and biases of our network into a giant column vector. The\n",
            "negative gradient of the cost function is just a vector. It's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to\n",
            "cause the most rapid decrease to the cost function. And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output\n",
            "of the network on each piece of training data look less like a random array of 10 values and more like an actual decision that we want it to make. It's important to remember, this\n",
            "cost function involves an average over all of the training data. So if you minimize it, it means it's a better performance on all of those samples. The algorithm for computing this\n",
            "gradient efficiently, which is effectively the heart of how a neural network learns, is called back propagation. And it's what I'm going to be talking about next video. There I\n",
            "really want to take the time to walk through what exactly happens to each weight and each bias for a given piece of training data. Trying to give an intuitive feel for what's\n",
            "happening beyond the pile of relevant calculus and formulas. Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean\n",
            "when we talk about a network learning is that it's just minimizing a cost function. And notice, one consequence of that is that it's important for this cost function to have a nice\n",
            "smooth output so that we can find a local minimum by taking little steps down hill. This is why, by the way, artificial neurons have continuously ranging activations, rather than\n",
            "simply being active or inactive in a binary way, the way that biological neurons are. This process of repeatedly nudging an input of a function by some multiple of the negative\n",
            "gradient is called gradient descent. It's a way to converge toward some local minimum of a cost function, basically a valley in this graph. I'm still showing the picture of a\n",
            "function with two inputs, of course, because nudges in a 13,000-dimensional input space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to\n",
            "think about this. Each component of the negative gradient tells us two things. The sign, of course, tells us whether the corresponding component of the input vector should be\n",
            "nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you which changes matter more. You see, in our network, an adjustment to one of\n",
            "the weights might have a much greater impact on the cost function than the adjustment to some other weight. Some of these connections just matter more for our training data. So a\n",
            "way that you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of\n",
            "these changes is going to carry the most bang for your buck. This really is just another way of thinking about direction. To take a simpler example, if you have some function with\n",
            "two variables as an input, and you compute that it's gradient at some particular point, comes out as 3-1. Then on the one hand, you can interpret that as saying that when you're\n",
            "standing at that input, moving along this direction increases the function most quickly. But when you graph the function above the plane of input points, that vector is what's\n",
            "giving you the straight uphill direction. But another way to read that is to say that changes to this first variable have three times the importance as changes to the second\n",
            "variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck. Alright, let's zoom out and sum up where we are so\n",
            "far. The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted sums. The cost function is a layer of complexity on top of that.\n",
            "It takes the 13,000 weights and biases as inputs, and spits out a single measure of laziness based on the training examples. And the gradient of the cost function is one more layer\n",
            "of complexity still. It tells us what nudges to all of these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying\n",
            "which changes to which weights matter the most. So, when you initialize the network with random weights and biases and adjust them many times based on this gradient descent\n",
            "process, how well does it actually perform on images that it's never seen before? Well the one that I've described here, with the two hidden layers of 16 neurons each, goes in\n",
            "mostly for aesthetic reasons. Well, it's not bad. It classifies about 96% of the new images that it sees correctly. And honestly, if you look at some of the examples that it messes\n",
            "up on, you kind of feel compelled to cut it a little slack. Now, if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%. And that's\n",
            "pretty good. It's not the best. You can certainly get better performance by getting more sophisticated than this plain vanilla network. But given how daunting the initial task is,\n",
            "I just think there's something incredible about any network doing this well on images that it's never seen before, given that we never specifically told it what patterns to look\n",
            "for. Originally, the way that I motivated this structure was by describing a hope that we might have, that the second layer might pick up on little edges, that the third layer\n",
            "would piece together those edges to recognize loops in longer lines, and that those might be piece together to recognize digits. So is this what our network is actually doing?\n",
            "Well, for this one, at least, not at all. Remember how last video we looked at how the weights of the connections from all of the neurons in the first layer to a given neuron in\n",
            "the second layer can be visualized as a given pixel pattern that that second layer neuron is picking up on? Well, when we actually do that, for the weights associated with these\n",
            "transitions from the first layer to the next. Instead of picking up on isolated little edges here and there, they look almost random, just with some very loose patterns in the\n",
            "middle there. It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that,\n",
            "despite successfully classifying most images, doesn't exactly pick up on the patterns that we might have hoped for. And to really drive this point home, watch what happens when you\n",
            "input a random image. If the system was smart, you might expect it to either feel uncertain, maybe, not really activating any of those 10 output neurons or activating them all\n",
            "evenly. But instead, it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as it does that an actual image of a 5 is a 5. Fraze\n",
            "differently? Even if this network can recognize digits pretty well, it has no idea how to draw them. A lot of this is because it's such a tightly constrained training setup. I\n",
            "mean, put yourself in the network's shoes here. From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid. And its\n",
            "cost function just never gave it any incentive to be anything but utterly confident in its decisions. So with this is the image of what those second layer neurons are really doing,\n",
            "you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. That's just not at all what it ends up doing. Well this is not meant to\n",
            "be our end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s and 90s. And you do need to understand it before you can understand\n",
            "more detailed modern variants, and it clearly is capable of solving some interesting problems. But the more you dig in to what those hidden layers are really doing, the less\n",
            "intelligent it seems. Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow. One\n",
            "pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you\n",
            "wanted it to better pick up on things like edges and patterns. But better than that, to actually engage with the material, I highly recommend the book by Michael Neilsen on deep\n",
            "learning and neural networks. In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that\n",
            "code is doing. What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Neilsen's\n",
            "efforts. I've also linked a couple other resources that I like a lot in the description, including the phenomenal and beautiful blog posts by Chris Ola and the articles in Distill.\n",
            "To close things off here for the last few minutes, I want to jump back into a snippet of the interview that I had with Lisha Lee. You might remember her from the last video, she\n",
            "did her PhD work in deep learning. And in this little snippet, she talks about two recent papers that really dig in to how some of the more modern image recognition networks are\n",
            "actually learning. Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition,\n",
            "and instead of training it on a properly labeled data set, it shuffled all of the labels around before training. Obviously the testing accuracy here was going to be no better than\n",
            "random, since everything's just randomly labeled. But it was still able to achieve the same training accuracy as you would on a properly labeled data set. Basically, the millions\n",
            "of weights for this particular network were enough for it to just memorize the random data, which kind of raises the question for whether minimizing this cost function actually\n",
            "corresponds to any sort of structure in the image, or is it just memorization? It reminds the entire data set of what the correct classification is. And so a couple of half a year\n",
            "later at ICML this year, there was not exactly rebuttal paper, paper that addressed some aspects of like, hey, actually these networks are doing something a little bit smarter than\n",
            "that. If you look at that accuracy curve, if you were just training on a random data set, that curve sort of went down very slowly in almost a linear fashion. So you're really\n",
            "struggling to find that local minimum of possible, the right weights that would get you that accuracy. Whereas if you're actually training on a structure data set, one that has the\n",
            "right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level. And so in some sense, it was easier to find\n",
            "that local maxima. And so it was also interesting about that, is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about\n",
            "the network layers. But one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal\n",
            "quality. So in some sense, if your data set a structure, you should be able to find that much more easily. My thanks, as always, to those of you supporting on Patreon. I've said\n",
            "before just what a game changer in Patreon is, but these videos really would not be possible without you. I also want to give a special thanks to the VC firm Amplify Partners in\n",
            "their support of these initial videos in the series. This is a 3. It's slobily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble\n",
            "recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean this, this and this are also recognizable as\n",
            "3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3\n",
            "are very different from the ones firing when you see this 3. But something in that crazy smart visual cortex of yours resolves these as representing the same idea, while at the\n",
            "same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this, and\n",
            "outputs a single number between 0 and 10, telling you what it thinks the digit is, while the task goes from comically trivial to dauntingly difficult. Unless you've been living\n",
            "under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is\n",
            "show you what a neural network actually is, assuming no background, and to help visualize what it's doing. Not as a buzzword, but as a piece of math. My hope is just that you come\n",
            "away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote unquote learning. This video is\n",
            "just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we're going to do is put together a neural network that can\n",
            "learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quote here, because at the end of the\n",
            "two videos, I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There\n",
            "are many, many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos, you and I are\n",
            "just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and\n",
            "trust me, it still has plenty of complexity for us to wrap our minds around. But even in this simplest form, it can learn to recognize handwritten digits, which is a pretty cool\n",
            "thing for a computer to be able to do. And at the same time, you'll see how it does fall short of a couple hopes that we might have for it. As the name suggests, neural networks\n",
            "are inspired by the brain, but let's break that down. What are the neurons and in what sense are they linked together? Right now, when I say neuron, all I want you to think about\n",
            "is a thing that holds a number, specifically a number between zero and one. It's really not more than that. For example, the network starts with a bunch of neurons corresponding to\n",
            "each of the 28 times 28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the gray scale value of the corresponding pixel,\n",
            "ranging from zero for black pixels up to one for white pixels. This number inside the neuron is called its activation. And the image you might have in mind here is that each neuron\n",
            "is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each\n",
            "representing one of the digits. The activation in these neurons, again, some number that's between zero and one, represents how much the system thinks that a given image\n",
            "corresponds with a given digit. There's also a couple layers in between called the hidden layers, which for the time being, should just be a giant question mark. For how on earth\n",
            "this process of recognizing digits is going to be handled. In this network, I chose two hidden layers, each one with 16 neurons. And admittedly, that's kind of an arbitrary choice.\n",
            "To be honest, I chose two layers based on how I want to motivate the structure in just a moment. And 16, well, that was just a nice number to fit on the screen. In practice, there\n",
            "is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer determine the activations of the next layer. And of course,\n",
            "the heart of the network, as an information processing mechanism, comes down to exactly how those activations from one layer bring about activations in the next layer. It's meant\n",
            "to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now, the network I'm showing here has already been\n",
            "trained to recognize digits. And let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer, according to the brightness\n",
            "of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer, which causes some pattern in the one after it, which finally gives some\n",
            "pattern in the output layer. And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. And before jumping into the\n",
            "math for how one layer influences the next or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.\n",
            "What are we expecting here? What is the best hope for what those middle layers might be doing? Well, when you or I recognize digits, we piece together various components. A 9 has a\n",
            "loop up top and a line on the right. And 8 also has a loop up top, but it's paired with another loop down low. A 4 basically breaks down into 3 specific lines and things like that.\n",
            "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents. That anytime you feed in an image with, say, a loop\n",
            "up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1. And I don't mean this specific loop of pixels. The hope would be that any\n",
            "generally loopy pattern towards the top sets off this neuron. That way, going from the third layer to the last one, just requires learning which combination of subcomponents\n",
            "corresponds to which digits. Of course, that just kicks the problem down the road because how would you recognize these sub components or even learn what the right subcomponents\n",
            "should be? And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment. Recognizing a loop can also break down into\n",
            "subproblems. One reasonable way to do this would be to first recognize the various little edges that make it up. Similarly, a long line, like the kind you might see in the digits 1\n",
            "or 4 or 7, well that's really just a long edge, or maybe you think of it as a certain pattern of several smaller edges. So maybe, our hope is that each neuron in the second layer\n",
            "of the network corresponds with the various relevant little edges. Maybe, when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10\n",
            "specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with the 9. Whether or\n",
            "not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network. But this is a hope that we might have, a sort\n",
            "of goal with the layered structure like this. Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition\n",
            "tasks. And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction. Parsing speech, for example,\n",
            "involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract\n",
            "thoughts, etc. But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the activations in\n",
            "the next. The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits. And to zoom in on one very specific\n",
            "example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here. The question at hand is what\n",
            "parameters should the network have? What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern,\n",
            "or the pattern that several edges can make a loop and other such things? Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons\n",
            "from the first layer. These weights are just numbers. Then take all of those activations from the first layer and compute their weighted sum according to these weights. I find it\n",
            "helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights and red pixels to indicate\n",
            "negative weights, where the brightness of that pixel is some loose depiction of the weights value. Now if we made the weights associated with almost all of the pixels zero, except\n",
            "for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in\n",
            "the region that we care about. And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding\n",
            "pixels. Then the sum is largest when those middle pixels are bright, but the surrounding pixels are darker. When you compute a weighted sum like this, you might come out with any\n",
            "number, but for this network, what we want is for activations to be some value between zero and one. So a common thing to do is to pump this weighted sum into some function that\n",
            "squishes the real number line into the range between zero and one. And a common function that does this is called the sigmoid function, also known as a logistic curve, basically\n",
            "very negative inputs end up close to zero, very positive inputs end up close to one, and it just steadily increases around the input zero. So the activation of the neuron here is\n",
            "basically a measure of how positive the relevant weighted sum is. But maybe it's not that you want the neuron to light up when the weighted sum is bigger than zero. Maybe you only\n",
            "want it to be active when the sum is bigger than say 10. That is, you want some bias for it to be inactive. What we'll do then is just add in some other number, like negative 10,\n",
            "to this weighted sum, before plugging it through the sigmoid squishing function. That additional number is called the bias. So the weights tell you what pixel pattern this neuron\n",
            "in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active. And that is just one\n",
            "neuron. Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated\n",
            "with it. Also, each one has some bias, some other number that you add onto the weighted sum before squishing it with the sigmoid. And that's a lot to think about. With this hidden\n",
            "layer of 16 neurons, that's a total of 784 x 16 weights, along with 16 biases. And all of that is just the connections from the first layer to the second. The connections between\n",
            "the other layers also have a bunch of weights and biases associated with them. All said and done, this network has almost exactly 13,000 total weights and biases. 13,000 knobs and\n",
            "dials that can be tweaked and turned to make this network behave in different ways. So when we talk about learning, what that's referring to is getting the computer to find a valid\n",
            "setting for all of these many, many numbers so that it'll actually solve the problem at hand. One thought experiment that is at once fun and kind of horrifying is to imagine\n",
            "sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on\n",
            "patterns, etc. I personally find this satisfying rather than just treating the network as a total black box because when the network doesn't perform the way you anticipate, if\n",
            "you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to\n",
            "improve. Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and\n",
            "really expose the full space of possible solutions. By the way, the actual function here is a little cumbersome to write down, don't you think? So let me show you a more\n",
            "notationally compact way that these connections are represented. This is how you'd see it if you choose to read out more about neural networks. Organize all of the activations from\n",
            "one layer into a column as a vector. Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular\n",
            "neuron in the next layer. What that means is that taking the weighted sum of the activations in the first layer, according to these weights, corresponds to one of the terms in the\n",
            "matrix vector product of everything we have on the left here. By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you\n",
            "who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3. Back to our\n",
            "expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector and adding the entire\n",
            "vector to the previous matrix vector product. Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply\n",
            "the sigmoid function to each specific component of the resulting vector inside. So once you write down this weight matrix and these vectors as their own symbols, you can\n",
            "communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression. And this makes the relevant code both a lot simpler and\n",
            "a lot faster, since many libraries optimize the heck out of matrix multiplication. Remember how earlier I said these neurons are simply things that hold numbers? Well, of course\n",
            "the specific numbers that they hold depends on the image you feed in. So it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the\n",
            "neurons in the previous layer, and spits out a number between 0 and 1. Really, the entire network is just a function, one that takes in 784 numbers as an input, and spits out 10\n",
            "numbers as an output. It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and\n",
            "which involves iterating many matrix vector products and the sigmoid squishification function. But it's just a function nonetheless. And in a way, it's kind of reassuring that it\n",
            "looks complicated. I mean, if there were any simpler, what hope would we have that it could take on the challenge of recognizing digits? And how does it take on that challenge? How\n",
            "does this network learn the appropriate weights and biases just by looking at data? Oh, that's what I'll show in the next video, and I'll also dig a little more into what this\n",
            "particular network we're seeing is really doing. Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but\n",
            "realistically, most of you don't actually receive notifications from YouTube, do you? Maybe more honestly, I should say subscribe so that the neural networks that underlie\n",
            "YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you. Anyway, stay posted for more. Thank you very much to\n",
            "everyone supporting these videos on Patreon. I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons,\n",
            "you can look out for updates there. To close things off here, I have with me Lisa Lee, who did her PhD work on the theoretical side of deep learning, and who currently works at a\n",
            "venture capital firm called Amplify Partners, who kindly provided some of the funding for this video. So, Lisa, one thing I think we should quickly bring up is this sigmoid\n",
            "function. As I understand it, early networks use this to squish the relevant weighted sum into that interval between 0 and 1, you know, kind of motivated by this biological analogy\n",
            "of neurons either being inactive or active. Exactly. But relatively few modern networks actually use sigmoid anymore. It's kind of old school, right? Yeah, or rather, relu seems to\n",
            "be much easier to train. And relu stands for rectified linear unit? Yes, it's this kind of function where you're just taking a max of 0 and a, where a is given by what you were\n",
            "explaining in the video, and what this was sort of motivated from, I think, was a partially biological analogy with how neurons would either be activated or not. And so if it\n",
            "passes a certain threshold, it would be the identity function, but if it did not, then it would just not be activated, so be zero. So it's kind of a simplification. Using sigmoids\n",
            "didn't help training, or it was very difficult to train at some point, and people just tried relu, and it happened to work very well for these incredibly deep neural networks. All\n",
            "right. Thank you, Lisa. If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts\n",
            "basketball. This would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport. And I think in\n",
            "general anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts. So a reasonable question you could ask is, how exactly\n",
            "does that work, and where do those facts live? Last December a few researchers from Google Deep Mind posted about work on this question, and they were using this specific example\n",
            "of matching athletes to their sports. And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including\n",
            "the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short. In\n",
            "the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other\n",
            "modern AI. In the most recent chapter we were focusing on a piece called Attention, and the next step for you and me is to dig into the details of what happens inside these multi-\n",
            "layer perceptrons, which make up the other big portion of the network. The computation here is actually relatively simple, especially when you compare it to attention. It boils\n",
            "down essentially to a pair of matrix multiplications with a simple something in between. However, interpreting what these computations are doing is exceedingly challenging. Our\n",
            "main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at\n",
            "least in principle, store a concrete fact. Specifically, it'll be storing the fact that Michael Jordan plays basketball. I should mention the layout here is inspired by a\n",
            "conversation I had with one of those deep-mind researchers, Neil Nanda. For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a\n",
            "basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow. You and I have been studying a model that's trained to take in a\n",
            "piece of text and predict what comes next. That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and\n",
            "each token is associated with a high-dimensional vector, which is to say a long list of numbers. This sequence of vectors then repeatedly passes through two kinds of operation.\n",
            "Attention, which allows the vectors to pass information between one another, and then the multi-layer perceptrons, the thing that we're going to dig into today. And also there's a\n",
            "certain normalization step in between. After the sequence of vectors has flowed through many many different iterations of both of these blocks, by the end, the hope is that each\n",
            "vector has soaked up enough information, both from the context, all of the other words and the input, and also from the general knowledge that was baked into the model weights\n",
            "through training, that it can be used to make a prediction of what token comes next. One of the key ideas that I want you to have in your mind is that all of these vectors live in\n",
            "a very very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning. So a very classic example that I like to refer\n",
            "back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like\n",
            "uncle, you land somewhere very very close to the corresponding feminine noun. In this sense, this particular direction encodes gender information. The idea is that many other\n",
            "distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent. In a transformer, these vectors don't merely\n",
            "encode the meaning of a single word, though. As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's\n",
            "knowledge. Ultimately, each one needs to encode something far far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next. We've already\n",
            "seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is\n",
            "that they offer extra capacity to store facts. Like I said, the lesson here is going to center on the concrete toy example of how exactly it could store the fact that Michael\n",
            "Jordan plays basketball. Now this toy example is going to require that you and I make a couple of assumptions about that high-dimensional space. First, we'll suppose that one of\n",
            "the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third\n",
            "direction will represent the idea of basketball. So specifically what I mean by this is if you look in the network and you plug out one of the vectors being processed, if its dot\n",
            "product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name. Otherwise, that dot\n",
            "product would be zero or negative, meaning the vector doesn't really align with that direction. And for simplicity, let's completely ignore the very reasonable question of what it\n",
            "might mean if that dot product was bigger than one. Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan, or basketball.\n",
            "So let's say a vector is meant to represent the full name Michael Jordan, then its dot product with both of these directions would have to be one. Since the text Michael Jordan\n",
            "spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to\n",
            "ensure that it can encode both names. With all of those as the assumptions, let's now dive into the meat of the lesson. What happens inside a multi-layer perceptron? You might\n",
            "think of this sequence of vectors flowing into the block, and remember each vector was originally associated with one of the tokens from the input text. What's going to happen is\n",
            "that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end we'll get another vector with the same\n",
            "dimension. That other vector is going to get added to the original one that flowed in, and that sum is the result flowing out. This sequence of operations is something you apply to\n",
            "every vector in the sequence associated with every token in the input, and it all happens in parallel. In particular, the vectors don't talk to each other in this step, they're all\n",
            "kind of doing their own thing. And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this\n",
            "block, we effectively understand what happens to all of them. When I say this block is going to encode the fact that Michael Jordan plays basketball, what I mean is that if a\n",
            "vector flows in that encodes, first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is\n",
            "what we'll add on to the vector in that position. The first step of this process looks like multiplying that vector by a very big matrix, no surprises there, this is deep learning,\n",
            "and this matrix like all of the other ones we've seen is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get\n",
            "tweaked and tuned to determine what the model behavior is. Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector and\n",
            "taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding. For example, suppose that very first row happened to equal\n",
            "this first name Michael direction that were presuming exists, that would mean that the first component in this output, this dot product right here, would be one if that vector\n",
            "encodes the first name Michael and zero or negative otherwise. Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus\n",
            "last name Jordan direction. And for simplicity, let me go ahead and write that down as m plus j, then taking a dot product with this embedding E, things distribute really nicely,\n",
            "so it looks like m.e plus j.e and notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan and otherwise it would be one or\n",
            "something smaller than one. And that's just one row in this matrix. You might think of all of the other rows as in parallel asking some other kinds of questions probing at some\n",
            "other sorts of features of the vector being processed. Very often this step also involves adding another vector through the output, which is full of model parameters learned from\n",
            "data, this other vector is known as the bias. For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final\n",
            "output looks like that relevant dot product, but minus one. You might very reasonably ask why I would want you to assume that the model has learned this. And in a moment, you'll\n",
            "see why it's very clean and nice if we have a value here, which is positive, if and only if a vector encodes the full name Michael Jordan and otherwise it's zero or negative. The\n",
            "total number of rows in this matrix, which is something like the number of questions being asked in the case of GPT3, whose numbers we've been following is just under 50,000. In\n",
            "fact, it's exactly four times the number of dimensions in this embedding space. That's a design choice you could make it more, you could make it less, but having a clean multiple\n",
            "tends to be friendly for hardware. Since this matrix full of weights maps us into a higher dimensional space, I'm going to give it the shorthand W up. I'll continue labeling the\n",
            "vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram. At this point, a problem is that this operation is purely linear,\n",
            "but language is a very non-linear process. If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps\n",
            "and also Alexis plus Jordan, despite those being unrelated conceptually. What you really want is a simple yes or no for the full name. So the next step is to pass this large\n",
            "intermediate vector through a very simple non-linear function. A common choice is one that takes all of the negative values and maps them to zero, and leaves all of the positive\n",
            "values unchanged. And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or Rayloo for short.\n",
            "Here's what the graph looks like. So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan, and zero\n",
            "or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value where all of the zero and negative values just get clipped to zero. So this output\n",
            "would be one for the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the behavior of an AND gate. Often models will use a slightly modified\n",
            "function that's called the J-LU, which has the same basic shape, it's just a bit smoother, but for our purposes, it's a little bit cleaner if we only think about the Rayloo. Also,\n",
            "when you hear people refer to the neurons of a transformer, they're talking about these values right here. Whenever you see that common neural network picture with a layer of dots\n",
            "and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix\n",
            "multiplication, followed by some simple, termwise non-linear function like a Rayloo. You would say that this neuron is active whenever this value is positive, and that it's\n",
            "inactive if that value is zero. The next step looks very similar to the first one. You multiply by a very large matrix and you add on a certain bias term. In this case, the number\n",
            "of dimensions in the output is back down to the size of that embedding space, so I'm going to go ahead and call this the down projection matrix. And this time, instead of thinking\n",
            "of things row by row, it's actually nicer to think of it column by column. You see, another way that you can hold matrix multiplication in your head is to imagine taking each\n",
            "column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns. The reason it's nicer to\n",
            "think about this way is because here, the columns have the same dimension as the embedding space, so we can think of them as directions in that space. For instance, we will imagine\n",
            "that the model has learned to make that first column into this basket ball direction that we suppose exists. What that would mean is that when the relevant neuron in that first\n",
            "position is active, we'll be adding this column to the final result. But if that neuron was inactive, if that number was zero, then this would have no effect. And it doesn't just\n",
            "have to be basketball. The model could also bake into this column many other features that it wants to associate with something that has the full name Michael Jordan. And at the\n",
            "same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active. And if you have a bias in this\n",
            "case, it's something that you're just adding every single time, regardless of the neuron values. You might wonder what's that doing, as with all parameter field objects here, it's\n",
            "kind of hard to say exactly, maybe there's some book keeping that the network needs to do, but you can feel free to ignore it for now. Making our notation a little more compact\n",
            "again, I'll call this big matrix W down, and similarly call that bias vector B down and put that back into our diagram. Like I previewed earlier, what you do with this final result\n",
            "is add it to the vector that flowed into the block at that position, and that gets you this final result. So for example, if the vector flowing in encoded both first name Michael\n",
            "and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction. So what pops out will encode all of those\n",
            "together. And remember, this is a process happening to every one of those vectors in parallel. In particular, taking the GPT-3 numbers, it means that this block doesn't just have\n",
            "50,000 neurons in it, it has 50,000 times the number of tokens in the input. So that is the entire operation, two matrix products each with a bias added, and a simple clipping\n",
            "function in between. Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there. In that\n",
            "example, it was trained to recognize handwritten digits. Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture, and any\n",
            "attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high dimensional embedding space. That is the core\n",
            "lesson, but I do want to step back and reflect on two different things. The first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact\n",
            "about higher dimensions that I actually didn't know until I dug into Transformers. In the last two chapters, you and I started counting up the total number of parameters in GPT-3\n",
            "and seeing exactly where they live. So let's quickly finish up the game here. I already mentioned how this up projection matrix has just under 50,000 rows, and that each row\n",
            "matches the size of the embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has\n",
            "the same number of parameters just with a transpose to shape. So together, they give about 1.2 billion parameters. The bias vector also accounts for a couple more parameters, but\n",
            "it's a trivial proportion of the total, so I'm not even going to show it. In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total\n",
            "number of parameters devoted to all of these blocks adds up to about 116 billion. This is around two-thirds of the total parameters in the network, and when you add it to\n",
            "everything that we had before for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised. It's probably worth\n",
            "mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very\n",
            "trivial proportion of the total. As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts\n",
            "are actually stored in real-large language models. It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the\n",
            "activation of each neuron tells you how much a given vector aligns with some specific direction. It's also true that the columns of that second matrix tell you what will be added\n",
            "to the result if that neuron is active. Both of those are just mathematical facts. However, the evidence does suggest that individual neurons very rarely represent a single clean\n",
            "feature like Michael Jordan. And there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as\n",
            "superposition. This is a hypothesis that might help to explain both why the models are especially hard to interpret, and also why they scale surprisingly well. The basic idea is\n",
            "that if you have an end-dimensional space and you want to represent a bunch of different features using directions that are all perpendicular to one another in that space, you\n",
            "know, that way if you add a component in one direction it doesn't influence any of the other directions. Then the maximum number of vectors you can fit is only n, the number of\n",
            "dimensions. To a mathematician actually this is the definition of dimension, but where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.\n",
            "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart. If we were in\n",
            "two or three dimensions this makes no difference, that gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher\n",
            "dimensions the answer changes dramatically. I can give you a really quick and dirty illustration of this using some scrappy python that's going to create a list of 100 dimensional\n",
            "vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions. This plot right here shows\n",
            "the distribution of angles between pairs of these vectors, so because they started at random those angles could be anything from 0 to 180 degrees, but you'll notice that already\n",
            "even just for random vectors there's this heavy bias for things to be closer to 90 degrees. Then what I'm going to do is run a certain optimization process that iteratively nudges\n",
            "all of these vectors so that they try to become more perpendicular to one another. After repeating this many different times, here's what the distribution of angles looks like. We\n",
            "have to actually zoom in on it here, because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees. In general, a consequence\n",
            "of something known as the Johnson-Lindon-Strauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this, grows exponentially with the\n",
            "number of dimensions. This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions. It means that\n",
            "it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted. This might partially explain why model performance seems to scale so\n",
            "well with size. A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas. And this is relevant not just to that embedding space\n",
            "where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multi-layer perceptron that we just studied. That is to say, at the\n",
            "sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it\n",
            "could be probing at many, many more features of the vector being processed. But if it was doing that, what it means is that individual features aren't going to be visible as a\n",
            "single neuron lighting up. It would have to look like some specific combination of neurons instead, a superposition. For any of you curious to learn more, a key relevant search\n",
            "term here is Sparce Autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're superimposed on all\n",
            "these neurons. I'll link to a couple really great anthropic posts all about this. At this point, we haven't touched every detail of a transformer, but you and I have hit the most\n",
            "important points. The main thing that I want to cover in a next chapter is the training process. On the one hand, the short answer for how training works is that it's all back\n",
            "propagation, and we covered back propagation in a separate context with earlier chapters in the series. But there is more to discuss, like the specific cost function used for\n",
            "language models, the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of scaling laws. Quick note for the active followers among you, there are\n",
            "a number of non-machine learning-related videos that I'm excited to sync my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due\n",
            "time. So, I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with\n",
            "the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start\n",
            "with the first one. I'll start with the first one. I'll start with the first one. The choice between using ReLU (Rectified Linear Unit) or sigmoid functions in a neural network\n",
            "depends on the specific requirements of the network. Sigmoid functions were commonly used in earlier neural networks due to their ability to introduce non-linearity and limit the\n",
            "output to a range between 0 and 1. However, they have some disadvantages, such as the \"vanishing gradient\" problem, which can make training difficult. ReLU functions, on the other\n",
            "hand, have a simpler structure and do not suffer from the vanishing gradient problem. They also allow the network to learn more complex features and can lead to faster convergence\n",
            "during training. Therefore, ReLU functions are often preferred in modern neural networks. However, there are also other activation functions, such as tanh and leaky ReLU, that can\n",
            "be used depending on the specific application.\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "def format_string_response(response_string, line_width=80):\n",
        "    lines = response_string.split(\"\\n\")\n",
        "\n",
        "    # Extract context, question, and answer parts from the input string\n",
        "    context = []\n",
        "    question = None\n",
        "    answer = None\n",
        "    in_context = False\n",
        "    in_answer = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(\"Context:\"):\n",
        "            in_context = True\n",
        "            context.append(line.replace(\"Context:\", \"\").strip())\n",
        "        elif line.startswith(\"Question:\"):\n",
        "            question = line.replace(\"Question:\", \"\").strip()\n",
        "        elif line.startswith(\"Answer:\"):\n",
        "            in_answer = True\n",
        "            answer = line.replace(\"Answer:\", \"\").strip()\n",
        "        elif in_context:\n",
        "            context.append(line.strip())\n",
        "        elif in_answer:\n",
        "            answer += \" \" + line.strip()\n",
        "\n",
        "    # Combine context into a single string and wrap the text if it's too long\n",
        "    context_str = \" \".join(context).strip()\n",
        "    wrapped_context = textwrap.fill(context_str, width=line_width)\n",
        "\n",
        "    # Wrap the answer if it's too long\n",
        "    wrapped_answer = textwrap.fill(answer, width=line_width)\n",
        "    formatted_response = f\"Question: {question}\\n\\nContext: {wrapped_context}\\n\\nAnswer: {wrapped_answer}\"\n",
        "\n",
        "    return formatted_response\n",
        "\n",
        "formatted_response = format_string_response(response['answer'], line_width=180)\n",
        "print(formatted_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg1a1E2oGta9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ba3579-d15a-4af9-9412-a3ad9dd44d5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for question: 'When to use Relu or Sigmoid in a Neural Network?'\n",
            "\n",
            "Question: When to use Relu or Sigmoid in a Neural Network?\n",
            "\n",
            "Context: The hard assumption here is that you've watched Part 3, giving an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the\n",
            "relevant calculus. It's normal for this to be at least a little confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal\n",
            "is to show how people in machine learning commonly think about the chain rule from calculus in the context of networks, which has kind of a different feel from how most\n",
            "introductory calculus courses approach the subject. For those of you uncomfortable with the relevant calculus, I do have a whole series on the topic. Let's just start off with an\n",
            "extremely simple network, one where each layer has a single neuron in it. So this particular network is determined by three weights and three biases, and our goal is to understand\n",
            "how sensitive the cost function is to these variables. That way we know which adjustments to those terms is going to cause the most efficient decrease to the cost function. And\n",
            "we're just going to focus on the connection between the last two neurons. Let's label the activation of that last neuron with a superscript L indicating which layer it's in. So the\n",
            "activation of the previous neuron is a L minus 1. These are not exponents, they're just a way of indexing what we're talking about since I want to save subscripts for different\n",
            "indices later on. Now let's say that the value we want this last activation to be for a given training example is y, for example, y might be 0 or 1. So the cost of this simple\n",
            "network for a single training example is AL minus y squared. We'll denote the cost of that one training example as C0. As a reminder, this last activation is determined by a\n",
            "weight, which I'm going to call WL, times the previous neurons activation plus some bias, which I'll call BL. And then you pump that through some special nonlinear function like\n",
            "the sigmoid or a ray loop. It's actually going to make things easier for us if we give a special name to this weighted sum, like Z, with the same superscript as the relevant\n",
            "activations. So this is a lot of terms and a way that you might conceptualize it is that the weight, the previous action and the bias altogether are used to compute Z, which in\n",
            "turn lets us compute A, which finally, along with a constant Y, lets us compute the cost. And of course, AL minus 1 is influenced by its own weight and bias and such. But we're not\n",
            "going to focus on that right now. Now all of these are just numbers, right? And it can be nice to think of each one as having its own little number line. Our first goal is to\n",
            "understand how sensitive the cost function is to small changes in our weight, WL. Never phrase differently. What is the derivative of C with respect to WL? When you see this Dell W\n",
            "term, think of it as meaning some tiny nudge to W, like a change by 0.01. And think of this Dell C term as meaning whatever the resulting nudge to the cost is. What we want is\n",
            "their ratio. Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn causes some nudge to AL, which directly influences the cost. So we break things up by first\n",
            "looking at the ratio of a tiny change to ZL to this tiny change W, that is the derivative of ZL with respect to WL. Likewise, you then consider the ratio of the change to AL to the\n",
            "tiny change in ZL that caused it, as well as the ratio between the final nudge to C and this intermediate nudge to AL. This right here is the chain rule, where multiplying together\n",
            "these three ratios gives us the sensitivity of C to small changes in WL. So on screen right now, there's kind of a lot of symbols, and take a moment to just make sure it's clear\n",
            "what they all are. Because now we're going to compute the relevant derivatives. The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this means that\n",
            "its size is proportional to the difference between the networks output and the thing that we want it to be. So if that output was very different, even slight changes stand to have\n",
            "a big impact on the final cost function. The derivative of AL with respect to ZL is just the derivative of our sigmoid function, or whatever non-linearity you choose to use. And\n",
            "the derivative of ZL with respect to WL, in this case comes out just to be AL minus 1. Now I don't know about you, but I think it's easy to get stuck head down in the formulas\n",
            "without taking a moment to sit back and remind yourself of what they all actually mean. In the case of this last derivative, the amount that that small nudge to the weight\n",
            "influenced the last layer depends on how strong the previous neuron is. Remember, this is where that neurons that fire together wire together idea comes in. And all of this is the\n",
            "derivative with respect to WL only of the cost for a specific single training example. Since the full cost function involves averaging together all those costs across many\n",
            "different training examples, its derivative requires averaging this expression that we found over all training examples. And of course that is just one component of the gradient\n",
            "vector, which itself is built up from the partial derivatives of the cost function with respect to all those weights and biases. But even though that's just one of the many partial\n",
            "derivatives we need, it's more than 50% of the work. The sensitivity to the bias, for example, is almost identical. We just need to change out this del Z del W term for a del Z del\n",
            "B. And if you look at the relevant formula, that derivative comes out to be 1. Also, and this is where the idea of propagating backwards comes in, you can see how sensitive this\n",
            "cost function is to the activation of the previous layer. Namely, this initial derivative in the chain rule expression, the sensitivity of Z to the previous activation, comes out\n",
            "to be the weight WL. And again, even though we're not going to be able to directly influence that previous layer activation, it's helpful to keep track of. Because now, we can just\n",
            "keep iterating this same chain rule idea backwards to see how sensitive the cost function is to previous weights and previous biases. And you might think that this is an overly\n",
            "simple example, since all layers just have one neuron, and that things are going to get exponentially more complicated for a real network. But honestly, not that much changes when\n",
            "we give the layers multiple neurons. Really it's just a few more indices to keep track of. Rather than the activation of a given layer simply being AL, it's also going to have a\n",
            "subscript, indicating which neuron of that layer it is. Let's go ahead and use the letter K to index the layer L minus 1, and J to index the layer L. For the cost, again, we look\n",
            "at what the desired output is, but this time we add up the squares of the differences between these last layer activations and the desired output. That is, you take a sum over ALJ\n",
            "minus YJ squared. Since there's a lot more weights, each one has to have a couple more indices to keep track of where it is. So let's call the weight of the edge connecting this\n",
            "Kth neuron to the Jth neuron, WLJK. Those indices might feel a little backwards at first, but it lines up with how you'd index the weight matrix that I talked about in the Part 1\n",
            "video. Just as before, it's still nice to give a name to the relevant weighted sum, like Z, so that the activation of the last layer is just your special function, like the\n",
            "sigmoid, applied to Z. You can kind of see what I mean, right, where all of these are essentially the same equations that we had before in the one neuron per layer case. It's just\n",
            "that it looks a little more complicated. And indeed, the chain ruled the rivetive expression, describing how sensitive the cost is to a specific weight, looks essentially the same.\n",
            "I'll leave it to you to pause and think about each of those terms if you want. What does change here, though, is the derivative of the cost with respect to one of the activations\n",
            "in the layer L minus 1. In this case, the difference is that the neuron influences the cost function through multiple different paths. It is on the one hand, it influences AL0,\n",
            "which plays a role in the cost function, but it also has an influence on AL1, which also plays a role in the cost function, and you have to add those up. And that, well, that's\n",
            "pretty much it. Once you know how sensitive the cost function is to the activations in this second to last layer, you can just repeat the process for all the weights and biases\n",
            "feeding into that layer. So pat yourself on the back. If all of this makes sense, you have now looked deep into the heart of back propagation, the workhorse behind how neural\n",
            "networks learn. These chain rule expressions give you the derivatives that determine each component in the gradient that helps minimize the cost of the network by repeatedly\n",
            "stepping downhill. If you sit back and think about all that, this is a lot of layers of complexity to wrap your mind around. So don't worry if it takes time for your mind to digest\n",
            "it all. Here we tackle back propagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walk-\n",
            "through for what the algorithm is actually doing without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the\n",
            "calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds\n",
            "forward information. Here, we're doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons, and\n",
            "I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as a tensor. I'm\n",
            "also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a\n",
            "certain cost function. As a quick reminder for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted\n",
            "it to give, and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results,\n",
            "this gives you the total cost of the network. As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of\n",
            "this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation,\n",
            "the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind\n",
            "right now, is that because thinking of the gradient vector as a direction in 13,000 dimensions is to put it lightly beyond the scope of our imaginations, there's another way you\n",
            "can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the\n",
            "process I'm about to describe when you compute the negative gradient and the component associated with the weight on this edge here comes out to be 3.2, while the component\n",
            "associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if\n",
            "you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight\n",
            "would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you\n",
            "unwrap what each part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive. It's just that there's a lot of little adjustments\n",
            "getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through those effects that each training\n",
            "example is having on the weights and biases. Because the cost function involves averaging a certain cost per example, over all the tens of thousands of training examples, the way\n",
            "that we adjust the weights and biases for a single gradient descent step also depends on every single example, or rather, in principle it should, but for computational efficiency,\n",
            "we're going to do a little trick later to keep you from needing to hit every single example for every single step. In other case, right now, all we're going to do is focus our\n",
            "attention on one single example, this image of a two. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where\n",
            "the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now, we can't directly change\n",
            "those activations. We only have influence on the weights and biases. But it is helpful to keep track of which adjustments we wish should take place to that output layer. And since\n",
            "we want it to classify the image as a two, we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be\n",
            "proportional to how far away each current value is from its target value. For example, the increase to that number two neurons activation is in a sense more important than the\n",
            "decrease to the number eight neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we\n",
            "wish to increase. Remember, that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias, which is all then plugged into\n",
            "something like the sigmoid squishification function, or a ray-lew. So there are three different avenues that can team up together to help increase that activation. You can increase\n",
            "the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing just on how the weights should be adjusted? Notice how the weights\n",
            "actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect, since those weights are multiplied by\n",
            "larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of\n",
            "connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each\n",
            "component should get nudged up or down, we care about which ones give you the most bang for your butt. This, by the way, is at least somewhat reminiscent of a theory in\n",
            "neuroscience for how biological networks of neurons learn, heavy in theory. Often summed up in the phrase, neurons that fire together, wire together. Here, the biggest increases to\n",
            "weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that\n",
            "are firing while seeing a two get more strongly linked to those firing when thinking about a two. To be clear, I really am not in a position to make statements one way or another\n",
            "about whether artificial networks of neurons behave anything like biological brains, and this fires together, wire together idea comes with a couple meaningful asterisks. But taken\n",
            "as a very loose analogy, I do find it interesting to note. Anyway, the third way that we can help increase this neuron's activation is by changing all the activations in the\n",
            "previous layer. Namely, if everything connected to that digit two neuron with a positive weight, got brighter, and if everything connected with a negative weight got dimmer, then\n",
            "that digit two neuron would become more active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the\n",
            "size of the corresponding weights. Now, of course, we cannot directly influence those activations. We only have control over the weights and biases. But just as with the last\n",
            "layer, it's helpful to just keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit two output neuron wants.\n",
            "Remember, we also want all of the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to\n",
            "that second to last layer. So the desire of this digit two neuron is added together with the desires of all the other output neurons for what should happen to this second to last\n",
            "layer. Again, in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating\n",
            "backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those,\n",
            "you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards\n",
            "through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only\n",
            "listen to what that two wanted, the network would ultimately be incentivized just to classify all images as a two. What you do is you go through this same back property for every\n",
            "other training example, recording how each of them would like to change the weights and the biases. And you average together those desired changes. This collection here of the\n",
            "average to nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I\n",
            "say loosely speaking only because I have yet to get quantitatively precise about those nudges. But if you understood every change that I just referenced, why some are\n",
            "proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what back propagation is actually doing. By the way, in practice, it\n",
            "takes computers an extremely long time to add up the influence of every single training example, every single gradient descent step. So here's what's commonly done instead. You\n",
            "randomly shuffle your training data and then divide it into a whole bunch of mini-batches. Let's say each one having 100 training examples. Then you compute a step according to the\n",
            "mini-batch. It's not going to be the actual gradient to the cost function, which depends on all of the training data, not this tiny subset. So it's not the most efficient step\n",
            "downhill. But each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speed up. If you would applaud the\n",
            "trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps, rather than a\n",
            "carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as\n",
            "stochastic gradient descent. There's kind of a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single\n",
            "training example would like to nudge the weights and biases. Not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes\n",
            "cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens and thousands of training examples and averaging the desired\n",
            "changes that you get. But that's computationally slow, so instead you randomly subdivide the data into these mini batches and compute each step with respect to a mini batch.\n",
            "Repeatedly going through all of the mini batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network is going\n",
            "to end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing Backprop actually corresponds with\n",
            "something that you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it\n",
            "gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the\n",
            "underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this\n",
            "algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten\n",
            "digits such a nice example is that there exists the M-NIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in\n",
            "machine learning will be familiar with is just getting the labeled training data that you actually need, whether that's having people label tens of thousands of images or whatever\n",
            "other data type you might be dealing with. Last video, I laid out the structure of a neural network. I'll give a quick recap here just so that it's fresh in our minds and then I\n",
            "have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine\n",
            "learning works as well. Then after that, we're going to dig in a little more to how this particular network performs and what those hidden layers of neurons end up actually looking\n",
            "for. As a reminder, our goal here is the classic example of handwritten digit recognition, the Hello World of Neural Networks. These digits are rendered on a 28x28 pixel grid, each\n",
            "pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons in the input layer of the network. And then the activation for each neuron\n",
            "in the following layers is based on a weighted sum of all the activations in the previous layer plus some special number called a bias. Then you compose that sum with some other\n",
            "function, like the sigmoid squishification or a ray-loo, the way that I walked through last video. In total, given the somewhat arbitrary choice of two hidden layers here with 16\n",
            "neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does. And what we mean when\n",
            "we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit. And remember, the motivation that we had\n",
            "in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last\n",
            "one could just piece together those patterns to recognize digits. So here we learn how the network learns. What we want is an algorithm where you can show this network a whole\n",
            "bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those\n",
            "13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered structure will mean that what it learns generalizes to images beyond that\n",
            "training data. And the way we test that is that after you train the network, you show it more labeled data, that it's never seen before, and you see how accurately it classifies\n",
            "those new images. Fortunately for us, and what makes this such a common example to start with, is that the good people behind the M-NIST database have put together a collection of\n",
            "tens of thousands of handwritten digit images, each one labeled with the numbers that they're supposed to be. And it's provocative as it is to describe a machine as learning. Once\n",
            "you actually see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like, a calculus exercise. I mean, basically, it comes down to finding the minimum\n",
            "of a certain function. Remember, conceptually, we're thinking of each neuron as being connected to all of the neurons in the previous layer, and the weights in the weighted sum\n",
            "defining its activation are kind of like the strengths of those connections. The bias is some indication of whether that neuron tends to be active or inactive. And to start things\n",
            "off, we're just going to initialize all of those weights and biases totally randomly. Needless to say, this network is going to perform pretty horribly on a given training example,\n",
            "since it's just doing something random. For example, you feed in this image of a three, and the output layer just looks like a mess. So what you do is you define a cost function, a\n",
            "way of telling the computer, no, bad computer, that output should have activations, which are zero for most neurons, but one for this neuron. What you gave me is utter trash. To\n",
            "say that a little more mathematically, what you do is add up the squares of the differences between each of those trash output activations and the value that you want them to have.\n",
            "And this is what we'll call the cost of a single training example. Notice, this sum is small when the network confidently classifies the image correctly, but it's large when the\n",
            "network seems like it doesn't really know what it's doing. So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal.\n",
            "This average cost is our measure for how lousy the network is and how bad the computer should feel. And that's a complicated thing. Remember how the network itself was basically a\n",
            "function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output. And in a sense, it's parameterized by all these weights and biases.\n",
            "Well the cost function is a layer of complexity on top of that. It takes as its input, those 13,000 or so weights and biases, and it spits out a single number describing how bad\n",
            "those weights and biases are. And the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data. That's a lot to think about. But\n",
            "just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to change those weights and biases so that it gets better. To make it easier,\n",
            "rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output. How do you find an\n",
            "input that minimizes the value of this function? Circular students will know that you can sometimes figure out that minimum explicitly. But that's not always feasible for really\n",
            "complicated functions. Certainly not in the 13,000 input version of this situation for our crazy complicated neural network cost function. A more flexible tactic is to start at any\n",
            "all input and figure out which direction you should step to make that output lower. Specifically, if you can figure out the slope of the function where you are, then shift to the\n",
            "left if that slope is positive and shift the input to the right if that slope is negative. If you do this repeatedly, at each point checking the new slope and taking the\n",
            "appropriate step, you're going to approach some local minimum of the function. And the image you might have in mind here is a ball rolling down a hill. And notice, even for this\n",
            "really simplified single input function, there are many possible valleys that you might land in, depending on which random input you start at. There's no guarantee that the local\n",
            "minimum you land in is going to be the smallest possible value of the cost function. That's going to carry over to our neural network case as well. And I also want you to notice\n",
            "how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum, your steps get smaller and smaller, and that kind of helps you\n",
            "from overshooting. Bumping up the complexity a bit, imagine instead a function with two inputs and one output. You might think of the input space as the x, y plane and the cost\n",
            "function as being graft as a surface above it. Now instead of asking about the slope of the function, you have to ask which direction should you step in this input space so as to\n",
            "decrease the output of the function most quickly. In other words, what's the downhill direction? And again, it's helpful to think of a ball rolling down that hill. Most of you\n",
            "familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent, basically which direction should you step to increase the\n",
            "function most quickly. Naturally enough, taking the negative of that gradient gives you the direction to step that decreases the function most quickly. And even more than that, the\n",
            "length of this gradient vector is actually an indication for just how steep that steepest slope is. Now if you're unfamiliar with multivariable calculus and you want to learn more,\n",
            "check out some of the work that I did for Khan Academy on the topic. Honestly though, all that matters for you and me right now is that in principle there exists a way to compute\n",
            "this vector. This vector that tells you what the downhill direction is and how steep it is. You'll be okay if that's all you know and you're not rock solid on the details. Because\n",
            "if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then take a small step downhill and just repeat that over and over. It's the\n",
            "same basic idea for a function that has 13,000 inputs instead of two inputs. Imagine organizing all 13,000 weights and biases of our network into a giant column vector. The\n",
            "negative gradient of the cost function is just a vector. It's some direction inside this insanely huge input space that tells you which nudges to all of those numbers is going to\n",
            "cause the most rapid decrease to the cost function. And of course, with our specially designed cost function, changing the weights and biases to decrease it means making the output\n",
            "of the network on each piece of training data look less like a random array of 10 values and more like an actual decision that we want it to make. It's important to remember, this\n",
            "cost function involves an average over all of the training data. So if you minimize it, it means it's a better performance on all of those samples. The algorithm for computing this\n",
            "gradient efficiently, which is effectively the heart of how a neural network learns, is called back propagation. And it's what I'm going to be talking about next video. There I\n",
            "really want to take the time to walk through what exactly happens to each weight and each bias for a given piece of training data. Trying to give an intuitive feel for what's\n",
            "happening beyond the pile of relevant calculus and formulas. Right here, right now, the main thing I want you to know, independent of implementation details, is that what we mean\n",
            "when we talk about a network learning is that it's just minimizing a cost function. And notice, one consequence of that is that it's important for this cost function to have a nice\n",
            "smooth output so that we can find a local minimum by taking little steps down hill. This is why, by the way, artificial neurons have continuously ranging activations, rather than\n",
            "simply being active or inactive in a binary way, the way that biological neurons are. This process of repeatedly nudging an input of a function by some multiple of the negative\n",
            "gradient is called gradient descent. It's a way to converge toward some local minimum of a cost function, basically a valley in this graph. I'm still showing the picture of a\n",
            "function with two inputs, of course, because nudges in a 13,000-dimensional input space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to\n",
            "think about this. Each component of the negative gradient tells us two things. The sign, of course, tells us whether the corresponding component of the input vector should be\n",
            "nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you which changes matter more. You see, in our network, an adjustment to one of\n",
            "the weights might have a much greater impact on the cost function than the adjustment to some other weight. Some of these connections just matter more for our training data. So a\n",
            "way that you can think about this gradient vector of our mind-warpingly massive cost function is that it encodes the relative importance of each weight and bias, that is, which of\n",
            "these changes is going to carry the most bang for your buck. This really is just another way of thinking about direction. To take a simpler example, if you have some function with\n",
            "two variables as an input, and you compute that it's gradient at some particular point, comes out as 3-1. Then on the one hand, you can interpret that as saying that when you're\n",
            "standing at that input, moving along this direction increases the function most quickly. But when you graph the function above the plane of input points, that vector is what's\n",
            "giving you the straight uphill direction. But another way to read that is to say that changes to this first variable have three times the importance as changes to the second\n",
            "variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for your buck. Alright, let's zoom out and sum up where we are so\n",
            "far. The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted sums. The cost function is a layer of complexity on top of that.\n",
            "It takes the 13,000 weights and biases as inputs, and spits out a single measure of laziness based on the training examples. And the gradient of the cost function is one more layer\n",
            "of complexity still. It tells us what nudges to all of these weights and biases cause the fastest change to the value of the cost function, which you might interpret as saying\n",
            "which changes to which weights matter the most. So, when you initialize the network with random weights and biases and adjust them many times based on this gradient descent\n",
            "process, how well does it actually perform on images that it's never seen before? Well the one that I've described here, with the two hidden layers of 16 neurons each, goes in\n",
            "mostly for aesthetic reasons. Well, it's not bad. It classifies about 96% of the new images that it sees correctly. And honestly, if you look at some of the examples that it messes\n",
            "up on, you kind of feel compelled to cut it a little slack. Now, if you play around with the hidden layer structure and make a couple tweaks, you can get this up to 98%. And that's\n",
            "pretty good. It's not the best. You can certainly get better performance by getting more sophisticated than this plain vanilla network. But given how daunting the initial task is,\n",
            "I just think there's something incredible about any network doing this well on images that it's never seen before, given that we never specifically told it what patterns to look\n",
            "for. Originally, the way that I motivated this structure was by describing a hope that we might have, that the second layer might pick up on little edges, that the third layer\n",
            "would piece together those edges to recognize loops in longer lines, and that those might be piece together to recognize digits. So is this what our network is actually doing?\n",
            "Well, for this one, at least, not at all. Remember how last video we looked at how the weights of the connections from all of the neurons in the first layer to a given neuron in\n",
            "the second layer can be visualized as a given pixel pattern that that second layer neuron is picking up on? Well, when we actually do that, for the weights associated with these\n",
            "transitions from the first layer to the next. Instead of picking up on isolated little edges here and there, they look almost random, just with some very loose patterns in the\n",
            "middle there. It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our network found itself a happy little local minimum that,\n",
            "despite successfully classifying most images, doesn't exactly pick up on the patterns that we might have hoped for. And to really drive this point home, watch what happens when you\n",
            "input a random image. If the system was smart, you might expect it to either feel uncertain, maybe, not really activating any of those 10 output neurons or activating them all\n",
            "evenly. But instead, it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as it does that an actual image of a 5 is a 5. Fraze\n",
            "differently? Even if this network can recognize digits pretty well, it has no idea how to draw them. A lot of this is because it's such a tightly constrained training setup. I\n",
            "mean, put yourself in the network's shoes here. From its point of view, the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid. And its\n",
            "cost function just never gave it any incentive to be anything but utterly confident in its decisions. So with this is the image of what those second layer neurons are really doing,\n",
            "you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. That's just not at all what it ends up doing. Well this is not meant to\n",
            "be our end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s and 90s. And you do need to understand it before you can understand\n",
            "more detailed modern variants, and it clearly is capable of solving some interesting problems. But the more you dig in to what those hidden layers are really doing, the less\n",
            "intelligent it seems. Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage actively with the material here somehow. One\n",
            "pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what changes you might make to this system and how it perceives images if you\n",
            "wanted it to better pick up on things like edges and patterns. But better than that, to actually engage with the material, I highly recommend the book by Michael Neilsen on deep\n",
            "learning and neural networks. In it, you can find the code and the data to download and play with for this exact example, and the book will walk you through step by step what that\n",
            "code is doing. What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining me in making a donation towards Neilsen's\n",
            "efforts. I've also linked a couple other resources that I like a lot in the description, including the phenomenal and beautiful blog posts by Chris Ola and the articles in Distill.\n",
            "To close things off here for the last few minutes, I want to jump back into a snippet of the interview that I had with Lisha Lee. You might remember her from the last video, she\n",
            "did her PhD work in deep learning. And in this little snippet, she talks about two recent papers that really dig in to how some of the more modern image recognition networks are\n",
            "actually learning. Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks that's really good at image recognition,\n",
            "and instead of training it on a properly labeled data set, it shuffled all of the labels around before training. Obviously the testing accuracy here was going to be no better than\n",
            "random, since everything's just randomly labeled. But it was still able to achieve the same training accuracy as you would on a properly labeled data set. Basically, the millions\n",
            "of weights for this particular network were enough for it to just memorize the random data, which kind of raises the question for whether minimizing this cost function actually\n",
            "corresponds to any sort of structure in the image, or is it just memorization? It reminds the entire data set of what the correct classification is. And so a couple of half a year\n",
            "later at ICML this year, there was not exactly rebuttal paper, paper that addressed some aspects of like, hey, actually these networks are doing something a little bit smarter than\n",
            "that. If you look at that accuracy curve, if you were just training on a random data set, that curve sort of went down very slowly in almost a linear fashion. So you're really\n",
            "struggling to find that local minimum of possible, the right weights that would get you that accuracy. Whereas if you're actually training on a structure data set, one that has the\n",
            "right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that accuracy level. And so in some sense, it was easier to find\n",
            "that local maxima. And so it was also interesting about that, is it brings into light another paper from actually a couple of years ago, which has a lot more simplifications about\n",
            "the network layers. But one of the results was saying how if you look at the optimization landscape, the local minima that these networks tend to learn are actually of equal\n",
            "quality. So in some sense, if your data set a structure, you should be able to find that much more easily. My thanks, as always, to those of you supporting on Patreon. I've said\n",
            "before just what a game changer in Patreon is, but these videos really would not be possible without you. I also want to give a special thanks to the VC firm Amplify Partners in\n",
            "their support of these initial videos in the series. This is a 3. It's slobily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble\n",
            "recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean this, this and this are also recognizable as\n",
            "3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3\n",
            "are very different from the ones firing when you see this 3. But something in that crazy smart visual cortex of yours resolves these as representing the same idea, while at the\n",
            "same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this, and\n",
            "outputs a single number between 0 and 10, telling you what it thinks the digit is, while the task goes from comically trivial to dauntingly difficult. Unless you've been living\n",
            "under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is\n",
            "show you what a neural network actually is, assuming no background, and to help visualize what it's doing. Not as a buzzword, but as a piece of math. My hope is just that you come\n",
            "away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote unquote learning. This video is\n",
            "just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we're going to do is put together a neural network that can\n",
            "learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quote here, because at the end of the\n",
            "two videos, I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There\n",
            "are many, many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos, you and I are\n",
            "just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and\n",
            "trust me, it still has plenty of complexity for us to wrap our minds around. But even in this simplest form, it can learn to recognize handwritten digits, which is a pretty cool\n",
            "thing for a computer to be able to do. And at the same time, you'll see how it does fall short of a couple hopes that we might have for it. As the name suggests, neural networks\n",
            "are inspired by the brain, but let's break that down. What are the neurons and in what sense are they linked together? Right now, when I say neuron, all I want you to think about\n",
            "is a thing that holds a number, specifically a number between zero and one. It's really not more than that. For example, the network starts with a bunch of neurons corresponding to\n",
            "each of the 28 times 28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the gray scale value of the corresponding pixel,\n",
            "ranging from zero for black pixels up to one for white pixels. This number inside the neuron is called its activation. And the image you might have in mind here is that each neuron\n",
            "is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each\n",
            "representing one of the digits. The activation in these neurons, again, some number that's between zero and one, represents how much the system thinks that a given image\n",
            "corresponds with a given digit. There's also a couple layers in between called the hidden layers, which for the time being, should just be a giant question mark. For how on earth\n",
            "this process of recognizing digits is going to be handled. In this network, I chose two hidden layers, each one with 16 neurons. And admittedly, that's kind of an arbitrary choice.\n",
            "To be honest, I chose two layers based on how I want to motivate the structure in just a moment. And 16, well, that was just a nice number to fit on the screen. In practice, there\n",
            "is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer determine the activations of the next layer. And of course,\n",
            "the heart of the network, as an information processing mechanism, comes down to exactly how those activations from one layer bring about activations in the next layer. It's meant\n",
            "to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now, the network I'm showing here has already been\n",
            "trained to recognize digits. And let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer, according to the brightness\n",
            "of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer, which causes some pattern in the one after it, which finally gives some\n",
            "pattern in the output layer. And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. And before jumping into the\n",
            "math for how one layer influences the next or how training works, let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently.\n",
            "What are we expecting here? What is the best hope for what those middle layers might be doing? Well, when you or I recognize digits, we piece together various components. A 9 has a\n",
            "loop up top and a line on the right. And 8 also has a loop up top, but it's paired with another loop down low. A 4 basically breaks down into 3 specific lines and things like that.\n",
            "Now in a perfect world, we might hope that each neuron in the second to last layer corresponds with one of these subcomponents. That anytime you feed in an image with, say, a loop\n",
            "up top, like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1. And I don't mean this specific loop of pixels. The hope would be that any\n",
            "generally loopy pattern towards the top sets off this neuron. That way, going from the third layer to the last one, just requires learning which combination of subcomponents\n",
            "corresponds to which digits. Of course, that just kicks the problem down the road because how would you recognize these sub components or even learn what the right subcomponents\n",
            "should be? And I still haven't even talked about how one layer influences the next, but run with me on this one for a moment. Recognizing a loop can also break down into\n",
            "subproblems. One reasonable way to do this would be to first recognize the various little edges that make it up. Similarly, a long line, like the kind you might see in the digits 1\n",
            "or 4 or 7, well that's really just a long edge, or maybe you think of it as a certain pattern of several smaller edges. So maybe, our hope is that each neuron in the second layer\n",
            "of the network corresponds with the various relevant little edges. Maybe, when an image like this one comes in, it lights up all of the neurons associated with around 8 to 10\n",
            "specific little edges, which in turn lights up the neurons associated with the upper loop and a long vertical line, and those light up the neuron associated with the 9. Whether or\n",
            "not this is what our final network actually does is another question, one that I'll come back to once we see how to train the network. But this is a hope that we might have, a sort\n",
            "of goal with the layered structure like this. Moreover, you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition\n",
            "tasks. And even beyond image recognition, there are all sorts of intelligent things you might want to do that break down into layers of abstraction. Parsing speech, for example,\n",
            "involves taking raw audio and picking out distinct sounds, which combine to make certain syllables, which combine to form words, which combine to make up phrases and more abstract\n",
            "thoughts, etc. But getting back to how any of this actually works, picture yourself right now designing how exactly the activations in one layer might determine the activations in\n",
            "the next. The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits. And to zoom in on one very specific\n",
            "example, let's say the hope is for one particular neuron in the second layer to pick up on whether or not the image has an edge in this region here. The question at hand is what\n",
            "parameters should the network have? What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern,\n",
            "or the pattern that several edges can make a loop and other such things? Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons\n",
            "from the first layer. These weights are just numbers. Then take all of those activations from the first layer and compute their weighted sum according to these weights. I find it\n",
            "helpful to think of these weights as being organized into a little grid of their own, and I'm going to use green pixels to indicate positive weights and red pixels to indicate\n",
            "negative weights, where the brightness of that pixel is some loose depiction of the weights value. Now if we made the weights associated with almost all of the pixels zero, except\n",
            "for some positive weights in this region that we care about, then taking the weighted sum of all the pixel values really just amounts to adding up the values of the pixel just in\n",
            "the region that we care about. And if you really wanted to pick up on whether there's an edge here, what you might do is have some negative weights associated with the surrounding\n",
            "pixels. Then the sum is largest when those middle pixels are bright, but the surrounding pixels are darker. When you compute a weighted sum like this, you might come out with any\n",
            "number, but for this network, what we want is for activations to be some value between zero and one. So a common thing to do is to pump this weighted sum into some function that\n",
            "squishes the real number line into the range between zero and one. And a common function that does this is called the sigmoid function, also known as a logistic curve, basically\n",
            "very negative inputs end up close to zero, very positive inputs end up close to one, and it just steadily increases around the input zero. So the activation of the neuron here is\n",
            "basically a measure of how positive the relevant weighted sum is. But maybe it's not that you want the neuron to light up when the weighted sum is bigger than zero. Maybe you only\n",
            "want it to be active when the sum is bigger than say 10. That is, you want some bias for it to be inactive. What we'll do then is just add in some other number, like negative 10,\n",
            "to this weighted sum, before plugging it through the sigmoid squishing function. That additional number is called the bias. So the weights tell you what pixel pattern this neuron\n",
            "in the second layer is picking up on, and the bias tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active. And that is just one\n",
            "neuron. Every other neuron in this layer is going to be connected to all 784 pixel neurons from the first layer, and each one of those 784 connections has its own weight associated\n",
            "with it. Also, each one has some bias, some other number that you add onto the weighted sum before squishing it with the sigmoid. And that's a lot to think about. With this hidden\n",
            "layer of 16 neurons, that's a total of 784 x 16 weights, along with 16 biases. And all of that is just the connections from the first layer to the second. The connections between\n",
            "the other layers also have a bunch of weights and biases associated with them. All said and done, this network has almost exactly 13,000 total weights and biases. 13,000 knobs and\n",
            "dials that can be tweaked and turned to make this network behave in different ways. So when we talk about learning, what that's referring to is getting the computer to find a valid\n",
            "setting for all of these many, many numbers so that it'll actually solve the problem at hand. One thought experiment that is at once fun and kind of horrifying is to imagine\n",
            "sitting down and setting all of these weights and biases by hand, purposefully tweaking the numbers so that the second layer picks up on edges, the third layer picks up on\n",
            "patterns, etc. I personally find this satisfying rather than just treating the network as a total black box because when the network doesn't perform the way you anticipate, if\n",
            "you've built up a little bit of a relationship with what those weights and biases actually mean, you have a starting place for experimenting with how to change the structure to\n",
            "improve. Or when the network does work but not for the reasons you might expect, digging into what the weights and biases are doing is a good way to challenge your assumptions and\n",
            "really expose the full space of possible solutions. By the way, the actual function here is a little cumbersome to write down, don't you think? So let me show you a more\n",
            "notationally compact way that these connections are represented. This is how you'd see it if you choose to read out more about neural networks. Organize all of the activations from\n",
            "one layer into a column as a vector. Then organize all of the weights as a matrix, where each row of that matrix corresponds to the connections between one layer and a particular\n",
            "neuron in the next layer. What that means is that taking the weighted sum of the activations in the first layer, according to these weights, corresponds to one of the terms in the\n",
            "matrix vector product of everything we have on the left here. By the way, so much of machine learning just comes down to having a good grasp of linear algebra, so for any of you\n",
            "who want a nice visual understanding for matrices and what matrix vector multiplication means, take a look at the series I did on linear algebra, especially chapter 3. Back to our\n",
            "expression, instead of talking about adding the bias to each one of these values independently, we represent it by organizing all those biases into a vector and adding the entire\n",
            "vector to the previous matrix vector product. Then as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed to represent is that you're going to apply\n",
            "the sigmoid function to each specific component of the resulting vector inside. So once you write down this weight matrix and these vectors as their own symbols, you can\n",
            "communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression. And this makes the relevant code both a lot simpler and\n",
            "a lot faster, since many libraries optimize the heck out of matrix multiplication. Remember how earlier I said these neurons are simply things that hold numbers? Well, of course\n",
            "the specific numbers that they hold depends on the image you feed in. So it's actually more accurate to think of each neuron as a function, one that takes in the outputs of all the\n",
            "neurons in the previous layer, and spits out a number between 0 and 1. Really, the entire network is just a function, one that takes in 784 numbers as an input, and spits out 10\n",
            "numbers as an output. It's an absurdly complicated function, one that involves 13,000 parameters in the forms of these weights and biases that pick up on certain patterns, and\n",
            "which involves iterating many matrix vector products and the sigmoid squishification function. But it's just a function nonetheless. And in a way, it's kind of reassuring that it\n",
            "looks complicated. I mean, if there were any simpler, what hope would we have that it could take on the challenge of recognizing digits? And how does it take on that challenge? How\n",
            "does this network learn the appropriate weights and biases just by looking at data? Oh, that's what I'll show in the next video, and I'll also dig a little more into what this\n",
            "particular network we're seeing is really doing. Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out, but\n",
            "realistically, most of you don't actually receive notifications from YouTube, do you? Maybe more honestly, I should say subscribe so that the neural networks that underlie\n",
            "YouTube's recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you. Anyway, stay posted for more. Thank you very much to\n",
            "everyone supporting these videos on Patreon. I've been a little slow to progress in the probability series this summer, but I'm jumping back into it after this project, so patrons,\n",
            "you can look out for updates there. To close things off here, I have with me Lisa Lee, who did her PhD work on the theoretical side of deep learning, and who currently works at a\n",
            "venture capital firm called Amplify Partners, who kindly provided some of the funding for this video. So, Lisa, one thing I think we should quickly bring up is this sigmoid\n",
            "function. As I understand it, early networks use this to squish the relevant weighted sum into that interval between 0 and 1, you know, kind of motivated by this biological analogy\n",
            "of neurons either being inactive or active. Exactly. But relatively few modern networks actually use sigmoid anymore. It's kind of old school, right? Yeah, or rather, relu seems to\n",
            "be much easier to train. And relu stands for rectified linear unit? Yes, it's this kind of function where you're just taking a max of 0 and a, where a is given by what you were\n",
            "explaining in the video, and what this was sort of motivated from, I think, was a partially biological analogy with how neurons would either be activated or not. And so if it\n",
            "passes a certain threshold, it would be the identity function, but if it did not, then it would just not be activated, so be zero. So it's kind of a simplification. Using sigmoids\n",
            "didn't help training, or it was very difficult to train at some point, and people just tried relu, and it happened to work very well for these incredibly deep neural networks. All\n",
            "right. Thank you, Lisa. If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you have it predict what comes next, and it correctly predicts\n",
            "basketball. This would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge about a specific person and his specific sport. And I think in\n",
            "general anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of facts. So a reasonable question you could ask is, how exactly\n",
            "does that work, and where do those facts live? Last December a few researchers from Google Deep Mind posted about work on this question, and they were using this specific example\n",
            "of matching athletes to their sports. And although a full mechanistic understanding of how facts are stored remains unsolved, they had some interesting partial results, including\n",
            "the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known fancifully as the multi-layer perceptrons, or MLPs for short. In\n",
            "the last couple of chapters, you and I have been digging into the details behind transformers, the architecture underlying large language models, and also underlying a lot of other\n",
            "modern AI. In the most recent chapter we were focusing on a piece called Attention, and the next step for you and me is to dig into the details of what happens inside these multi-\n",
            "layer perceptrons, which make up the other big portion of the network. The computation here is actually relatively simple, especially when you compare it to attention. It boils\n",
            "down essentially to a pair of matrix multiplications with a simple something in between. However, interpreting what these computations are doing is exceedingly challenging. Our\n",
            "main goal here is to step through the computations and make them memorable, but I'd like to do it in the context of showing a specific example of how one of these blocks could, at\n",
            "least in principle, store a concrete fact. Specifically, it'll be storing the fact that Michael Jordan plays basketball. I should mention the layout here is inspired by a\n",
            "conversation I had with one of those deep-mind researchers, Neil Nanda. For the most part, I will assume that you've either watched the last two chapters, or otherwise you have a\n",
            "basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the overall flow. You and I have been studying a model that's trained to take in a\n",
            "piece of text and predict what comes next. That input text is first broken into a bunch of tokens, which means little chunks that are typically words or little pieces of words, and\n",
            "each token is associated with a high-dimensional vector, which is to say a long list of numbers. This sequence of vectors then repeatedly passes through two kinds of operation.\n",
            "Attention, which allows the vectors to pass information between one another, and then the multi-layer perceptrons, the thing that we're going to dig into today. And also there's a\n",
            "certain normalization step in between. After the sequence of vectors has flowed through many many different iterations of both of these blocks, by the end, the hope is that each\n",
            "vector has soaked up enough information, both from the context, all of the other words and the input, and also from the general knowledge that was baked into the model weights\n",
            "through training, that it can be used to make a prediction of what token comes next. One of the key ideas that I want you to have in your mind is that all of these vectors live in\n",
            "a very very high-dimensional space, and when you think about that space, different directions can encode different kinds of meaning. So a very classic example that I like to refer\n",
            "back to is how if you look at the embedding of woman and subtract the embedding of man, and you take that little step and you add it to another masculine noun, something like\n",
            "uncle, you land somewhere very very close to the corresponding feminine noun. In this sense, this particular direction encodes gender information. The idea is that many other\n",
            "distinct directions in this super high-dimensional space could correspond to other features that the model might want to represent. In a transformer, these vectors don't merely\n",
            "encode the meaning of a single word, though. As they flow through the network, they imbibe a much richer meaning based on all the context around them, and also based on the model's\n",
            "knowledge. Ultimately, each one needs to encode something far far beyond the meaning of a single word, since it needs to be sufficient to predict what will come next. We've already\n",
            "seen how attention blocks let you incorporate context, but a majority of the model parameters actually live inside the MLP blocks, and one thought for what they might be doing is\n",
            "that they offer extra capacity to store facts. Like I said, the lesson here is going to center on the concrete toy example of how exactly it could store the fact that Michael\n",
            "Jordan plays basketball. Now this toy example is going to require that you and I make a couple of assumptions about that high-dimensional space. First, we'll suppose that one of\n",
            "the directions represents the idea of a first name Michael, and then another nearly perpendicular direction represents the idea of the last name Jordan, and then yet a third\n",
            "direction will represent the idea of basketball. So specifically what I mean by this is if you look in the network and you plug out one of the vectors being processed, if its dot\n",
            "product with this first name Michael direction is one, that's what it would mean for the vector to be encoding the idea of a person with that first name. Otherwise, that dot\n",
            "product would be zero or negative, meaning the vector doesn't really align with that direction. And for simplicity, let's completely ignore the very reasonable question of what it\n",
            "might mean if that dot product was bigger than one. Similarly, its dot product with these other directions would tell you whether it represents the last name Jordan, or basketball.\n",
            "So let's say a vector is meant to represent the full name Michael Jordan, then its dot product with both of these directions would have to be one. Since the text Michael Jordan\n",
            "spans two different tokens, this would also mean we have to assume that an earlier attention block has successfully passed information to the second of these two vectors so as to\n",
            "ensure that it can encode both names. With all of those as the assumptions, let's now dive into the meat of the lesson. What happens inside a multi-layer perceptron? You might\n",
            "think of this sequence of vectors flowing into the block, and remember each vector was originally associated with one of the tokens from the input text. What's going to happen is\n",
            "that each individual vector from that sequence goes through a short series of operations, we'll unpack them in just a moment, and at the end we'll get another vector with the same\n",
            "dimension. That other vector is going to get added to the original one that flowed in, and that sum is the result flowing out. This sequence of operations is something you apply to\n",
            "every vector in the sequence associated with every token in the input, and it all happens in parallel. In particular, the vectors don't talk to each other in this step, they're all\n",
            "kind of doing their own thing. And for you and me, that actually makes it a lot simpler, because it means if we understand what happens to just one of the vectors through this\n",
            "block, we effectively understand what happens to all of them. When I say this block is going to encode the fact that Michael Jordan plays basketball, what I mean is that if a\n",
            "vector flows in that encodes, first name Michael and last name Jordan, then this sequence of computations will produce something that includes that direction basketball, which is\n",
            "what we'll add on to the vector in that position. The first step of this process looks like multiplying that vector by a very big matrix, no surprises there, this is deep learning,\n",
            "and this matrix like all of the other ones we've seen is filled with model parameters that are learned from data, which you might think of as a bunch of knobs and dials that get\n",
            "tweaked and tuned to determine what the model behavior is. Now, one nice way to think about matrix multiplication is to imagine each row of that matrix as being its own vector and\n",
            "taking a bunch of dot products between those rows and the vector being processed, which I'll label as E for embedding. For example, suppose that very first row happened to equal\n",
            "this first name Michael direction that were presuming exists, that would mean that the first component in this output, this dot product right here, would be one if that vector\n",
            "encodes the first name Michael and zero or negative otherwise. Even more fun, take a moment to think about what it would mean if that first row was this first name Michael plus\n",
            "last name Jordan direction. And for simplicity, let me go ahead and write that down as m plus j, then taking a dot product with this embedding E, things distribute really nicely,\n",
            "so it looks like m.e plus j.e and notice how that means the ultimate value would be two if the vector encodes the full name Michael Jordan and otherwise it would be one or\n",
            "something smaller than one. And that's just one row in this matrix. You might think of all of the other rows as in parallel asking some other kinds of questions probing at some\n",
            "other sorts of features of the vector being processed. Very often this step also involves adding another vector through the output, which is full of model parameters learned from\n",
            "data, this other vector is known as the bias. For our example, I want you to imagine that the value of this bias in that very first component is negative one, meaning our final\n",
            "output looks like that relevant dot product, but minus one. You might very reasonably ask why I would want you to assume that the model has learned this. And in a moment, you'll\n",
            "see why it's very clean and nice if we have a value here, which is positive, if and only if a vector encodes the full name Michael Jordan and otherwise it's zero or negative. The\n",
            "total number of rows in this matrix, which is something like the number of questions being asked in the case of GPT3, whose numbers we've been following is just under 50,000. In\n",
            "fact, it's exactly four times the number of dimensions in this embedding space. That's a design choice you could make it more, you could make it less, but having a clean multiple\n",
            "tends to be friendly for hardware. Since this matrix full of weights maps us into a higher dimensional space, I'm going to give it the shorthand W up. I'll continue labeling the\n",
            "vector we're processing as E, and let's label this bias vector as B up and put that all back down in the diagram. At this point, a problem is that this operation is purely linear,\n",
            "but language is a very non-linear process. If the entry that we're measuring is high for Michael plus Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps\n",
            "and also Alexis plus Jordan, despite those being unrelated conceptually. What you really want is a simple yes or no for the full name. So the next step is to pass this large\n",
            "intermediate vector through a very simple non-linear function. A common choice is one that takes all of the negative values and maps them to zero, and leaves all of the positive\n",
            "values unchanged. And continuing with the deep learning tradition of overly fancy names, this very simple function is often called the rectified linear unit, or Rayloo for short.\n",
            "Here's what the graph looks like. So taking our imagined example where this first entry of the intermediate vector is one, if and only if the full name is Michael Jordan, and zero\n",
            "or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value where all of the zero and negative values just get clipped to zero. So this output\n",
            "would be one for the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the behavior of an AND gate. Often models will use a slightly modified\n",
            "function that's called the J-LU, which has the same basic shape, it's just a bit smoother, but for our purposes, it's a little bit cleaner if we only think about the Rayloo. Also,\n",
            "when you hear people refer to the neurons of a transformer, they're talking about these values right here. Whenever you see that common neural network picture with a layer of dots\n",
            "and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's typically meant to convey this combination of a linear step, a matrix\n",
            "multiplication, followed by some simple, termwise non-linear function like a Rayloo. You would say that this neuron is active whenever this value is positive, and that it's\n",
            "inactive if that value is zero. The next step looks very similar to the first one. You multiply by a very large matrix and you add on a certain bias term. In this case, the number\n",
            "of dimensions in the output is back down to the size of that embedding space, so I'm going to go ahead and call this the down projection matrix. And this time, instead of thinking\n",
            "of things row by row, it's actually nicer to think of it column by column. You see, another way that you can hold matrix multiplication in your head is to imagine taking each\n",
            "column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding together all of those rescaled columns. The reason it's nicer to\n",
            "think about this way is because here, the columns have the same dimension as the embedding space, so we can think of them as directions in that space. For instance, we will imagine\n",
            "that the model has learned to make that first column into this basket ball direction that we suppose exists. What that would mean is that when the relevant neuron in that first\n",
            "position is active, we'll be adding this column to the final result. But if that neuron was inactive, if that number was zero, then this would have no effect. And it doesn't just\n",
            "have to be basketball. The model could also bake into this column many other features that it wants to associate with something that has the full name Michael Jordan. And at the\n",
            "same time, all of the other columns in this matrix are telling you what will be added to the final result if the corresponding neuron is active. And if you have a bias in this\n",
            "case, it's something that you're just adding every single time, regardless of the neuron values. You might wonder what's that doing, as with all parameter field objects here, it's\n",
            "kind of hard to say exactly, maybe there's some book keeping that the network needs to do, but you can feel free to ignore it for now. Making our notation a little more compact\n",
            "again, I'll call this big matrix W down, and similarly call that bias vector B down and put that back into our diagram. Like I previewed earlier, what you do with this final result\n",
            "is add it to the vector that flowed into the block at that position, and that gets you this final result. So for example, if the vector flowing in encoded both first name Michael\n",
            "and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the basketball direction. So what pops out will encode all of those\n",
            "together. And remember, this is a process happening to every one of those vectors in parallel. In particular, taking the GPT-3 numbers, it means that this block doesn't just have\n",
            "50,000 neurons in it, it has 50,000 times the number of tokens in the input. So that is the entire operation, two matrix products each with a bias added, and a simple clipping\n",
            "function in between. Any of you who watched the earlier videos of the series will recognize this structure as the most basic kind of neural network that we studied there. In that\n",
            "example, it was trained to recognize handwritten digits. Over here, in the context of a transformer for a large language model, this is one piece in a larger architecture, and any\n",
            "attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of a high dimensional embedding space. That is the core\n",
            "lesson, but I do want to step back and reflect on two different things. The first of which is a kind of bookkeeping, and the second of which involves a very thought-provoking fact\n",
            "about higher dimensions that I actually didn't know until I dug into Transformers. In the last two chapters, you and I started counting up the total number of parameters in GPT-3\n",
            "and seeing exactly where they live. So let's quickly finish up the game here. I already mentioned how this up projection matrix has just under 50,000 rows, and that each row\n",
            "matches the size of the embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604 million parameters just for that matrix, and the down projection has\n",
            "the same number of parameters just with a transpose to shape. So together, they give about 1.2 billion parameters. The bias vector also accounts for a couple more parameters, but\n",
            "it's a trivial proportion of the total, so I'm not even going to show it. In GPT-3, this sequence of embedding vectors flows through not one, but 96 distinct MLPs, so the total\n",
            "number of parameters devoted to all of these blocks adds up to about 116 billion. This is around two-thirds of the total parameters in the network, and when you add it to\n",
            "everything that we had before for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of 175 billion as advertised. It's probably worth\n",
            "mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but like the bias vector, they account for a very\n",
            "trivial proportion of the total. As to that second point of reflection, you might be wondering if this central toy example we've been spending so much time on reflects how facts\n",
            "are actually stored in real-large language models. It is true that the rows of that first matrix can be thought of as directions in this embedding space, and that means the\n",
            "activation of each neuron tells you how much a given vector aligns with some specific direction. It's also true that the columns of that second matrix tell you what will be added\n",
            "to the result if that neuron is active. Both of those are just mathematical facts. However, the evidence does suggest that individual neurons very rarely represent a single clean\n",
            "feature like Michael Jordan. And there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as\n",
            "superposition. This is a hypothesis that might help to explain both why the models are especially hard to interpret, and also why they scale surprisingly well. The basic idea is\n",
            "that if you have an end-dimensional space and you want to represent a bunch of different features using directions that are all perpendicular to one another in that space, you\n",
            "know, that way if you add a component in one direction it doesn't influence any of the other directions. Then the maximum number of vectors you can fit is only n, the number of\n",
            "dimensions. To a mathematician actually this is the definition of dimension, but where it gets interesting is if you relax that constraint a little bit and you tolerate some noise.\n",
            "Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89 and 91 degrees apart. If we were in\n",
            "two or three dimensions this makes no difference, that gives you hardly any extra wiggle room to fit more vectors in, which makes it all the more counterintuitive that for higher\n",
            "dimensions the answer changes dramatically. I can give you a really quick and dirty illustration of this using some scrappy python that's going to create a list of 100 dimensional\n",
            "vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there are dimensions. This plot right here shows\n",
            "the distribution of angles between pairs of these vectors, so because they started at random those angles could be anything from 0 to 180 degrees, but you'll notice that already\n",
            "even just for random vectors there's this heavy bias for things to be closer to 90 degrees. Then what I'm going to do is run a certain optimization process that iteratively nudges\n",
            "all of these vectors so that they try to become more perpendicular to one another. After repeating this many different times, here's what the distribution of angles looks like. We\n",
            "have to actually zoom in on it here, because all of the possible angles between pairs of vectors sit inside this narrow range between 89 and 91 degrees. In general, a consequence\n",
            "of something known as the Johnson-Lindon-Strauss lemma is that the number of vectors you can cram into a space that are nearly perpendicular like this, grows exponentially with the\n",
            "number of dimensions. This is very significant for large language models, which might benefit from associating independent ideas with nearly perpendicular directions. It means that\n",
            "it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted. This might partially explain why model performance seems to scale so\n",
            "well with size. A space that has 10 times as many dimensions can store way, way more than 10 times as many independent ideas. And this is relevant not just to that embedding space\n",
            "where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multi-layer perceptron that we just studied. That is to say, at the\n",
            "sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added capacity by using nearly perpendicular directions of the space, it\n",
            "could be probing at many, many more features of the vector being processed. But if it was doing that, what it means is that individual features aren't going to be visible as a\n",
            "single neuron lighting up. It would have to look like some specific combination of neurons instead, a superposition. For any of you curious to learn more, a key relevant search\n",
            "term here is Sparce Autoencoder, which is a tool that some of the interpretability people use to try to extract what the true features are, even if they're superimposed on all\n",
            "these neurons. I'll link to a couple really great anthropic posts all about this. At this point, we haven't touched every detail of a transformer, but you and I have hit the most\n",
            "important points. The main thing that I want to cover in a next chapter is the training process. On the one hand, the short answer for how training works is that it's all back\n",
            "propagation, and we covered back propagation in a separate context with earlier chapters in the series. But there is more to discuss, like the specific cost function used for\n",
            "language models, the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of scaling laws. Quick note for the active followers among you, there are\n",
            "a number of non-machine learning-related videos that I'm excited to sync my teeth into before I make that next chapter, so it might be a while, but I do promise it'll come in due\n",
            "time. So, I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with\n",
            "the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start\n",
            "with the first one. I'll start with the first one. I'll start with the first one. The choice between using ReLU (Rectified Linear Unit) or sigmoid functions in a neural network\n",
            "depends on the specific requirements of the network. Sigmoid functions were commonly used in earlier neural networks due to their ability to introduce non-linearity and limit the\n",
            "output to a range between 0 and 1. However, they have some disadvantages, such as the \"vanishing gradient\" problem, which can make training difficult. ReLU functions, on the other\n",
            "hand, have a simpler structure and do not suffer from the vanishing gradient problem. They also allow the network to learn more complex features and can lead to faster convergence\n",
            "during training. Therefore, ReLU functions are often preferred in modern neural networks. However, there are also other activation functions, such as tanh and leaky ReLU, that can\n",
            "be used depending on the specific application.\n",
            "\n",
            "Answer: \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for question: 'What is Gradient Descent and how does it work?'\n",
            "\n",
            "Question: What is Gradient Descent and how does it work?\n",
            "\n",
            "Context: Here we tackle back propagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walk-through for\n",
            "what the algorithm is actually doing without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus\n",
            "underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward\n",
            "information. Here, we're doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons, and I've been\n",
            "showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as a tensor. I'm also\n",
            "expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain\n",
            "cost function. As a quick reminder for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted it to\n",
            "give, and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this\n",
            "gives you the total cost of the network. As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this\n",
            "cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the\n",
            "topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right\n",
            "now, is that because thinking of the gradient vector as a direction in 13,000 dimensions is to put it lightly beyond the scope of our imaginations, there's another way you can\n",
            "think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process\n",
            "I'm about to describe when you compute the negative gradient and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with\n",
            "this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if you were to\n",
            "wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.\n",
            "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each\n",
            "part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive. It's just that there's a lot of little adjustments getting layered on\n",
            "top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through those effects that each training example is having on the\n",
            "weights and biases. Because the cost function involves averaging a certain cost per example, over all the tens of thousands of training examples, the way that we adjust the weights\n",
            "and biases for a single gradient descent step also depends on every single example, or rather, in principle it should, but for computational efficiency, we're going to do a little\n",
            "trick later to keep you from needing to hit every single example for every single step. In other case, right now, all we're going to do is focus our attention on one single\n",
            "example, this image of a two. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well\n",
            "trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now, we can't directly change those activations. We\n",
            "only have influence on the weights and biases. But it is helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify\n",
            "the image as a two, we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away\n",
            "each current value is from its target value. For example, the increase to that number two neurons activation is in a sense more important than the decrease to the number eight\n",
            "neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember,\n",
            "that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid\n",
            "squishification function, or a ray-lew. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase\n",
            "the weights, and you can change the activations from the previous layer. Focusing just on how the weights should be adjusted? Notice how the weights actually have differing levels\n",
            "of influence. The connections with the brightest neurons from the preceding layer have the biggest effect, since those weights are multiplied by larger activation values. So if you\n",
            "were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least\n",
            "as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we\n",
            "care about which ones give you the most bang for your butt. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons\n",
            "learn, heavy in theory. Often summed up in the phrase, neurons that fire together, wire together. Here, the biggest increases to weights, the biggest strengthening of connections,\n",
            "happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a two get more strongly\n",
            "linked to those firing when thinking about a two. To be clear, I really am not in a position to make statements one way or another about whether artificial networks of neurons\n",
            "behave anything like biological brains, and this fires together, wire together idea comes with a couple meaningful asterisks. But taken as a very loose analogy, I do find it\n",
            "interesting to note. Anyway, the third way that we can help increase this neuron's activation is by changing all the activations in the previous layer. Namely, if everything\n",
            "connected to that digit two neuron with a positive weight, got brighter, and if everything connected with a negative weight got dimmer, then that digit two neuron would become more\n",
            "active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now,\n",
            "of course, we cannot directly influence those activations. We only have control over the weights and biases. But just as with the last layer, it's helpful to just keep a note of\n",
            "what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit two output neuron wants. Remember, we also want all of the other neurons\n",
            "in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So the desire of this\n",
            "digit two neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer. Again, in proportion to the corresponding\n",
            "weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these\n",
            "desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the\n",
            "relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further,\n",
            "remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listen to what that two wanted, the network would\n",
            "ultimately be incentivized just to classify all images as a two. What you do is you go through this same back property for every other training example, recording how each of them\n",
            "would like to change the weights and the biases. And you average together those desired changes. This collection here of the average to nudges to each weight and bias is, loosely\n",
            "speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get\n",
            "quantitatively precise about those nudges. But if you understood every change that I just referenced, why some are proportionally bigger than others, and how they all need to be\n",
            "added together, you understand the mechanics for what back propagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence\n",
            "of every single training example, every single gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a\n",
            "whole bunch of mini-batches. Let's say each one having 100 training examples. Then you compute a step according to the mini-batch. It's not going to be the actual gradient to the\n",
            "cost function, which depends on all of the training data, not this tiny subset. So it's not the most efficient step downhill. But each mini-batch does give you a pretty good\n",
            "approximation, and more importantly, it gives you a significant computational speed up. If you would applaud the trajectory of your network under the relevant cost surface, it\n",
            "would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of\n",
            "each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's kind of a lot going on here, so let's\n",
            "just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases. Not just in terms\n",
            "of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would\n",
            "involve doing this for all your tens and thousands of training examples and averaging the desired changes that you get. But that's computationally slow, so instead you randomly\n",
            "subdivide the data into these mini batches and compute each step with respect to a mini batch. Repeatedly going through all of the mini batches and making these adjustments, you\n",
            "will converge towards a local minimum of the cost function, which is to say your network is going to end up doing a really good job on the training examples. So with all of that\n",
            "said, every line of code that would go into implementing Backprop actually corresponds with something that you have now seen, at least in informal terms. But sometimes knowing what\n",
            "the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video\n",
            "goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in\n",
            "other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need\n",
            "a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the M-NIST database, with so many examples that have been\n",
            "labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data that you actually need,\n",
            "whether that's having people label tens of thousands of images or whatever other data type you might be dealing with. Last video, I laid out the structure of a neural network. I'll\n",
            "give a quick recap here just so that it's fresh in our minds and then I have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies\n",
            "not only how neural networks learn, but how a lot of other machine learning works as well. Then after that, we're going to dig in a little more to how this particular network\n",
            "performs and what those hidden layers of neurons end up actually looking for. As a reminder, our goal here is the classic example of handwritten digit recognition, the Hello World\n",
            "of Neural Networks. These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons\n",
            "in the input layer of the network. And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer plus some\n",
            "special number called a bias. Then you compose that sum with some other function, like the sigmoid squishification or a ray-loo, the way that I walked through last video. In total,\n",
            "given the somewhat arbitrary choice of two hidden layers here with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that\n",
            "determine what exactly the network actually does. And what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final\n",
            "layer corresponds to that digit. And remember, the motivation that we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the\n",
            "third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits. So here we learn how the network learns.\n",
            "What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along\n",
            "with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered\n",
            "structure will mean that what it learns generalizes to images beyond that training data. And the way we test that is that after you train the network, you show it more labeled\n",
            "data, that it's never seen before, and you see how accurately it classifies those new images. Fortunately for us, and what makes this such a common example to start with, is that\n",
            "the good people behind the M-NIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers that they're supposed\n",
            "to be. And it's provocative as it is to describe a machine as learning. Once you actually see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like,\n",
            "a calculus exercise. I mean, basically, it comes down to finding the minimum of a certain function. Remember, conceptually, we're thinking of each neuron as being connected to all\n",
            "of the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections. The bias is some indication\n",
            "of whether that neuron tends to be active or inactive. And to start things off, we're just going to initialize all of those weights and biases totally randomly. Needless to say,\n",
            "this network is going to perform pretty horribly on a given training example, since it's just doing something random. For example, you feed in this image of a three, and the output\n",
            "layer just looks like a mess. So what you do is you define a cost function, a way of telling the computer, no, bad computer, that output should have activations, which are zero for\n",
            "most neurons, but one for this neuron. What you gave me is utter trash. To say that a little more mathematically, what you do is add up the squares of the differences between each\n",
            "of those trash output activations and the value that you want them to have. And this is what we'll call the cost of a single training example. Notice, this sum is small when the\n",
            "network confidently classifies the image correctly, but it's large when the network seems like it doesn't really know what it's doing. So then what you do is consider the average\n",
            "cost over all of the tens of thousands of training examples at your disposal. This average cost is our measure for how lousy the network is and how bad the computer should feel.\n",
            "And that's a complicated thing. Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its\n",
            "output. And in a sense, it's parameterized by all these weights and biases. Well the cost function is a layer of complexity on top of that. It takes as its input, those 13,000 or\n",
            "so weights and biases, and it spits out a single number describing how bad those weights and biases are. And the way it's defined depends on the network's behavior over all the\n",
            "tens of thousands of pieces of training data. That's a lot to think about. But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to\n",
            "change those weights and biases so that it gets better. To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has\n",
            "one number as an input and one number as an output. How do you find an input that minimizes the value of this function? Circular students will know that you can sometimes figure\n",
            "out that minimum explicitly. But that's not always feasible for really complicated functions. Certainly not in the 13,000 input version of this situation for our crazy complicated\n",
            "neural network cost function. A more flexible tactic is to start at any all input and figure out which direction you should step to make that output lower. Specifically, if you can\n",
            "figure out the slope of the function where you are, then shift to the left if that slope is positive and shift the input to the right if that slope is negative. If you do this\n",
            "repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function. And the image you might have in mind\n",
            "here is a ball rolling down a hill. And notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which\n",
            "random input you start at. There's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function. That's going to carry over to\n",
            "our neural network case as well. And I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum,\n",
            "your steps get smaller and smaller, and that kind of helps you from overshooting. Bumping up the complexity a bit, imagine instead a function with two inputs and one output. You\n",
            "might think of the input space as the x, y plane and the cost function as being graft as a surface above it. Now instead of asking about the slope of the function, you have to ask\n",
            "which direction should you step in this input space so as to decrease the output of the function most quickly. In other words, what's the downhill direction? And again, it's\n",
            "helpful to think of a ball rolling down that hill. Most of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest\n",
            "ascent, basically which direction should you step to increase the function most quickly. Naturally enough, taking the negative of that gradient gives you the direction to step that\n",
            "decreases the function most quickly. And even more than that, the length of this gradient vector is actually an indication for just how steep that steepest slope is. Now if you're\n",
            "unfamiliar with multivariable calculus and you want to learn more, check out some of the work that I did for Khan Academy on the topic. Honestly though, all that matters for you\n",
            "and me right now is that in principle there exists a way to compute this vector. This vector that tells you what the downhill direction is and how steep it is. You'll be okay if\n",
            "that's all you know and you're not rock solid on the details. Because if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then\n",
            "take a small step downhill and just repeat that over and over. It's the same basic idea for a function that has 13,000 inputs instead of two inputs. Imagine organizing all 13,000\n",
            "weights and biases of our network into a giant column vector. The negative gradient of the cost function is just a vector. It's some direction inside this insanely huge input space\n",
            "that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function. And of course, with our specially designed cost function,\n",
            "changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values and more like an\n",
            "actual decision that we want it to make. It's important to remember, this cost function involves an average over all of the training data. So if you minimize it, it means it's a\n",
            "better performance on all of those samples. The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called back\n",
            "propagation. And it's what I'm going to be talking about next video. There I really want to take the time to walk through what exactly happens to each weight and each bias for a\n",
            "given piece of training data. Trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas. Right here, right now, the main thing I want\n",
            "you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function. And notice, one\n",
            "consequence of that is that it's important for this cost function to have a nice smooth output so that we can find a local minimum by taking little steps down hill. This is why, by\n",
            "the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way that biological neurons are. This process of\n",
            "repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent. It's a way to converge toward some local minimum of a cost function,\n",
            "basically a valley in this graph. I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000-dimensional input space are a little hard to\n",
            "wrap your mind around, but there is actually a nice non-spatial way to think about this. Each component of the negative gradient tells us two things. The sign, of course, tells us\n",
            "whether the corresponding component of the input vector should be nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you which\n",
            "changes matter more. You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.\n",
            "Some of these connections just matter more for our training data. So a way that you can think about this gradient vector of our mind-warpingly massive cost function is that it\n",
            "encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck. This really is just another way of thinking\n",
            "about direction. To take a simpler example, if you have some function with two variables as an input, and you compute that it's gradient at some particular point, comes out as 3-1.\n",
            "Then on the one hand, you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly. But when you graph\n",
            "the function above the plane of input points, that vector is what's giving you the straight uphill direction. But another way to read that is to say that changes to this first\n",
            "variable have three times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for\n",
            "your buck. Alright, let's zoom out and sum up where we are so far. The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted\n",
            "sums. The cost function is a layer of complexity on top of that. It takes the 13,000 weights and biases as inputs, and spits out a single measure of laziness based on the training\n",
            "examples. And the gradient of the cost function is one more layer of complexity still. It tells us what nudges to all of these weights and biases cause the fastest change to the\n",
            "value of the cost function, which you might interpret as saying which changes to which weights matter the most. So, when you initialize the network with random weights and biases\n",
            "and adjust them many times based on this gradient descent process, how well does it actually perform on images that it's never seen before? Well the one that I've described here,\n",
            "with the two hidden layers of 16 neurons each, goes in mostly for aesthetic reasons. Well, it's not bad. It classifies about 96% of the new images that it sees correctly. And\n",
            "honestly, if you look at some of the examples that it messes up on, you kind of feel compelled to cut it a little slack. Now, if you play around with the hidden layer structure and\n",
            "make a couple tweaks, you can get this up to 98%. And that's pretty good. It's not the best. You can certainly get better performance by getting more sophisticated than this plain\n",
            "vanilla network. But given how daunting the initial task is, I just think there's something incredible about any network doing this well on images that it's never seen before,\n",
            "given that we never specifically told it what patterns to look for. Originally, the way that I motivated this structure was by describing a hope that we might have, that the second\n",
            "layer might pick up on little edges, that the third layer would piece together those edges to recognize loops in longer lines, and that those might be piece together to recognize\n",
            "digits. So is this what our network is actually doing? Well, for this one, at least, not at all. Remember how last video we looked at how the weights of the connections from all of\n",
            "the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that that second layer neuron is picking up on? Well, when we\n",
            "actually do that, for the weights associated with these transitions from the first layer to the next. Instead of picking up on isolated little edges here and there, they look\n",
            "almost random, just with some very loose patterns in the middle there. It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our\n",
            "network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns that we might have hoped for. And to\n",
            "really drive this point home, watch what happens when you input a random image. If the system was smart, you might expect it to either feel uncertain, maybe, not really activating\n",
            "any of those 10 output neurons or activating them all evenly. But instead, it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as\n",
            "it does that an actual image of a 5 is a 5. Fraze differently? Even if this network can recognize digits pretty well, it has no idea how to draw them. A lot of this is because it's\n",
            "such a tightly constrained training setup. I mean, put yourself in the network's shoes here. From its point of view, the entire universe consists of nothing but clearly defined\n",
            "unmoving digits centered in a tiny grid. And its cost function just never gave it any incentive to be anything but utterly confident in its decisions. So with this is the image of\n",
            "what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. That's just not at\n",
            "all what it ends up doing. Well this is not meant to be our end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s and 90s. And you\n",
            "do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems. But the more you dig in to what\n",
            "those hidden layers are really doing, the less intelligent it seems. Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage\n",
            "actively with the material here somehow. One pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what changes you might make to\n",
            "this system and how it perceives images if you wanted it to better pick up on things like edges and patterns. But better than that, to actually engage with the material, I highly\n",
            "recommend the book by Michael Neilsen on deep learning and neural networks. In it, you can find the code and the data to download and play with for this exact example, and the book\n",
            "will walk you through step by step what that code is doing. What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining\n",
            "me in making a donation towards Neilsen's efforts. I've also linked a couple other resources that I like a lot in the description, including the phenomenal and beautiful blog posts\n",
            "by Chris Ola and the articles in Distill. To close things off here for the last few minutes, I want to jump back into a snippet of the interview that I had with Lisha Lee. You\n",
            "might remember her from the last video, she did her PhD work in deep learning. And in this little snippet, she talks about two recent papers that really dig in to how some of the\n",
            "more modern image recognition networks are actually learning. Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks\n",
            "that's really good at image recognition, and instead of training it on a properly labeled data set, it shuffled all of the labels around before training. Obviously the testing\n",
            "accuracy here was going to be no better than random, since everything's just randomly labeled. But it was still able to achieve the same training accuracy as you would on a\n",
            "properly labeled data set. Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which kind of raises the question for\n",
            "whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization? It reminds the entire data set of what the correct\n",
            "classification is. And so a couple of half a year later at ICML this year, there was not exactly rebuttal paper, paper that addressed some aspects of like, hey, actually these\n",
            "networks are doing something a little bit smarter than that. If you look at that accuracy curve, if you were just training on a random data set, that curve sort of went down very\n",
            "slowly in almost a linear fashion. So you're really struggling to find that local minimum of possible, the right weights that would get you that accuracy. Whereas if you're\n",
            "actually training on a structure data set, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that\n",
            "accuracy level. And so in some sense, it was easier to find that local maxima. And so it was also interesting about that, is it brings into light another paper from actually a\n",
            "couple of years ago, which has a lot more simplifications about the network layers. But one of the results was saying how if you look at the optimization landscape, the local\n",
            "minima that these networks tend to learn are actually of equal quality. So in some sense, if your data set a structure, you should be able to find that much more easily. My thanks,\n",
            "as always, to those of you supporting on Patreon. I've said before just what a game changer in Patreon is, but these videos really would not be possible without you. I also want to\n",
            "give a special thanks to the VC firm Amplify Partners in their support of these initial videos in the series. The hard assumption here is that you've watched Part 3, giving an\n",
            "intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the relevant calculus. It's normal for this to be at least a little\n",
            "confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal is to show how people in machine learning commonly think about\n",
            "the chain rule from calculus in the context of networks, which has kind of a different feel from how most introductory calculus courses approach the subject. For those of you\n",
            "uncomfortable with the relevant calculus, I do have a whole series on the topic. Let's just start off with an extremely simple network, one where each layer has a single neuron in\n",
            "it. So this particular network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. That way we\n",
            "know which adjustments to those terms is going to cause the most efficient decrease to the cost function. And we're just going to focus on the connection between the last two\n",
            "neurons. Let's label the activation of that last neuron with a superscript L indicating which layer it's in. So the activation of the previous neuron is a L minus 1. These are not\n",
            "exponents, they're just a way of indexing what we're talking about since I want to save subscripts for different indices later on. Now let's say that the value we want this last\n",
            "activation to be for a given training example is y, for example, y might be 0 or 1. So the cost of this simple network for a single training example is AL minus y squared. We'll\n",
            "denote the cost of that one training example as C0. As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neurons activation\n",
            "plus some bias, which I'll call BL. And then you pump that through some special nonlinear function like the sigmoid or a ray loop. It's actually going to make things easier for us\n",
            "if we give a special name to this weighted sum, like Z, with the same superscript as the relevant activations. So this is a lot of terms and a way that you might conceptualize it\n",
            "is that the weight, the previous action and the bias altogether are used to compute Z, which in turn lets us compute A, which finally, along with a constant Y, lets us compute the\n",
            "cost. And of course, AL minus 1 is influenced by its own weight and bias and such. But we're not going to focus on that right now. Now all of these are just numbers, right? And it\n",
            "can be nice to think of each one as having its own little number line. Our first goal is to understand how sensitive the cost function is to small changes in our weight, WL. Never\n",
            "phrase differently. What is the derivative of C with respect to WL? When you see this Dell W term, think of it as meaning some tiny nudge to W, like a change by 0.01. And think of\n",
            "this Dell C term as meaning whatever the resulting nudge to the cost is. What we want is their ratio. Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn\n",
            "causes some nudge to AL, which directly influences the cost. So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is the\n",
            "derivative of ZL with respect to WL. Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge\n",
            "to C and this intermediate nudge to AL. This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of C to small changes in WL. So on\n",
            "screen right now, there's kind of a lot of symbols, and take a moment to just make sure it's clear what they all are. Because now we're going to compute the relevant derivatives.\n",
            "The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this means that its size is proportional to the difference between the networks output and the\n",
            "thing that we want it to be. So if that output was very different, even slight changes stand to have a big impact on the final cost function. The derivative of AL with respect to\n",
            "ZL is just the derivative of our sigmoid function, or whatever non-linearity you choose to use. And the derivative of ZL with respect to WL, in this case comes out just to be AL\n",
            "minus 1. Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all actually\n",
            "mean. In the case of this last derivative, the amount that that small nudge to the weight influenced the last layer depends on how strong the previous neuron is. Remember, this is\n",
            "where that neurons that fire together wire together idea comes in. And all of this is the derivative with respect to WL only of the cost for a specific single training example.\n",
            "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression that we found\n",
            "over all training examples. And of course that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect\n",
            "to all those weights and biases. But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. The sensitivity to the bias, for example,\n",
            "is almost identical. We just need to change out this del Z del W term for a del Z del B. And if you look at the relevant formula, that derivative comes out to be 1. Also, and this\n",
            "is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. Namely, this initial derivative in the\n",
            "chain rule expression, the sensitivity of Z to the previous activation, comes out to be the weight WL. And again, even though we're not going to be able to directly influence that\n",
            "previous layer activation, it's helpful to keep track of. Because now, we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to\n",
            "previous weights and previous biases. And you might think that this is an overly simple example, since all layers just have one neuron, and that things are going to get\n",
            "exponentially more complicated for a real network. But honestly, not that much changes when we give the layers multiple neurons. Really it's just a few more indices to keep track\n",
            "of. Rather than the activation of a given layer simply being AL, it's also going to have a subscript, indicating which neuron of that layer it is. Let's go ahead and use the letter\n",
            "K to index the layer L minus 1, and J to index the layer L. For the cost, again, we look at what the desired output is, but this time we add up the squares of the differences\n",
            "between these last layer activations and the desired output. That is, you take a sum over ALJ minus YJ squared. Since there's a lot more weights, each one has to have a couple more\n",
            "indices to keep track of where it is. So let's call the weight of the edge connecting this Kth neuron to the Jth neuron, WLJK. Those indices might feel a little backwards at first,\n",
            "but it lines up with how you'd index the weight matrix that I talked about in the Part 1 video. Just as before, it's still nice to give a name to the relevant weighted sum, like Z,\n",
            "so that the activation of the last layer is just your special function, like the sigmoid, applied to Z. You can kind of see what I mean, right, where all of these are essentially\n",
            "the same equations that we had before in the one neuron per layer case. It's just that it looks a little more complicated. And indeed, the chain ruled the rivetive expression,\n",
            "describing how sensitive the cost is to a specific weight, looks essentially the same. I'll leave it to you to pause and think about each of those terms if you want. What does\n",
            "change here, though, is the derivative of the cost with respect to one of the activations in the layer L minus 1. In this case, the difference is that the neuron influences the\n",
            "cost function through multiple different paths. It is on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also\n",
            "plays a role in the cost function, and you have to add those up. And that, well, that's pretty much it. Once you know how sensitive the cost function is to the activations in this\n",
            "second to last layer, you can just repeat the process for all the weights and biases feeding into that layer. So pat yourself on the back. If all of this makes sense, you have now\n",
            "looked deep into the heart of back propagation, the workhorse behind how neural networks learn. These chain rule expressions give you the derivatives that determine each component\n",
            "in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. If you sit back and think about all that, this is a lot of layers of complexity to wrap\n",
            "your mind around. So don't worry if it takes time for your mind to digest it all. If you feed a large language model the phrase, Michael Jordan plays the sport of blank, and you\n",
            "have it predict what comes next, and it correctly predicts basketball. This would suggest that somewhere, inside its hundreds of billions of parameters, it's baked in knowledge\n",
            "about a specific person and his specific sport. And I think in general anyone who's played around with one of these models has the clear sense that it's memorized tons and tons of\n",
            "facts. So a reasonable question you could ask is, how exactly does that work, and where do those facts live? Last December a few researchers from Google Deep Mind posted about work\n",
            "on this question, and they were using this specific example of matching athletes to their sports. And although a full mechanistic understanding of how facts are stored remains\n",
            "unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of these networks, known\n",
            "fancifully as the multi-layer perceptrons, or MLPs for short. In the last couple of chapters, you and I have been digging into the details behind transformers, the architecture\n",
            "underlying large language models, and also underlying a lot of other modern AI. In the most recent chapter we were focusing on a piece called Attention, and the next step for you\n",
            "and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network. The computation here is actually\n",
            "relatively simple, especially when you compare it to attention. It boils down essentially to a pair of matrix multiplications with a simple something in between. However,\n",
            "interpreting what these computations are doing is exceedingly challenging. Our main goal here is to step through the computations and make them memorable, but I'd like to do it in\n",
            "the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact. Specifically, it'll be storing the fact that Michael\n",
            "Jordan plays basketball. I should mention the layout here is inspired by a conversation I had with one of those deep-mind researchers, Neil Nanda. For the most part, I will assume\n",
            "that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick reminder of the\n",
            "overall flow. You and I have been studying a model that's trained to take in a piece of text and predict what comes next. That input text is first broken into a bunch of tokens,\n",
            "which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long list of numbers.\n",
            "This sequence of vectors then repeatedly passes through two kinds of operation. Attention, which allows the vectors to pass information between one another, and then the multi-\n",
            "layer perceptrons, the thing that we're going to dig into today. And also there's a certain normalization step in between. After the sequence of vectors has flowed through many\n",
            "many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other words and the\n",
            "input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next. One of the key\n",
            "ideas that I want you to have in your mind is that all of these vectors live in a very very high-dimensional space, and when you think about that space, different directions can\n",
            "encode different kinds of meaning. So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of man, and you\n",
            "take that little step and you add it to another masculine noun, something like uncle, you land somewhere very very close to the corresponding feminine noun. In this sense, this\n",
            "particular direction encodes gender information. The idea is that many other distinct directions in this super high-dimensional space could correspond to other features that the\n",
            "model might want to represent. In a transformer, these vectors don't merely encode the meaning of a single word, though. As they flow through the network, they imbibe a much richer\n",
            "meaning based on all the context around them, and also based on the model's knowledge. Ultimately, each one needs to encode something far far beyond the meaning of a single word,\n",
            "since it needs to be sufficient to predict what will come next. We've already seen how attention blocks let you incorporate context, but a majority of the model parameters actually\n",
            "live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts. Like I said, the lesson here is going to center on the\n",
            "concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball. Now this toy example is going to require that you and I make a couple of\n",
            "assumptions about that high-dimensional space. First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly perpendicular\n",
            "direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball. So specifically what I mean by this is if you look in\n",
            "the network and you plug out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the vector to be\n",
            "encoding the idea of a person with that first name. Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction. And for\n",
            "simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one. Similarly, its dot product with these other\n",
            "directions would tell you whether it represents the last name Jordan, or basketball. So let's say a vector is meant to represent the full name Michael Jordan, then its dot product\n",
            "with both of these directions would have to be one. Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block\n",
            "has successfully passed information to the second of these two vectors so as to ensure that it can encode both names. With all of those as the assumptions, let's now dive into the\n",
            "meat of the lesson. What happens inside a multi-layer perceptron? You might think of this sequence of vectors flowing into the block, and remember each vector was originally\n",
            "associated with one of the tokens from the input text. What's going to happen is that each individual vector from that sequence goes through a short series of operations, we'll\n",
            "unpack them in just a moment, and at the end we'll get another vector with the same dimension. That other vector is going to get added to the original one that flowed in, and that\n",
            "sum is the result flowing out. This sequence of operations is something you apply to every vector in the sequence associated with every token in the input, and it all happens in\n",
            "parallel. In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing. And for you and me, that actually makes it a lot simpler,\n",
            "because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them. When I say this block is going\n",
            "to encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes, first name Michael and last name Jordan, then this sequence of\n",
            "computations will produce something that includes that direction basketball, which is what we'll add on to the vector in that position. The first step of this process looks like\n",
            "multiplying that vector by a very big matrix, no surprises there, this is deep learning, and this matrix like all of the other ones we've seen is filled with model parameters that\n",
            "are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is. Now, one nice way to think about\n",
            "matrix multiplication is to imagine each row of that matrix as being its own vector and taking a bunch of dot products between those rows and the vector being processed, which I'll\n",
            "label as E for embedding. For example, suppose that very first row happened to equal this first name Michael direction that were presuming exists, that would mean that the first\n",
            "component in this output, this dot product right here, would be one if that vector encodes the first name Michael and zero or negative otherwise. Even more fun, take a moment to\n",
            "think about what it would mean if that first row was this first name Michael plus last name Jordan direction. And for simplicity, let me go ahead and write that down as m plus j,\n",
            "then taking a dot product with this embedding E, things distribute really nicely, so it looks like m.e plus j.e and notice how that means the ultimate value would be two if the\n",
            "vector encodes the full name Michael Jordan and otherwise it would be one or something smaller than one. And that's just one row in this matrix. You might think of all of the other\n",
            "rows as in parallel asking some other kinds of questions probing at some other sorts of features of the vector being processed. Very often this step also involves adding another\n",
            "vector through the output, which is full of model parameters learned from data, this other vector is known as the bias. For our example, I want you to imagine that the value of\n",
            "this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one. You might very reasonably ask why I would want\n",
            "you to assume that the model has learned this. And in a moment, you'll see why it's very clean and nice if we have a value here, which is positive, if and only if a vector encodes\n",
            "the full name Michael Jordan and otherwise it's zero or negative. The total number of rows in this matrix, which is something like the number of questions being asked in the case\n",
            "of GPT3, whose numbers we've been following is just under 50,000. In fact, it's exactly four times the number of dimensions in this embedding space. That's a design choice you\n",
            "could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware. Since this matrix full of weights maps us into a higher dimensional\n",
            "space, I'm going to give it the shorthand W up. I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the\n",
            "diagram. At this point, a problem is that this operation is purely linear, but language is a very non-linear process. If the entry that we're measuring is high for Michael plus\n",
            "Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually. What you really want is a\n",
            "simple yes or no for the full name. So the next step is to pass this large intermediate vector through a very simple non-linear function. A common choice is one that takes all of\n",
            "the negative values and maps them to zero, and leaves all of the positive values unchanged. And continuing with the deep learning tradition of overly fancy names, this very simple\n",
            "function is often called the rectified linear unit, or Rayloo for short. Here's what the graph looks like. So taking our imagined example where this first entry of the intermediate\n",
            "vector is one, if and only if the full name is Michael Jordan, and zero or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value where all of\n",
            "the zero and negative values just get clipped to zero. So this output would be one for the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the\n",
            "behavior of an AND gate. Often models will use a slightly modified function that's called the J-LU, which has the same basic shape, it's just a bit smoother, but for our purposes,\n",
            "it's a little bit cleaner if we only think about the Rayloo. Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.\n",
            "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's\n",
            "typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple, termwise non-linear function like a Rayloo. You would say that this\n",
            "neuron is active whenever this value is positive, and that it's inactive if that value is zero. The next step looks very similar to the first one. You multiply by a very large\n",
            "matrix and you add on a certain bias term. In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm going to go ahead and call\n",
            "this the down projection matrix. And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column. You see, another way that you can\n",
            "hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding\n",
            "together all of those rescaled columns. The reason it's nicer to think about this way is because here, the columns have the same dimension as the embedding space, so we can think\n",
            "of them as directions in that space. For instance, we will imagine that the model has learned to make that first column into this basket ball direction that we suppose exists. What\n",
            "that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result. But if that neuron was inactive, if that number\n",
            "was zero, then this would have no effect. And it doesn't just have to be basketball. The model could also bake into this column many other features that it wants to associate with\n",
            "something that has the full name Michael Jordan. And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the\n",
            "corresponding neuron is active. And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values. You might wonder\n",
            "what's that doing, as with all parameter field objects here, it's kind of hard to say exactly, maybe there's some book keeping that the network needs to do, but you can feel free\n",
            "to ignore it for now. Making our notation a little more compact again, I'll call this big matrix W down, and similarly call that bias vector B down and put that back into our\n",
            "diagram. Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position, and that gets you this final result. So\n",
            "for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the\n",
            "basketball direction. So what pops out will encode all of those together. And remember, this is a process happening to every one of those vectors in parallel. In particular, taking\n",
            "the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input. So that is the entire operation, two\n",
            "matrix products each with a bias added, and a simple clipping function in between. Any of you who watched the earlier videos of the series will recognize this structure as the most\n",
            "basic kind of neural network that we studied there. In that example, it was trained to recognize handwritten digits. Over here, in the context of a transformer for a large language\n",
            "model, this is one piece in a larger architecture, and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of\n",
            "a high dimensional embedding space. That is the core lesson, but I do want to step back and reflect on two different things. The first of which is a kind of bookkeeping, and the\n",
            "second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into Transformers. In the last two chapters, you and I\n",
            "started counting up the total number of parameters in GPT-3 and seeing exactly where they live. So let's quickly finish up the game here. I already mentioned how this up projection\n",
            "matrix has just under 50,000 rows, and that each row matches the size of the embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604 million\n",
            "parameters just for that matrix, and the down projection has the same number of parameters just with a transpose to shape. So together, they give about 1.2 billion parameters. The\n",
            "bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even going to show it. In GPT-3, this sequence of embedding vectors\n",
            "flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion. This is around two-thirds of the total\n",
            "parameters in the network, and when you add it to everything that we had before for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of\n",
            "175 billion as advertised. It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but\n",
            "like the bias vector, they account for a very trivial proportion of the total. As to that second point of reflection, you might be wondering if this central toy example we've been\n",
            "spending so much time on reflects how facts are actually stored in real-large language models. It is true that the rows of that first matrix can be thought of as directions in this\n",
            "embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction. It's also true that the columns of that second\n",
            "matrix tell you what will be added to the result if that neuron is active. Both of those are just mathematical facts. However, the evidence does suggest that individual neurons\n",
            "very rarely represent a single clean feature like Michael Jordan. And there may actually be a very good reason this is the case, related to an idea floating around interpretability\n",
            "researchers these days known as superposition. This is a hypothesis that might help to explain both why the models are especially hard to interpret, and also why they scale\n",
            "surprisingly well. The basic idea is that if you have an end-dimensional space and you want to represent a bunch of different features using directions that are all perpendicular\n",
            "to one another in that space, you know, that way if you add a component in one direction it doesn't influence any of the other directions. Then the maximum number of vectors you\n",
            "can fit is only n, the number of dimensions. To a mathematician actually this is the definition of dimension, but where it gets interesting is if you relax that constraint a little\n",
            "bit and you tolerate some noise. Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89\n",
            "and 91 degrees apart. If we were in two or three dimensions this makes no difference, that gives you hardly any extra wiggle room to fit more vectors in, which makes it all the\n",
            "more counterintuitive that for higher dimensions the answer changes dramatically. I can give you a really quick and dirty illustration of this using some scrappy python that's\n",
            "going to create a list of 100 dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there\n",
            "are dimensions. This plot right here shows the distribution of angles between pairs of these vectors, so because they started at random those angles could be anything from 0 to 180\n",
            "degrees, but you'll notice that already even just for random vectors there's this heavy bias for things to be closer to 90 degrees. Then what I'm going to do is run a certain\n",
            "optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another. After repeating this many different times, here's\n",
            "what the distribution of angles looks like. We have to actually zoom in on it here, because all of the possible angles between pairs of vectors sit inside this narrow range between\n",
            "89 and 91 degrees. In general, a consequence of something known as the Johnson-Lindon-Strauss lemma is that the number of vectors you can cram into a space that are nearly\n",
            "perpendicular like this, grows exponentially with the number of dimensions. This is very significant for large language models, which might benefit from associating independent\n",
            "ideas with nearly perpendicular directions. It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted. This might\n",
            "partially explain why model performance seems to scale so well with size. A space that has 10 times as many dimensions can store way, way more than 10 times as many independent\n",
            "ideas. And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multi-\n",
            "layer perceptron that we just studied. That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added\n",
            "capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed. But if it was doing that, what it\n",
            "means is that individual features aren't going to be visible as a single neuron lighting up. It would have to look like some specific combination of neurons instead, a\n",
            "superposition. For any of you curious to learn more, a key relevant search term here is Sparce Autoencoder, which is a tool that some of the interpretability people use to try to\n",
            "extract what the true features are, even if they're superimposed on all these neurons. I'll link to a couple really great anthropic posts all about this. At this point, we haven't\n",
            "touched every detail of a transformer, but you and I have hit the most important points. The main thing that I want to cover in a next chapter is the training process. On the one\n",
            "hand, the short answer for how training works is that it's all back propagation, and we covered back propagation in a separate context with earlier chapters in the series. But\n",
            "there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of\n",
            "scaling laws. Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sync my teeth into before I make that\n",
            "next chapter, so it might be a while, but I do promise it'll come in due time. So, I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll\n",
            "start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one.\n",
            "I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. Imagine you happen across\n",
            "a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose\n",
            "you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You can then finish the script by feeding in what you\n",
            "have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact\n",
            "with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead\n",
            "of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, what you do is lay out some text that describes\n",
            "an interaction between a user and a hypothetical AI assistant. You add on whatever the user types in as the first part of that interaction. Then you have the model repeatedly\n",
            "predict the next word that such a hypothetical AI assistant would say in response, and that's what's presented to the user. In doing this, the output tends to look a lot more\n",
            "natural if you allow it to select less likely words along the way at random. So what this means is, even though the model itself is deterministic, a given prompt typically gives a\n",
            "different answer each time it's run. Models learn how to make these predictions by processing an enormous amount of text, typically pulled from the internet. For a standard human\n",
            "to read the amount of text that was used to train GPT-3, for example, if they read non-stop 24-7, it would take over 2600 years, larger models since then, train on much, much more.\n",
            "You can think of training a little bit like tuning the dials on a big machine. The way that a language model behaves is entirely determined by these many different continuous\n",
            "values, usually called parameters or weights. Changing those parameters will change the probabilities that the model gives for the next word on a given input. What puts the large\n",
            "in large language model is how they can have hundreds of billions of these parameters. No human ever deliberately sets those parameters. Instead they begin at random, meaning the\n",
            "model just outputs gibberish, but they're repeatedly refined based on many example pieces of text. One of these training examples could be just a handful of words, or it could be\n",
            "thousands, but in either case the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last\n",
            "word from the example. An algorithm called back propagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true\n",
            "last word and a little less likely to choose all the others. When you do this for many, many trillions of examples, not only does the model start to give more accurate predictions\n",
            "on the training data, but it also starts to make more reasonable predictions on text that it's never seen before. Given the huge number of parameters and the enormous amount of\n",
            "training data, the scale of computation involved in training a large language model is mind-boggling. To illustrate, imagine that you could perform one billion additions and\n",
            "multiplications every single second. How long do you think that it would take for you to do all of the operations involved in training the largest language models? Do you think it\n",
            "would take a year? Maybe something like 10,000 years? The answer is actually much more than that. It's well over 100 million years. This is only part of the story though. This\n",
            "whole process is called pre-training. The goal of auto-completing a random passage of text from the internet is very different from the goal of being a good AI assistant. To\n",
            "address this, chatbots undergo another type of training, just as important, called reinforcement learning with human feedback. Use flag-unhelpful or problematic predictions and\n",
            "their corrections further change the model's parameters, making them more likely to give predictions that users prefer. Looking back at the pre-training though, this staggering\n",
            "amount of computation is only made possible by using special computer chips that are optimized for running many, many operations in parallel, known as GPUs. However, not all\n",
            "language models can be easily parallelized. Prior to 2017, most language models would process text one word at a time. But then, a team of researchers at Google introduced a new\n",
            "model known as the transformer. Transformers don't read text from the start to the finish. They soak it all in at once in parallel. The very first step inside a transformer, and\n",
            "most other language models for that matter, is to associate each word with a long list of numbers. The reason for this is that the training process only works with continuous\n",
            "values, so you have to somehow encode language using numbers, and each of these list of numbers may somehow encode the meaning of the corresponding word. What makes Transformers\n",
            "unique is their reliance on a special operation known as attention. This operation gives all of these lists of numbers a chance to talk to one another, and refine the meanings that\n",
            "they encode based on the context around, all done in parallel. For example, the numbers encoding the word bank might be changed based on the context surrounding it to somehow\n",
            "encode the more specific notion of a riverbank. Transformers typically also include a second type of operation known as a feed-forward neural network, and this gives the model\n",
            "extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental\n",
            "operations. And as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows\n",
            "in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input\n",
            "text, as well as everything the model learned during training, to produce a prediction of the next word. Then the model's prediction looks like a probability for every possible\n",
            "next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on\n",
            "how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does.\n",
            "What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If\n",
            "you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made\n",
            "about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. But also, on my second channel I just posted a talk that I\n",
            "gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content that I make as a casual talk rather than a produced video, but I\n",
            "leave it up to you which one of these feels like the better follow on.\n",
            "\n",
            "Answer: Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models, including neural networks. It works by iteratively adjusting the\n",
            "weights and biases in the direction of the negative gradient of the cost function, which indicates the direction of steepest descent. The size of the adjustment is proportional to\n",
            "the magnitude of the gradient, allowing for efficient convergence towards a local minimum. The algorithm is widely used due to its simplicity and effectiveness in handling large\n",
            "datasets and complex models.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for question: 'How does backpropagation improve the performance of a neural network?'\n",
            "\n",
            "Question: How does backpropagation improve the performance of a neural network?\n",
            "\n",
            "Context: Here we tackle back propagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walk-through for\n",
            "what the algorithm is actually doing without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus\n",
            "underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward\n",
            "information. Here, we're doing the classic example of recognizing handwritten digits, whose pixel values get fed into the first layer of the network with 784 neurons, and I've been\n",
            "showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as a tensor. I'm also\n",
            "expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain\n",
            "cost function. As a quick reminder for the cost of a single training example, what you do is take the output that the network gives, along with the output that you wanted it to\n",
            "give, and you just add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this\n",
            "gives you the total cost of the network. As if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this\n",
            "cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the\n",
            "topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right\n",
            "now, is that because thinking of the gradient vector as a direction in 13,000 dimensions is to put it lightly beyond the scope of our imaginations, there's another way you can\n",
            "think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process\n",
            "I'm about to describe when you compute the negative gradient and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with\n",
            "this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight. So if you were to\n",
            "wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give.\n",
            "Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each\n",
            "part of this algorithm is really doing, each individual effect that it's having is actually pretty intuitive. It's just that there's a lot of little adjustments getting layered on\n",
            "top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through those effects that each training example is having on the\n",
            "weights and biases. Because the cost function involves averaging a certain cost per example, over all the tens of thousands of training examples, the way that we adjust the weights\n",
            "and biases for a single gradient descent step also depends on every single example, or rather, in principle it should, but for computational efficiency, we're going to do a little\n",
            "trick later to keep you from needing to hit every single example for every single step. In other case, right now, all we're going to do is focus our attention on one single\n",
            "example, this image of a two. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well\n",
            "trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. Now, we can't directly change those activations. We\n",
            "only have influence on the weights and biases. But it is helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify\n",
            "the image as a two, we want that third value to get nudged up, while all of the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away\n",
            "each current value is from its target value. For example, the increase to that number two neurons activation is in a sense more important than the decrease to the number eight\n",
            "neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember,\n",
            "that activation is defined as a certain weighted sum of all of the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid\n",
            "squishification function, or a ray-lew. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase\n",
            "the weights, and you can change the activations from the previous layer. Focusing just on how the weights should be adjusted? Notice how the weights actually have differing levels\n",
            "of influence. The connections with the brightest neurons from the preceding layer have the biggest effect, since those weights are multiplied by larger activation values. So if you\n",
            "were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least\n",
            "as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we\n",
            "care about which ones give you the most bang for your butt. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons\n",
            "learn, heavy in theory. Often summed up in the phrase, neurons that fire together, wire together. Here, the biggest increases to weights, the biggest strengthening of connections,\n",
            "happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a two get more strongly\n",
            "linked to those firing when thinking about a two. To be clear, I really am not in a position to make statements one way or another about whether artificial networks of neurons\n",
            "behave anything like biological brains, and this fires together, wire together idea comes with a couple meaningful asterisks. But taken as a very loose analogy, I do find it\n",
            "interesting to note. Anyway, the third way that we can help increase this neuron's activation is by changing all the activations in the previous layer. Namely, if everything\n",
            "connected to that digit two neuron with a positive weight, got brighter, and if everything connected with a negative weight got dimmer, then that digit two neuron would become more\n",
            "active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now,\n",
            "of course, we cannot directly influence those activations. We only have control over the weights and biases. But just as with the last layer, it's helpful to just keep a note of\n",
            "what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit two output neuron wants. Remember, we also want all of the other neurons\n",
            "in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So the desire of this\n",
            "digit two neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer. Again, in proportion to the corresponding\n",
            "weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these\n",
            "desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the\n",
            "relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further,\n",
            "remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listen to what that two wanted, the network would\n",
            "ultimately be incentivized just to classify all images as a two. What you do is you go through this same back property for every other training example, recording how each of them\n",
            "would like to change the weights and the biases. And you average together those desired changes. This collection here of the average to nudges to each weight and bias is, loosely\n",
            "speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get\n",
            "quantitatively precise about those nudges. But if you understood every change that I just referenced, why some are proportionally bigger than others, and how they all need to be\n",
            "added together, you understand the mechanics for what back propagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence\n",
            "of every single training example, every single gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a\n",
            "whole bunch of mini-batches. Let's say each one having 100 training examples. Then you compute a step according to the mini-batch. It's not going to be the actual gradient to the\n",
            "cost function, which depends on all of the training data, not this tiny subset. So it's not the most efficient step downhill. But each mini-batch does give you a pretty good\n",
            "approximation, and more importantly, it gives you a significant computational speed up. If you would applaud the trajectory of your network under the relevant cost surface, it\n",
            "would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of\n",
            "each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's kind of a lot going on here, so let's\n",
            "just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases. Not just in terms\n",
            "of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would\n",
            "involve doing this for all your tens and thousands of training examples and averaging the desired changes that you get. But that's computationally slow, so instead you randomly\n",
            "subdivide the data into these mini batches and compute each step with respect to a mini batch. Repeatedly going through all of the mini batches and making these adjustments, you\n",
            "will converge towards a local minimum of the cost function, which is to say your network is going to end up doing a really good job on the training examples. So with all of that\n",
            "said, every line of code that would go into implementing Backprop actually corresponds with something that you have now seen, at least in informal terms. But sometimes knowing what\n",
            "the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video\n",
            "goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in\n",
            "other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need\n",
            "a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the M-NIST database, with so many examples that have been\n",
            "labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data that you actually need,\n",
            "whether that's having people label tens of thousands of images or whatever other data type you might be dealing with. The hard assumption here is that you've watched Part 3, giving\n",
            "an intuitive walkthrough of the back propagation algorithm. Here we get a little bit more formal and dive into the relevant calculus. It's normal for this to be at least a little\n",
            "confusing so the mantra to regularly pause and ponder certainly applies as much here as anywhere else. Our main goal is to show how people in machine learning commonly think about\n",
            "the chain rule from calculus in the context of networks, which has kind of a different feel from how most introductory calculus courses approach the subject. For those of you\n",
            "uncomfortable with the relevant calculus, I do have a whole series on the topic. Let's just start off with an extremely simple network, one where each layer has a single neuron in\n",
            "it. So this particular network is determined by three weights and three biases, and our goal is to understand how sensitive the cost function is to these variables. That way we\n",
            "know which adjustments to those terms is going to cause the most efficient decrease to the cost function. And we're just going to focus on the connection between the last two\n",
            "neurons. Let's label the activation of that last neuron with a superscript L indicating which layer it's in. So the activation of the previous neuron is a L minus 1. These are not\n",
            "exponents, they're just a way of indexing what we're talking about since I want to save subscripts for different indices later on. Now let's say that the value we want this last\n",
            "activation to be for a given training example is y, for example, y might be 0 or 1. So the cost of this simple network for a single training example is AL minus y squared. We'll\n",
            "denote the cost of that one training example as C0. As a reminder, this last activation is determined by a weight, which I'm going to call WL, times the previous neurons activation\n",
            "plus some bias, which I'll call BL. And then you pump that through some special nonlinear function like the sigmoid or a ray loop. It's actually going to make things easier for us\n",
            "if we give a special name to this weighted sum, like Z, with the same superscript as the relevant activations. So this is a lot of terms and a way that you might conceptualize it\n",
            "is that the weight, the previous action and the bias altogether are used to compute Z, which in turn lets us compute A, which finally, along with a constant Y, lets us compute the\n",
            "cost. And of course, AL minus 1 is influenced by its own weight and bias and such. But we're not going to focus on that right now. Now all of these are just numbers, right? And it\n",
            "can be nice to think of each one as having its own little number line. Our first goal is to understand how sensitive the cost function is to small changes in our weight, WL. Never\n",
            "phrase differently. What is the derivative of C with respect to WL? When you see this Dell W term, think of it as meaning some tiny nudge to W, like a change by 0.01. And think of\n",
            "this Dell C term as meaning whatever the resulting nudge to the cost is. What we want is their ratio. Conceptually, this tiny nudge to WL causes some nudge to ZL, which in turn\n",
            "causes some nudge to AL, which directly influences the cost. So we break things up by first looking at the ratio of a tiny change to ZL to this tiny change W, that is the\n",
            "derivative of ZL with respect to WL. Likewise, you then consider the ratio of the change to AL to the tiny change in ZL that caused it, as well as the ratio between the final nudge\n",
            "to C and this intermediate nudge to AL. This right here is the chain rule, where multiplying together these three ratios gives us the sensitivity of C to small changes in WL. So on\n",
            "screen right now, there's kind of a lot of symbols, and take a moment to just make sure it's clear what they all are. Because now we're going to compute the relevant derivatives.\n",
            "The derivative of C with respect to AL works out to be 2 times AL minus Y. Notice, this means that its size is proportional to the difference between the networks output and the\n",
            "thing that we want it to be. So if that output was very different, even slight changes stand to have a big impact on the final cost function. The derivative of AL with respect to\n",
            "ZL is just the derivative of our sigmoid function, or whatever non-linearity you choose to use. And the derivative of ZL with respect to WL, in this case comes out just to be AL\n",
            "minus 1. Now I don't know about you, but I think it's easy to get stuck head down in the formulas without taking a moment to sit back and remind yourself of what they all actually\n",
            "mean. In the case of this last derivative, the amount that that small nudge to the weight influenced the last layer depends on how strong the previous neuron is. Remember, this is\n",
            "where that neurons that fire together wire together idea comes in. And all of this is the derivative with respect to WL only of the cost for a specific single training example.\n",
            "Since the full cost function involves averaging together all those costs across many different training examples, its derivative requires averaging this expression that we found\n",
            "over all training examples. And of course that is just one component of the gradient vector, which itself is built up from the partial derivatives of the cost function with respect\n",
            "to all those weights and biases. But even though that's just one of the many partial derivatives we need, it's more than 50% of the work. The sensitivity to the bias, for example,\n",
            "is almost identical. We just need to change out this del Z del W term for a del Z del B. And if you look at the relevant formula, that derivative comes out to be 1. Also, and this\n",
            "is where the idea of propagating backwards comes in, you can see how sensitive this cost function is to the activation of the previous layer. Namely, this initial derivative in the\n",
            "chain rule expression, the sensitivity of Z to the previous activation, comes out to be the weight WL. And again, even though we're not going to be able to directly influence that\n",
            "previous layer activation, it's helpful to keep track of. Because now, we can just keep iterating this same chain rule idea backwards to see how sensitive the cost function is to\n",
            "previous weights and previous biases. And you might think that this is an overly simple example, since all layers just have one neuron, and that things are going to get\n",
            "exponentially more complicated for a real network. But honestly, not that much changes when we give the layers multiple neurons. Really it's just a few more indices to keep track\n",
            "of. Rather than the activation of a given layer simply being AL, it's also going to have a subscript, indicating which neuron of that layer it is. Let's go ahead and use the letter\n",
            "K to index the layer L minus 1, and J to index the layer L. For the cost, again, we look at what the desired output is, but this time we add up the squares of the differences\n",
            "between these last layer activations and the desired output. That is, you take a sum over ALJ minus YJ squared. Since there's a lot more weights, each one has to have a couple more\n",
            "indices to keep track of where it is. So let's call the weight of the edge connecting this Kth neuron to the Jth neuron, WLJK. Those indices might feel a little backwards at first,\n",
            "but it lines up with how you'd index the weight matrix that I talked about in the Part 1 video. Just as before, it's still nice to give a name to the relevant weighted sum, like Z,\n",
            "so that the activation of the last layer is just your special function, like the sigmoid, applied to Z. You can kind of see what I mean, right, where all of these are essentially\n",
            "the same equations that we had before in the one neuron per layer case. It's just that it looks a little more complicated. And indeed, the chain ruled the rivetive expression,\n",
            "describing how sensitive the cost is to a specific weight, looks essentially the same. I'll leave it to you to pause and think about each of those terms if you want. What does\n",
            "change here, though, is the derivative of the cost with respect to one of the activations in the layer L minus 1. In this case, the difference is that the neuron influences the\n",
            "cost function through multiple different paths. It is on the one hand, it influences AL0, which plays a role in the cost function, but it also has an influence on AL1, which also\n",
            "plays a role in the cost function, and you have to add those up. And that, well, that's pretty much it. Once you know how sensitive the cost function is to the activations in this\n",
            "second to last layer, you can just repeat the process for all the weights and biases feeding into that layer. So pat yourself on the back. If all of this makes sense, you have now\n",
            "looked deep into the heart of back propagation, the workhorse behind how neural networks learn. These chain rule expressions give you the derivatives that determine each component\n",
            "in the gradient that helps minimize the cost of the network by repeatedly stepping downhill. If you sit back and think about all that, this is a lot of layers of complexity to wrap\n",
            "your mind around. So don't worry if it takes time for your mind to digest it all. Last video, I laid out the structure of a neural network. I'll give a quick recap here just so\n",
            "that it's fresh in our minds and then I have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies not only how neural networks\n",
            "learn, but how a lot of other machine learning works as well. Then after that, we're going to dig in a little more to how this particular network performs and what those hidden\n",
            "layers of neurons end up actually looking for. As a reminder, our goal here is the classic example of handwritten digit recognition, the Hello World of Neural Networks. These\n",
            "digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons in the input layer of the\n",
            "network. And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer plus some special number called a\n",
            "bias. Then you compose that sum with some other function, like the sigmoid squishification or a ray-loo, the way that I walked through last video. In total, given the somewhat\n",
            "arbitrary choice of two hidden layers here with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what\n",
            "exactly the network actually does. And what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds\n",
            "to that digit. And remember, the motivation that we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might\n",
            "pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits. So here we learn how the network learns. What we want is an\n",
            "algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what\n",
            "they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered structure will mean that\n",
            "what it learns generalizes to images beyond that training data. And the way we test that is that after you train the network, you show it more labeled data, that it's never seen\n",
            "before, and you see how accurately it classifies those new images. Fortunately for us, and what makes this such a common example to start with, is that the good people behind the\n",
            "M-NIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers that they're supposed to be. And it's provocative\n",
            "as it is to describe a machine as learning. Once you actually see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like, a calculus exercise. I\n",
            "mean, basically, it comes down to finding the minimum of a certain function. Remember, conceptually, we're thinking of each neuron as being connected to all of the neurons in the\n",
            "previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections. The bias is some indication of whether that neuron\n",
            "tends to be active or inactive. And to start things off, we're just going to initialize all of those weights and biases totally randomly. Needless to say, this network is going to\n",
            "perform pretty horribly on a given training example, since it's just doing something random. For example, you feed in this image of a three, and the output layer just looks like a\n",
            "mess. So what you do is you define a cost function, a way of telling the computer, no, bad computer, that output should have activations, which are zero for most neurons, but one\n",
            "for this neuron. What you gave me is utter trash. To say that a little more mathematically, what you do is add up the squares of the differences between each of those trash output\n",
            "activations and the value that you want them to have. And this is what we'll call the cost of a single training example. Notice, this sum is small when the network confidently\n",
            "classifies the image correctly, but it's large when the network seems like it doesn't really know what it's doing. So then what you do is consider the average cost over all of the\n",
            "tens of thousands of training examples at your disposal. This average cost is our measure for how lousy the network is and how bad the computer should feel. And that's a\n",
            "complicated thing. Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output. And\n",
            "in a sense, it's parameterized by all these weights and biases. Well the cost function is a layer of complexity on top of that. It takes as its input, those 13,000 or so weights\n",
            "and biases, and it spits out a single number describing how bad those weights and biases are. And the way it's defined depends on the network's behavior over all the tens of\n",
            "thousands of pieces of training data. That's a lot to think about. But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to change\n",
            "those weights and biases so that it gets better. To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one\n",
            "number as an input and one number as an output. How do you find an input that minimizes the value of this function? Circular students will know that you can sometimes figure out\n",
            "that minimum explicitly. But that's not always feasible for really complicated functions. Certainly not in the 13,000 input version of this situation for our crazy complicated\n",
            "neural network cost function. A more flexible tactic is to start at any all input and figure out which direction you should step to make that output lower. Specifically, if you can\n",
            "figure out the slope of the function where you are, then shift to the left if that slope is positive and shift the input to the right if that slope is negative. If you do this\n",
            "repeatedly, at each point checking the new slope and taking the appropriate step, you're going to approach some local minimum of the function. And the image you might have in mind\n",
            "here is a ball rolling down a hill. And notice, even for this really simplified single input function, there are many possible valleys that you might land in, depending on which\n",
            "random input you start at. There's no guarantee that the local minimum you land in is going to be the smallest possible value of the cost function. That's going to carry over to\n",
            "our neural network case as well. And I also want you to notice how if you make your step sizes proportional to the slope, then when the slope is flattening out towards the minimum,\n",
            "your steps get smaller and smaller, and that kind of helps you from overshooting. Bumping up the complexity a bit, imagine instead a function with two inputs and one output. You\n",
            "might think of the input space as the x, y plane and the cost function as being graft as a surface above it. Now instead of asking about the slope of the function, you have to ask\n",
            "which direction should you step in this input space so as to decrease the output of the function most quickly. In other words, what's the downhill direction? And again, it's\n",
            "helpful to think of a ball rolling down that hill. Most of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest\n",
            "ascent, basically which direction should you step to increase the function most quickly. Naturally enough, taking the negative of that gradient gives you the direction to step that\n",
            "decreases the function most quickly. And even more than that, the length of this gradient vector is actually an indication for just how steep that steepest slope is. Now if you're\n",
            "unfamiliar with multivariable calculus and you want to learn more, check out some of the work that I did for Khan Academy on the topic. Honestly though, all that matters for you\n",
            "and me right now is that in principle there exists a way to compute this vector. This vector that tells you what the downhill direction is and how steep it is. You'll be okay if\n",
            "that's all you know and you're not rock solid on the details. Because if you can get that, the algorithm for minimizing the function is to compute this gradient direction, then\n",
            "take a small step downhill and just repeat that over and over. It's the same basic idea for a function that has 13,000 inputs instead of two inputs. Imagine organizing all 13,000\n",
            "weights and biases of our network into a giant column vector. The negative gradient of the cost function is just a vector. It's some direction inside this insanely huge input space\n",
            "that tells you which nudges to all of those numbers is going to cause the most rapid decrease to the cost function. And of course, with our specially designed cost function,\n",
            "changing the weights and biases to decrease it means making the output of the network on each piece of training data look less like a random array of 10 values and more like an\n",
            "actual decision that we want it to make. It's important to remember, this cost function involves an average over all of the training data. So if you minimize it, it means it's a\n",
            "better performance on all of those samples. The algorithm for computing this gradient efficiently, which is effectively the heart of how a neural network learns, is called back\n",
            "propagation. And it's what I'm going to be talking about next video. There I really want to take the time to walk through what exactly happens to each weight and each bias for a\n",
            "given piece of training data. Trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas. Right here, right now, the main thing I want\n",
            "you to know, independent of implementation details, is that what we mean when we talk about a network learning is that it's just minimizing a cost function. And notice, one\n",
            "consequence of that is that it's important for this cost function to have a nice smooth output so that we can find a local minimum by taking little steps down hill. This is why, by\n",
            "the way, artificial neurons have continuously ranging activations, rather than simply being active or inactive in a binary way, the way that biological neurons are. This process of\n",
            "repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent. It's a way to converge toward some local minimum of a cost function,\n",
            "basically a valley in this graph. I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000-dimensional input space are a little hard to\n",
            "wrap your mind around, but there is actually a nice non-spatial way to think about this. Each component of the negative gradient tells us two things. The sign, of course, tells us\n",
            "whether the corresponding component of the input vector should be nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you which\n",
            "changes matter more. You see, in our network, an adjustment to one of the weights might have a much greater impact on the cost function than the adjustment to some other weight.\n",
            "Some of these connections just matter more for our training data. So a way that you can think about this gradient vector of our mind-warpingly massive cost function is that it\n",
            "encodes the relative importance of each weight and bias, that is, which of these changes is going to carry the most bang for your buck. This really is just another way of thinking\n",
            "about direction. To take a simpler example, if you have some function with two variables as an input, and you compute that it's gradient at some particular point, comes out as 3-1.\n",
            "Then on the one hand, you can interpret that as saying that when you're standing at that input, moving along this direction increases the function most quickly. But when you graph\n",
            "the function above the plane of input points, that vector is what's giving you the straight uphill direction. But another way to read that is to say that changes to this first\n",
            "variable have three times the importance as changes to the second variable, that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang for\n",
            "your buck. Alright, let's zoom out and sum up where we are so far. The network itself is this function with 784 inputs and 10 outputs, defined in terms of all of these weighted\n",
            "sums. The cost function is a layer of complexity on top of that. It takes the 13,000 weights and biases as inputs, and spits out a single measure of laziness based on the training\n",
            "examples. And the gradient of the cost function is one more layer of complexity still. It tells us what nudges to all of these weights and biases cause the fastest change to the\n",
            "value of the cost function, which you might interpret as saying which changes to which weights matter the most. So, when you initialize the network with random weights and biases\n",
            "and adjust them many times based on this gradient descent process, how well does it actually perform on images that it's never seen before? Well the one that I've described here,\n",
            "with the two hidden layers of 16 neurons each, goes in mostly for aesthetic reasons. Well, it's not bad. It classifies about 96% of the new images that it sees correctly. And\n",
            "honestly, if you look at some of the examples that it messes up on, you kind of feel compelled to cut it a little slack. Now, if you play around with the hidden layer structure and\n",
            "make a couple tweaks, you can get this up to 98%. And that's pretty good. It's not the best. You can certainly get better performance by getting more sophisticated than this plain\n",
            "vanilla network. But given how daunting the initial task is, I just think there's something incredible about any network doing this well on images that it's never seen before,\n",
            "given that we never specifically told it what patterns to look for. Originally, the way that I motivated this structure was by describing a hope that we might have, that the second\n",
            "layer might pick up on little edges, that the third layer would piece together those edges to recognize loops in longer lines, and that those might be piece together to recognize\n",
            "digits. So is this what our network is actually doing? Well, for this one, at least, not at all. Remember how last video we looked at how the weights of the connections from all of\n",
            "the neurons in the first layer to a given neuron in the second layer can be visualized as a given pixel pattern that that second layer neuron is picking up on? Well, when we\n",
            "actually do that, for the weights associated with these transitions from the first layer to the next. Instead of picking up on isolated little edges here and there, they look\n",
            "almost random, just with some very loose patterns in the middle there. It would seem that in the unfathomably large 13,000 dimensional space of possible weights and biases, our\n",
            "network found itself a happy little local minimum that, despite successfully classifying most images, doesn't exactly pick up on the patterns that we might have hoped for. And to\n",
            "really drive this point home, watch what happens when you input a random image. If the system was smart, you might expect it to either feel uncertain, maybe, not really activating\n",
            "any of those 10 output neurons or activating them all evenly. But instead, it confidently gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as\n",
            "it does that an actual image of a 5 is a 5. Fraze differently? Even if this network can recognize digits pretty well, it has no idea how to draw them. A lot of this is because it's\n",
            "such a tightly constrained training setup. I mean, put yourself in the network's shoes here. From its point of view, the entire universe consists of nothing but clearly defined\n",
            "unmoving digits centered in a tiny grid. And its cost function just never gave it any incentive to be anything but utterly confident in its decisions. So with this is the image of\n",
            "what those second layer neurons are really doing, you might wonder why I would introduce this network with the motivation of picking up on edges and patterns. That's just not at\n",
            "all what it ends up doing. Well this is not meant to be our end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s and 90s. And you\n",
            "do need to understand it before you can understand more detailed modern variants, and it clearly is capable of solving some interesting problems. But the more you dig in to what\n",
            "those hidden layers are really doing, the less intelligent it seems. Shifting the focus for a moment from how networks learn to how you learn, that'll only happen if you engage\n",
            "actively with the material here somehow. One pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what changes you might make to\n",
            "this system and how it perceives images if you wanted it to better pick up on things like edges and patterns. But better than that, to actually engage with the material, I highly\n",
            "recommend the book by Michael Neilsen on deep learning and neural networks. In it, you can find the code and the data to download and play with for this exact example, and the book\n",
            "will walk you through step by step what that code is doing. What's awesome is that this book is free and publicly available, so if you do get something out of it, consider joining\n",
            "me in making a donation towards Neilsen's efforts. I've also linked a couple other resources that I like a lot in the description, including the phenomenal and beautiful blog posts\n",
            "by Chris Ola and the articles in Distill. To close things off here for the last few minutes, I want to jump back into a snippet of the interview that I had with Lisha Lee. You\n",
            "might remember her from the last video, she did her PhD work in deep learning. And in this little snippet, she talks about two recent papers that really dig in to how some of the\n",
            "more modern image recognition networks are actually learning. Just to set up where we were in the conversation, the first paper took one of these particularly deep neural networks\n",
            "that's really good at image recognition, and instead of training it on a properly labeled data set, it shuffled all of the labels around before training. Obviously the testing\n",
            "accuracy here was going to be no better than random, since everything's just randomly labeled. But it was still able to achieve the same training accuracy as you would on a\n",
            "properly labeled data set. Basically, the millions of weights for this particular network were enough for it to just memorize the random data, which kind of raises the question for\n",
            "whether minimizing this cost function actually corresponds to any sort of structure in the image, or is it just memorization? It reminds the entire data set of what the correct\n",
            "classification is. And so a couple of half a year later at ICML this year, there was not exactly rebuttal paper, paper that addressed some aspects of like, hey, actually these\n",
            "networks are doing something a little bit smarter than that. If you look at that accuracy curve, if you were just training on a random data set, that curve sort of went down very\n",
            "slowly in almost a linear fashion. So you're really struggling to find that local minimum of possible, the right weights that would get you that accuracy. Whereas if you're\n",
            "actually training on a structure data set, one that has the right labels, you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that\n",
            "accuracy level. And so in some sense, it was easier to find that local maxima. And so it was also interesting about that, is it brings into light another paper from actually a\n",
            "couple of years ago, which has a lot more simplifications about the network layers. But one of the results was saying how if you look at the optimization landscape, the local\n",
            "minima that these networks tend to learn are actually of equal quality. So in some sense, if your data set a structure, you should be able to find that much more easily. My thanks,\n",
            "as always, to those of you supporting on Patreon. I've said before just what a game changer in Patreon is, but these videos really would not be possible without you. I also want to\n",
            "give a special thanks to the VC firm Amplify Partners in their support of these initial videos in the series. If you feed a large language model the phrase, Michael Jordan plays\n",
            "the sport of blank, and you have it predict what comes next, and it correctly predicts basketball. This would suggest that somewhere, inside its hundreds of billions of parameters,\n",
            "it's baked in knowledge about a specific person and his specific sport. And I think in general anyone who's played around with one of these models has the clear sense that it's\n",
            "memorized tons and tons of facts. So a reasonable question you could ask is, how exactly does that work, and where do those facts live? Last December a few researchers from Google\n",
            "Deep Mind posted about work on this question, and they were using this specific example of matching athletes to their sports. And although a full mechanistic understanding of how\n",
            "facts are stored remains unsolved, they had some interesting partial results, including the very general high-level conclusion that the facts seem to live inside a specific part of\n",
            "these networks, known fancifully as the multi-layer perceptrons, or MLPs for short. In the last couple of chapters, you and I have been digging into the details behind\n",
            "transformers, the architecture underlying large language models, and also underlying a lot of other modern AI. In the most recent chapter we were focusing on a piece called\n",
            "Attention, and the next step for you and me is to dig into the details of what happens inside these multi-layer perceptrons, which make up the other big portion of the network. The\n",
            "computation here is actually relatively simple, especially when you compare it to attention. It boils down essentially to a pair of matrix multiplications with a simple something\n",
            "in between. However, interpreting what these computations are doing is exceedingly challenging. Our main goal here is to step through the computations and make them memorable, but\n",
            "I'd like to do it in the context of showing a specific example of how one of these blocks could, at least in principle, store a concrete fact. Specifically, it'll be storing the\n",
            "fact that Michael Jordan plays basketball. I should mention the layout here is inspired by a conversation I had with one of those deep-mind researchers, Neil Nanda. For the most\n",
            "part, I will assume that you've either watched the last two chapters, or otherwise you have a basic sense for what a transformer is, but refreshers never hurt, so here's the quick\n",
            "reminder of the overall flow. You and I have been studying a model that's trained to take in a piece of text and predict what comes next. That input text is first broken into a\n",
            "bunch of tokens, which means little chunks that are typically words or little pieces of words, and each token is associated with a high-dimensional vector, which is to say a long\n",
            "list of numbers. This sequence of vectors then repeatedly passes through two kinds of operation. Attention, which allows the vectors to pass information between one another, and\n",
            "then the multi-layer perceptrons, the thing that we're going to dig into today. And also there's a certain normalization step in between. After the sequence of vectors has flowed\n",
            "through many many different iterations of both of these blocks, by the end, the hope is that each vector has soaked up enough information, both from the context, all of the other\n",
            "words and the input, and also from the general knowledge that was baked into the model weights through training, that it can be used to make a prediction of what token comes next.\n",
            "One of the key ideas that I want you to have in your mind is that all of these vectors live in a very very high-dimensional space, and when you think about that space, different\n",
            "directions can encode different kinds of meaning. So a very classic example that I like to refer back to is how if you look at the embedding of woman and subtract the embedding of\n",
            "man, and you take that little step and you add it to another masculine noun, something like uncle, you land somewhere very very close to the corresponding feminine noun. In this\n",
            "sense, this particular direction encodes gender information. The idea is that many other distinct directions in this super high-dimensional space could correspond to other features\n",
            "that the model might want to represent. In a transformer, these vectors don't merely encode the meaning of a single word, though. As they flow through the network, they imbibe a\n",
            "much richer meaning based on all the context around them, and also based on the model's knowledge. Ultimately, each one needs to encode something far far beyond the meaning of a\n",
            "single word, since it needs to be sufficient to predict what will come next. We've already seen how attention blocks let you incorporate context, but a majority of the model\n",
            "parameters actually live inside the MLP blocks, and one thought for what they might be doing is that they offer extra capacity to store facts. Like I said, the lesson here is going\n",
            "to center on the concrete toy example of how exactly it could store the fact that Michael Jordan plays basketball. Now this toy example is going to require that you and I make a\n",
            "couple of assumptions about that high-dimensional space. First, we'll suppose that one of the directions represents the idea of a first name Michael, and then another nearly\n",
            "perpendicular direction represents the idea of the last name Jordan, and then yet a third direction will represent the idea of basketball. So specifically what I mean by this is if\n",
            "you look in the network and you plug out one of the vectors being processed, if its dot product with this first name Michael direction is one, that's what it would mean for the\n",
            "vector to be encoding the idea of a person with that first name. Otherwise, that dot product would be zero or negative, meaning the vector doesn't really align with that direction.\n",
            "And for simplicity, let's completely ignore the very reasonable question of what it might mean if that dot product was bigger than one. Similarly, its dot product with these other\n",
            "directions would tell you whether it represents the last name Jordan, or basketball. So let's say a vector is meant to represent the full name Michael Jordan, then its dot product\n",
            "with both of these directions would have to be one. Since the text Michael Jordan spans two different tokens, this would also mean we have to assume that an earlier attention block\n",
            "has successfully passed information to the second of these two vectors so as to ensure that it can encode both names. With all of those as the assumptions, let's now dive into the\n",
            "meat of the lesson. What happens inside a multi-layer perceptron? You might think of this sequence of vectors flowing into the block, and remember each vector was originally\n",
            "associated with one of the tokens from the input text. What's going to happen is that each individual vector from that sequence goes through a short series of operations, we'll\n",
            "unpack them in just a moment, and at the end we'll get another vector with the same dimension. That other vector is going to get added to the original one that flowed in, and that\n",
            "sum is the result flowing out. This sequence of operations is something you apply to every vector in the sequence associated with every token in the input, and it all happens in\n",
            "parallel. In particular, the vectors don't talk to each other in this step, they're all kind of doing their own thing. And for you and me, that actually makes it a lot simpler,\n",
            "because it means if we understand what happens to just one of the vectors through this block, we effectively understand what happens to all of them. When I say this block is going\n",
            "to encode the fact that Michael Jordan plays basketball, what I mean is that if a vector flows in that encodes, first name Michael and last name Jordan, then this sequence of\n",
            "computations will produce something that includes that direction basketball, which is what we'll add on to the vector in that position. The first step of this process looks like\n",
            "multiplying that vector by a very big matrix, no surprises there, this is deep learning, and this matrix like all of the other ones we've seen is filled with model parameters that\n",
            "are learned from data, which you might think of as a bunch of knobs and dials that get tweaked and tuned to determine what the model behavior is. Now, one nice way to think about\n",
            "matrix multiplication is to imagine each row of that matrix as being its own vector and taking a bunch of dot products between those rows and the vector being processed, which I'll\n",
            "label as E for embedding. For example, suppose that very first row happened to equal this first name Michael direction that were presuming exists, that would mean that the first\n",
            "component in this output, this dot product right here, would be one if that vector encodes the first name Michael and zero or negative otherwise. Even more fun, take a moment to\n",
            "think about what it would mean if that first row was this first name Michael plus last name Jordan direction. And for simplicity, let me go ahead and write that down as m plus j,\n",
            "then taking a dot product with this embedding E, things distribute really nicely, so it looks like m.e plus j.e and notice how that means the ultimate value would be two if the\n",
            "vector encodes the full name Michael Jordan and otherwise it would be one or something smaller than one. And that's just one row in this matrix. You might think of all of the other\n",
            "rows as in parallel asking some other kinds of questions probing at some other sorts of features of the vector being processed. Very often this step also involves adding another\n",
            "vector through the output, which is full of model parameters learned from data, this other vector is known as the bias. For our example, I want you to imagine that the value of\n",
            "this bias in that very first component is negative one, meaning our final output looks like that relevant dot product, but minus one. You might very reasonably ask why I would want\n",
            "you to assume that the model has learned this. And in a moment, you'll see why it's very clean and nice if we have a value here, which is positive, if and only if a vector encodes\n",
            "the full name Michael Jordan and otherwise it's zero or negative. The total number of rows in this matrix, which is something like the number of questions being asked in the case\n",
            "of GPT3, whose numbers we've been following is just under 50,000. In fact, it's exactly four times the number of dimensions in this embedding space. That's a design choice you\n",
            "could make it more, you could make it less, but having a clean multiple tends to be friendly for hardware. Since this matrix full of weights maps us into a higher dimensional\n",
            "space, I'm going to give it the shorthand W up. I'll continue labeling the vector we're processing as E, and let's label this bias vector as B up and put that all back down in the\n",
            "diagram. At this point, a problem is that this operation is purely linear, but language is a very non-linear process. If the entry that we're measuring is high for Michael plus\n",
            "Jordan, it would also necessarily be somewhat triggered by Michael plus Phelps and also Alexis plus Jordan, despite those being unrelated conceptually. What you really want is a\n",
            "simple yes or no for the full name. So the next step is to pass this large intermediate vector through a very simple non-linear function. A common choice is one that takes all of\n",
            "the negative values and maps them to zero, and leaves all of the positive values unchanged. And continuing with the deep learning tradition of overly fancy names, this very simple\n",
            "function is often called the rectified linear unit, or Rayloo for short. Here's what the graph looks like. So taking our imagined example where this first entry of the intermediate\n",
            "vector is one, if and only if the full name is Michael Jordan, and zero or negative otherwise, after you pass it through the Rayloo, you end up with a very clean value where all of\n",
            "the zero and negative values just get clipped to zero. So this output would be one for the full name Michael Jordan and zero otherwise. In other words, it very directly mimics the\n",
            "behavior of an AND gate. Often models will use a slightly modified function that's called the J-LU, which has the same basic shape, it's just a bit smoother, but for our purposes,\n",
            "it's a little bit cleaner if we only think about the Rayloo. Also, when you hear people refer to the neurons of a transformer, they're talking about these values right here.\n",
            "Whenever you see that common neural network picture with a layer of dots and a bunch of lines connecting to the previous layer, which we had earlier in this series, that's\n",
            "typically meant to convey this combination of a linear step, a matrix multiplication, followed by some simple, termwise non-linear function like a Rayloo. You would say that this\n",
            "neuron is active whenever this value is positive, and that it's inactive if that value is zero. The next step looks very similar to the first one. You multiply by a very large\n",
            "matrix and you add on a certain bias term. In this case, the number of dimensions in the output is back down to the size of that embedding space, so I'm going to go ahead and call\n",
            "this the down projection matrix. And this time, instead of thinking of things row by row, it's actually nicer to think of it column by column. You see, another way that you can\n",
            "hold matrix multiplication in your head is to imagine taking each column of the matrix and multiplying it by the corresponding term in the vector that it's processing and adding\n",
            "together all of those rescaled columns. The reason it's nicer to think about this way is because here, the columns have the same dimension as the embedding space, so we can think\n",
            "of them as directions in that space. For instance, we will imagine that the model has learned to make that first column into this basket ball direction that we suppose exists. What\n",
            "that would mean is that when the relevant neuron in that first position is active, we'll be adding this column to the final result. But if that neuron was inactive, if that number\n",
            "was zero, then this would have no effect. And it doesn't just have to be basketball. The model could also bake into this column many other features that it wants to associate with\n",
            "something that has the full name Michael Jordan. And at the same time, all of the other columns in this matrix are telling you what will be added to the final result if the\n",
            "corresponding neuron is active. And if you have a bias in this case, it's something that you're just adding every single time, regardless of the neuron values. You might wonder\n",
            "what's that doing, as with all parameter field objects here, it's kind of hard to say exactly, maybe there's some book keeping that the network needs to do, but you can feel free\n",
            "to ignore it for now. Making our notation a little more compact again, I'll call this big matrix W down, and similarly call that bias vector B down and put that back into our\n",
            "diagram. Like I previewed earlier, what you do with this final result is add it to the vector that flowed into the block at that position, and that gets you this final result. So\n",
            "for example, if the vector flowing in encoded both first name Michael and last name Jordan, then because this sequence of operations will trigger that AND gate, it will add on the\n",
            "basketball direction. So what pops out will encode all of those together. And remember, this is a process happening to every one of those vectors in parallel. In particular, taking\n",
            "the GPT-3 numbers, it means that this block doesn't just have 50,000 neurons in it, it has 50,000 times the number of tokens in the input. So that is the entire operation, two\n",
            "matrix products each with a bias added, and a simple clipping function in between. Any of you who watched the earlier videos of the series will recognize this structure as the most\n",
            "basic kind of neural network that we studied there. In that example, it was trained to recognize handwritten digits. Over here, in the context of a transformer for a large language\n",
            "model, this is one piece in a larger architecture, and any attempt to interpret what exactly it's doing is heavily intertwined with the idea of encoding information into vectors of\n",
            "a high dimensional embedding space. That is the core lesson, but I do want to step back and reflect on two different things. The first of which is a kind of bookkeeping, and the\n",
            "second of which involves a very thought-provoking fact about higher dimensions that I actually didn't know until I dug into Transformers. In the last two chapters, you and I\n",
            "started counting up the total number of parameters in GPT-3 and seeing exactly where they live. So let's quickly finish up the game here. I already mentioned how this up projection\n",
            "matrix has just under 50,000 rows, and that each row matches the size of the embedding space, which for GPT-3 is 12,288. Multiplying those together, it gives us 604 million\n",
            "parameters just for that matrix, and the down projection has the same number of parameters just with a transpose to shape. So together, they give about 1.2 billion parameters. The\n",
            "bias vector also accounts for a couple more parameters, but it's a trivial proportion of the total, so I'm not even going to show it. In GPT-3, this sequence of embedding vectors\n",
            "flows through not one, but 96 distinct MLPs, so the total number of parameters devoted to all of these blocks adds up to about 116 billion. This is around two-thirds of the total\n",
            "parameters in the network, and when you add it to everything that we had before for the attention blocks, the embedding, and the unembedding, you do indeed get that grand total of\n",
            "175 billion as advertised. It's probably worth mentioning there's another set of parameters associated with those normalization steps that this explanation has skipped over, but\n",
            "like the bias vector, they account for a very trivial proportion of the total. As to that second point of reflection, you might be wondering if this central toy example we've been\n",
            "spending so much time on reflects how facts are actually stored in real-large language models. It is true that the rows of that first matrix can be thought of as directions in this\n",
            "embedding space, and that means the activation of each neuron tells you how much a given vector aligns with some specific direction. It's also true that the columns of that second\n",
            "matrix tell you what will be added to the result if that neuron is active. Both of those are just mathematical facts. However, the evidence does suggest that individual neurons\n",
            "very rarely represent a single clean feature like Michael Jordan. And there may actually be a very good reason this is the case, related to an idea floating around interpretability\n",
            "researchers these days known as superposition. This is a hypothesis that might help to explain both why the models are especially hard to interpret, and also why they scale\n",
            "surprisingly well. The basic idea is that if you have an end-dimensional space and you want to represent a bunch of different features using directions that are all perpendicular\n",
            "to one another in that space, you know, that way if you add a component in one direction it doesn't influence any of the other directions. Then the maximum number of vectors you\n",
            "can fit is only n, the number of dimensions. To a mathematician actually this is the definition of dimension, but where it gets interesting is if you relax that constraint a little\n",
            "bit and you tolerate some noise. Say you allow those features to be represented by vectors that aren't exactly perpendicular, they're just nearly perpendicular, maybe between 89\n",
            "and 91 degrees apart. If we were in two or three dimensions this makes no difference, that gives you hardly any extra wiggle room to fit more vectors in, which makes it all the\n",
            "more counterintuitive that for higher dimensions the answer changes dramatically. I can give you a really quick and dirty illustration of this using some scrappy python that's\n",
            "going to create a list of 100 dimensional vectors, each one initialized randomly, and this list is going to contain 10,000 distinct vectors, so 100 times as many vectors as there\n",
            "are dimensions. This plot right here shows the distribution of angles between pairs of these vectors, so because they started at random those angles could be anything from 0 to 180\n",
            "degrees, but you'll notice that already even just for random vectors there's this heavy bias for things to be closer to 90 degrees. Then what I'm going to do is run a certain\n",
            "optimization process that iteratively nudges all of these vectors so that they try to become more perpendicular to one another. After repeating this many different times, here's\n",
            "what the distribution of angles looks like. We have to actually zoom in on it here, because all of the possible angles between pairs of vectors sit inside this narrow range between\n",
            "89 and 91 degrees. In general, a consequence of something known as the Johnson-Lindon-Strauss lemma is that the number of vectors you can cram into a space that are nearly\n",
            "perpendicular like this, grows exponentially with the number of dimensions. This is very significant for large language models, which might benefit from associating independent\n",
            "ideas with nearly perpendicular directions. It means that it's possible for it to store many, many more ideas than there are dimensions in the space that it's allotted. This might\n",
            "partially explain why model performance seems to scale so well with size. A space that has 10 times as many dimensions can store way, way more than 10 times as many independent\n",
            "ideas. And this is relevant not just to that embedding space where the vectors flowing through the model live, but also to that vector full of neurons in the middle of that multi-\n",
            "layer perceptron that we just studied. That is to say, at the sizes of GPT-3, it might not just be probing at 50,000 features, but if it instead leveraged this enormous added\n",
            "capacity by using nearly perpendicular directions of the space, it could be probing at many, many more features of the vector being processed. But if it was doing that, what it\n",
            "means is that individual features aren't going to be visible as a single neuron lighting up. It would have to look like some specific combination of neurons instead, a\n",
            "superposition. For any of you curious to learn more, a key relevant search term here is Sparce Autoencoder, which is a tool that some of the interpretability people use to try to\n",
            "extract what the true features are, even if they're superimposed on all these neurons. I'll link to a couple really great anthropic posts all about this. At this point, we haven't\n",
            "touched every detail of a transformer, but you and I have hit the most important points. The main thing that I want to cover in a next chapter is the training process. On the one\n",
            "hand, the short answer for how training works is that it's all back propagation, and we covered back propagation in a separate context with earlier chapters in the series. But\n",
            "there is more to discuss, like the specific cost function used for language models, the idea of fine-tuning, using reinforcement learning with human feedback, and the notion of\n",
            "scaling laws. Quick note for the active followers among you, there are a number of non-machine learning-related videos that I'm excited to sync my teeth into before I make that\n",
            "next chapter, so it might be a while, but I do promise it'll come in due time. So, I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll\n",
            "start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one.\n",
            "I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. I'll start with the first one. Imagine you happen across\n",
            "a short movie script that describes a scene between a person and their AI assistant. The script has what the person asks the AI, but the AI's response has been torn off. Suppose\n",
            "you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next. You can then finish the script by feeding in what you\n",
            "have to the machine, seeing what it would predict to start the AI's answer, and then repeating this over and over with a growing script completing the dialogue. When you interact\n",
            "with a chatbot, this is exactly what's happening. A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead\n",
            "of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To build a chatbot, what you do is lay out some text that describes\n",
            "an interaction between a user and a hypothetical AI assistant. You add on whatever the user types in as the first part of that interaction. Then you have the model repeatedly\n",
            "predict the next word that such a hypothetical AI assistant would say in response, and that's what's presented to the user. In doing this, the output tends to look a lot more\n",
            "natural if you allow it to select less likely words along the way at random. So what this means is, even though the model itself is deterministic, a given prompt typically gives a\n",
            "different answer each time it's run. Models learn how to make these predictions by processing an enormous amount of text, typically pulled from the internet. For a standard human\n",
            "to read the amount of text that was used to train GPT-3, for example, if they read non-stop 24-7, it would take over 2600 years, larger models since then, train on much, much more.\n",
            "You can think of training a little bit like tuning the dials on a big machine. The way that a language model behaves is entirely determined by these many different continuous\n",
            "values, usually called parameters or weights. Changing those parameters will change the probabilities that the model gives for the next word on a given input. What puts the large\n",
            "in large language model is how they can have hundreds of billions of these parameters. No human ever deliberately sets those parameters. Instead they begin at random, meaning the\n",
            "model just outputs gibberish, but they're repeatedly refined based on many example pieces of text. One of these training examples could be just a handful of words, or it could be\n",
            "thousands, but in either case the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last\n",
            "word from the example. An algorithm called back propagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true\n",
            "last word and a little less likely to choose all the others. When you do this for many, many trillions of examples, not only does the model start to give more accurate predictions\n",
            "on the training data, but it also starts to make more reasonable predictions on text that it's never seen before. Given the huge number of parameters and the enormous amount of\n",
            "training data, the scale of computation involved in training a large language model is mind-boggling. To illustrate, imagine that you could perform one billion additions and\n",
            "multiplications every single second. How long do you think that it would take for you to do all of the operations involved in training the largest language models? Do you think it\n",
            "would take a year? Maybe something like 10,000 years? The answer is actually much more than that. It's well over 100 million years. This is only part of the story though. This\n",
            "whole process is called pre-training. The goal of auto-completing a random passage of text from the internet is very different from the goal of being a good AI assistant. To\n",
            "address this, chatbots undergo another type of training, just as important, called reinforcement learning with human feedback. Use flag-unhelpful or problematic predictions and\n",
            "their corrections further change the model's parameters, making them more likely to give predictions that users prefer. Looking back at the pre-training though, this staggering\n",
            "amount of computation is only made possible by using special computer chips that are optimized for running many, many operations in parallel, known as GPUs. However, not all\n",
            "language models can be easily parallelized. Prior to 2017, most language models would process text one word at a time. But then, a team of researchers at Google introduced a new\n",
            "model known as the transformer. Transformers don't read text from the start to the finish. They soak it all in at once in parallel. The very first step inside a transformer, and\n",
            "most other language models for that matter, is to associate each word with a long list of numbers. The reason for this is that the training process only works with continuous\n",
            "values, so you have to somehow encode language using numbers, and each of these list of numbers may somehow encode the meaning of the corresponding word. What makes Transformers\n",
            "unique is their reliance on a special operation known as attention. This operation gives all of these lists of numbers a chance to talk to one another, and refine the meanings that\n",
            "they encode based on the context around, all done in parallel. For example, the numbers encoding the word bank might be changed based on the context surrounding it to somehow\n",
            "encode the more specific notion of a riverbank. Transformers typically also include a second type of operation known as a feed-forward neural network, and this gives the model\n",
            "extra capacity to store more patterns about language learned during training. All of this data repeatedly flows through many different iterations of these two fundamental\n",
            "operations. And as it does so, the hope is that each list of numbers is enriched to encode whatever information might be needed to make an accurate prediction of what word follows\n",
            "in the passage. At the end, one final function is performed on the last vector in this sequence, which now has had a chance to be influenced by all the other context from the input\n",
            "text, as well as everything the model learned during training, to produce a prediction of the next word. Then the model's prediction looks like a probability for every possible\n",
            "next word. Although researchers design the framework for how each of these steps work, it's important to understand that the specific behavior is an emergent phenomenon based on\n",
            "how those hundreds of billions of parameters are tuned during training. This makes it incredibly challenging to determine why the model makes the exact predictions that it does.\n",
            "What you can see is that when you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful. If\n",
            "you're a new viewer and you're curious about more details on how transformers and attention work, boy do I have some material for you. One option is to jump into a series I made\n",
            "about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer. But also, on my second channel I just posted a talk that I\n",
            "gave a couple months ago about this topic for the company TNG in Munich. Sometimes I actually prefer the content that I make as a casual talk rather than a produced video, but I\n",
            "leave it up to you which one of these feels like the better follow on.\n",
            "\n",
            "Answer: Backpropagation is an algorithm used in neural networks to compute the negative gradient of the cost function, which is necessary for the weights and biases to be adjusted during\n",
            "the training process using gradient descent. It allows the network to learn from its errors by propagating the error back through the network, adjusting the weights and biases in\n",
            "the opposite direction of the gradient to minimize the cost function. This process is repeated for multiple iterations until the network reaches a satisfactory level of accuracy.\n",
            "The intuition behind backpropagation is that small adjustments to the weights and biases in the direction of the negative gradient will lead to a significant decrease in the cost\n",
            "function, making the network's predictions more accurate.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the list of questions you want to ask\n",
        "Questions = [\n",
        "    \"When to use Relu or Sigmoid in a Neural Network?\",\n",
        "    \"What is Gradient Descent and how does it work?\",\n",
        "    \"How does backpropagation improve the performance of a neural network?\"\n",
        "]\n",
        "\n",
        "# Iterate over each question, retrieve the response, and format the output\n",
        "for Question in Questions:\n",
        "    # Get the response from the RAG chain\n",
        "    response = rag_chain_with_source.invoke(Question)\n",
        "\n",
        "    # Format the response using the format_string_response function\n",
        "    formatted_response = format_string_response(response['answer'], line_width=180)\n",
        "\n",
        "    # Print the formatted response\n",
        "    print(f\"Response for question: '{Question}'\\n\")\n",
        "    print(formatted_response)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")  # Print a separator between responses\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afede7beef5b42d180a542bd964834e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15269081ec00482dbce6b7f00b0f270d",
              "IPY_MODEL_5f5b2f331f194df8a24326cf86af9ea0",
              "IPY_MODEL_091a97f89f064280ada55f2426f2a817"
            ],
            "layout": "IPY_MODEL_c5a5e4667db14002ab76f6680f98eaa6"
          }
        },
        "15269081ec00482dbce6b7f00b0f270d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfc341c6259744a1a0793fd02d678ac8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_72e53132a4bf4d278d0225d336dbdfdf",
            "value": "modules.json:‚Äá100%"
          }
        },
        "5f5b2f331f194df8a24326cf86af9ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c53a584e42554d9c85bc7bf1f05920f9",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eaf78000b7a04b91aded555eacb3a9f2",
            "value": 349
          }
        },
        "091a97f89f064280ada55f2426f2a817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14daad888b9427d9cb21617c40adb9d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_249e95d64ec6437784204ff854b9bb12",
            "value": "‚Äá349/349‚Äá[00:00&lt;00:00,‚Äá22.3kB/s]"
          }
        },
        "c5a5e4667db14002ab76f6680f98eaa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfc341c6259744a1a0793fd02d678ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72e53132a4bf4d278d0225d336dbdfdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c53a584e42554d9c85bc7bf1f05920f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaf78000b7a04b91aded555eacb3a9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c14daad888b9427d9cb21617c40adb9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "249e95d64ec6437784204ff854b9bb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed49078acfa24144bac00ac286fda058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07c8e1904e6c4536aa4e3f17977b9519",
              "IPY_MODEL_ab6ffa0031eb40b0a4f17a91c9c73312",
              "IPY_MODEL_e0691dd7eb314550855ece83be3a5e3e"
            ],
            "layout": "IPY_MODEL_2d8e66d3065d49b0ac34d3d7a405640f"
          }
        },
        "07c8e1904e6c4536aa4e3f17977b9519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77d079abfa74479f8dc846d381fe0c71",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_15d5c14398054885ad7f19a28026e709",
            "value": "config_sentence_transformers.json:‚Äá100%"
          }
        },
        "ab6ffa0031eb40b0a4f17a91c9c73312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43b11d270eba4566b91cf5ad52da7efb",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b87d6171a98d4224ac987fee580ba0ce",
            "value": 124
          }
        },
        "e0691dd7eb314550855ece83be3a5e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8afca47dab7142f19ee812bac0d0a6a4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1d17f86752d544778e96aad3484b8f8b",
            "value": "‚Äá124/124‚Äá[00:00&lt;00:00,‚Äá13.0kB/s]"
          }
        },
        "2d8e66d3065d49b0ac34d3d7a405640f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d079abfa74479f8dc846d381fe0c71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d5c14398054885ad7f19a28026e709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43b11d270eba4566b91cf5ad52da7efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87d6171a98d4224ac987fee580ba0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8afca47dab7142f19ee812bac0d0a6a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d17f86752d544778e96aad3484b8f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fae9a33752244eebbb7ea6b58192bf93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_831d11b5813144e5bbe9314634cdd056",
              "IPY_MODEL_6317712cd28c45e6a81b654fa67750ea",
              "IPY_MODEL_b8f118efd5ef406c9acb9845a3731290"
            ],
            "layout": "IPY_MODEL_b8f71a543c2e4e2bb94d854f9640a57b"
          }
        },
        "831d11b5813144e5bbe9314634cdd056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f59877cc604f2aaff3d8459ec45ec6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_177ae749f7db4c0c8e7202a5578cfe93",
            "value": "README.md:‚Äá100%"
          }
        },
        "6317712cd28c45e6a81b654fa67750ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78a2808da09b4ffe8d1fd0a807253a06",
            "max": 90069,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_723b09ee739d402f9737763322c74d2d",
            "value": 90069
          }
        },
        "b8f118efd5ef406c9acb9845a3731290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_286c3c3e09754ebbaa56234d0525cd4d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5883404f0dae461e9434d485898b28d4",
            "value": "‚Äá90.1k/90.1k‚Äá[00:00&lt;00:00,‚Äá4.71MB/s]"
          }
        },
        "b8f71a543c2e4e2bb94d854f9640a57b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f59877cc604f2aaff3d8459ec45ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "177ae749f7db4c0c8e7202a5578cfe93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78a2808da09b4ffe8d1fd0a807253a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "723b09ee739d402f9737763322c74d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "286c3c3e09754ebbaa56234d0525cd4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5883404f0dae461e9434d485898b28d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28b790a576c24395adb668efe6d7896a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be181be21b37411bb5f4073348df1cf5",
              "IPY_MODEL_a288e3d2f05944bab12abf1fa45cf57a",
              "IPY_MODEL_1d60c970da224435980d3fd8129c3ee3"
            ],
            "layout": "IPY_MODEL_2b58f7ee6ffb45eeba73c8815aeb9613"
          }
        },
        "be181be21b37411bb5f4073348df1cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7cfd11f00cc4652ab3bc735d0fa5363",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b38314a06b3c4792bcd7b448a0ef3b93",
            "value": "sentence_bert_config.json:‚Äá100%"
          }
        },
        "a288e3d2f05944bab12abf1fa45cf57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aabe0234473471687683015de8198b7",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ee007f6fffb484096c0f8aa2d45093c",
            "value": 52
          }
        },
        "1d60c970da224435980d3fd8129c3ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131b34524e6f4bc49bf6320dc4504c0c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_29e41c6a781a465cb523b9f2e9495066",
            "value": "‚Äá52.0/52.0‚Äá[00:00&lt;00:00,‚Äá5.49kB/s]"
          }
        },
        "2b58f7ee6ffb45eeba73c8815aeb9613": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7cfd11f00cc4652ab3bc735d0fa5363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b38314a06b3c4792bcd7b448a0ef3b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aabe0234473471687683015de8198b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee007f6fffb484096c0f8aa2d45093c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "131b34524e6f4bc49bf6320dc4504c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e41c6a781a465cb523b9f2e9495066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff5fc402d415469a91af8e2e97300be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67a6eb17b5da4cbbbac3a777126b9368",
              "IPY_MODEL_514b9d4ca8de46e3ba4ce71d0af1eeb9",
              "IPY_MODEL_de02a76c9a114251960f64a0cd6267db"
            ],
            "layout": "IPY_MODEL_bd2de18e8d5547b19a12d6c27c026f51"
          }
        },
        "67a6eb17b5da4cbbbac3a777126b9368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba38f64a0c0043ee86bd0dfa53f2b9d5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fce9cce645264628a22143aed95fd416",
            "value": "config.json:‚Äá100%"
          }
        },
        "514b9d4ca8de46e3ba4ce71d0af1eeb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fd29a410a6c4110ab566c34fe196946",
            "max": 719,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4692a8f8a3f14d5da7b78bfc9e1238d7",
            "value": 719
          }
        },
        "de02a76c9a114251960f64a0cd6267db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c15ba2f6ac1f44c9973faa30b6e2f2d9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ac8a755343144a6588733838bce53585",
            "value": "‚Äá719/719‚Äá[00:00&lt;00:00,‚Äá76.1kB/s]"
          }
        },
        "bd2de18e8d5547b19a12d6c27c026f51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba38f64a0c0043ee86bd0dfa53f2b9d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fce9cce645264628a22143aed95fd416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fd29a410a6c4110ab566c34fe196946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4692a8f8a3f14d5da7b78bfc9e1238d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c15ba2f6ac1f44c9973faa30b6e2f2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac8a755343144a6588733838bce53585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b50021544ba148d9bdf28320ae756b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e2374699dfb48f3801bc107c92251ad",
              "IPY_MODEL_93a5e9ea029c4043ba908ac552066adc",
              "IPY_MODEL_ffd256cdad164b35b28647bd92896420"
            ],
            "layout": "IPY_MODEL_7a5ca52007754b4baa9b5414ab346da6"
          }
        },
        "3e2374699dfb48f3801bc107c92251ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_309d14f9f73c4f46a6430b03c3ef8f63",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_48f52f8f33b4459ab2b6a08909038d73",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "93a5e9ea029c4043ba908ac552066adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86995a783ba847e397bc14d6c36063ac",
            "max": 437955512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96739e945b694088bd8b86af223b6667",
            "value": 437955512
          }
        },
        "ffd256cdad164b35b28647bd92896420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19b50b88d074cddaacd2f61e76b3dbb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fb9963daa6a148c78573c4aa83f9d5d8",
            "value": "‚Äá438M/438M‚Äá[00:02&lt;00:00,‚Äá196MB/s]"
          }
        },
        "7a5ca52007754b4baa9b5414ab346da6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309d14f9f73c4f46a6430b03c3ef8f63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f52f8f33b4459ab2b6a08909038d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86995a783ba847e397bc14d6c36063ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96739e945b694088bd8b86af223b6667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c19b50b88d074cddaacd2f61e76b3dbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb9963daa6a148c78573c4aa83f9d5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532607063cf04320a6a282f644fca438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9edba58c82c34c90a3ec8ae51f551e07",
              "IPY_MODEL_83fe066cfa964447ab8f7ab311fcc820",
              "IPY_MODEL_49e4fac0bb4d4cb3a001ce483dd62411"
            ],
            "layout": "IPY_MODEL_4dfd5c7c6a9e44ba80123d2c9839de9a"
          }
        },
        "9edba58c82c34c90a3ec8ae51f551e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34a770703bed4b0394466a631d8bbcb6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6cf85aab0abd463c959b8a174fa7116b",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "83fe066cfa964447ab8f7ab311fcc820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df76689d4b7b4727a52f8c0ca748fb03",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17de5066bda844e7b47da10b04bb8c26",
            "value": 366
          }
        },
        "49e4fac0bb4d4cb3a001ce483dd62411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f840b3a4fd174a498f2d95d8fb2fa5f7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b7f2562edbb147c3bb8a65c7f8c85fb5",
            "value": "‚Äá366/366‚Äá[00:00&lt;00:00,‚Äá26.9kB/s]"
          }
        },
        "4dfd5c7c6a9e44ba80123d2c9839de9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a770703bed4b0394466a631d8bbcb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf85aab0abd463c959b8a174fa7116b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df76689d4b7b4727a52f8c0ca748fb03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17de5066bda844e7b47da10b04bb8c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f840b3a4fd174a498f2d95d8fb2fa5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7f2562edbb147c3bb8a65c7f8c85fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0c066b52d94493289473ee98dfa0282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_197e0495377c4e7fbf45076a5474055d",
              "IPY_MODEL_0f32f69418754162acbc350410ebc454",
              "IPY_MODEL_06be9fa6a4484e6fb45d5adfe385f642"
            ],
            "layout": "IPY_MODEL_3b15d1cca9074a78aeddeb5db1084a7c"
          }
        },
        "197e0495377c4e7fbf45076a5474055d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c2ccb35a7cb4db29838d2c4f9b37eb7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c90276d5ed9e409b85e062fae77aeec2",
            "value": "vocab.txt:‚Äá100%"
          }
        },
        "0f32f69418754162acbc350410ebc454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75f63fc784b541d58c77aa5d28e2cee9",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4be6ffab9e9b44dd96b9eaf29c56d9a2",
            "value": 231508
          }
        },
        "06be9fa6a4484e6fb45d5adfe385f642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6c59845e82c44c789ffa2933f6ed93d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5ec617bf3ec548b8bc72c7b6299fb2b9",
            "value": "‚Äá232k/232k‚Äá[00:00&lt;00:00,‚Äá14.7MB/s]"
          }
        },
        "3b15d1cca9074a78aeddeb5db1084a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c2ccb35a7cb4db29838d2c4f9b37eb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c90276d5ed9e409b85e062fae77aeec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75f63fc784b541d58c77aa5d28e2cee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be6ffab9e9b44dd96b9eaf29c56d9a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6c59845e82c44c789ffa2933f6ed93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ec617bf3ec548b8bc72c7b6299fb2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acde75cef7cd4c489f479244309f1839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b330b33328a420b8bd7683666129c62",
              "IPY_MODEL_1612205dd8d748698f1c6f0f53c42c32",
              "IPY_MODEL_09f771dfdbdd41af99445fce72cf31a9"
            ],
            "layout": "IPY_MODEL_34f4e44a754e47e19f3ccbb934a4690b"
          }
        },
        "9b330b33328a420b8bd7683666129c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3c0a957187645fca7de1b06284cc0ab",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_82947310a26647deb1f11ec2e19ea958",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "1612205dd8d748698f1c6f0f53c42c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc039026db9b4589b90ed5174ca26f85",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c572528ddc9841c29420523ced1e0f31",
            "value": 711396
          }
        },
        "09f771dfdbdd41af99445fce72cf31a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7335efdc79c240d1856396fe865cb95c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7d19e0d732ed47ad837a323061aa212b",
            "value": "‚Äá711k/711k‚Äá[00:00&lt;00:00,‚Äá3.68MB/s]"
          }
        },
        "34f4e44a754e47e19f3ccbb934a4690b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c0a957187645fca7de1b06284cc0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82947310a26647deb1f11ec2e19ea958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc039026db9b4589b90ed5174ca26f85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c572528ddc9841c29420523ced1e0f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7335efdc79c240d1856396fe865cb95c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d19e0d732ed47ad837a323061aa212b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdf7a653af6743108a444763f781778b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5404673346a40aa8d7356d7dccd51c2",
              "IPY_MODEL_8ca167b2b0db48bb926916a65596e4b7",
              "IPY_MODEL_b79231fae1384628809ecb68d4310e1f"
            ],
            "layout": "IPY_MODEL_8cbf05461eb34a18bc5c600845ac264c"
          }
        },
        "b5404673346a40aa8d7356d7dccd51c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52f9fa59fa794350af6b3ca281849fad",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c0ed2d4c14b47378f8443c86af04758",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "8ca167b2b0db48bb926916a65596e4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5c35468e43f49329d884d8c561d2d9b",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afbef1111f244868b3147ad0c8132b73",
            "value": 125
          }
        },
        "b79231fae1384628809ecb68d4310e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a8a709d3d6a4320a91096691054f301",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7ca1559f346948d79840cbbff65770f3",
            "value": "‚Äá125/125‚Äá[00:00&lt;00:00,‚Äá7.05kB/s]"
          }
        },
        "8cbf05461eb34a18bc5c600845ac264c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52f9fa59fa794350af6b3ca281849fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c0ed2d4c14b47378f8443c86af04758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5c35468e43f49329d884d8c561d2d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afbef1111f244868b3147ad0c8132b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a8a709d3d6a4320a91096691054f301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ca1559f346948d79840cbbff65770f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67180b30b4244ed096ed332975bdbfc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8978e23552de45a3b8e004bc4299ee37",
              "IPY_MODEL_21b05f6a67b44d88a81c38957b59b5dc",
              "IPY_MODEL_eb40d295b1174f39a575d929fd8124b1"
            ],
            "layout": "IPY_MODEL_ad9f78b024264757aa44be6f75d07eb9"
          }
        },
        "8978e23552de45a3b8e004bc4299ee37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_406e7c2afde040a38d48089a7b2830e9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f987889727f44df0adf51484b42a11d9",
            "value": "config.json:‚Äá100%"
          }
        },
        "21b05f6a67b44d88a81c38957b59b5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39869865c0f34831be670dc97311125c",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db6911111853488aa4b47f2c94680bf5",
            "value": 190
          }
        },
        "eb40d295b1174f39a575d929fd8124b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f097580494dc458cbc7021cd6c0459ea",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2dbeba8785b94ab083c3f58a458ff0a9",
            "value": "‚Äá190/190‚Äá[00:00&lt;00:00,‚Äá20.0kB/s]"
          }
        },
        "ad9f78b024264757aa44be6f75d07eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "406e7c2afde040a38d48089a7b2830e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f987889727f44df0adf51484b42a11d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39869865c0f34831be670dc97311125c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db6911111853488aa4b47f2c94680bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f097580494dc458cbc7021cd6c0459ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dbeba8785b94ab083c3f58a458ff0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}